{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 08:21:22.511765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-11 08:21:23.937268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import VAE_Model_ATLAS_beta, Qmake_encoder_set_weights, Qmake_decoder_set_weights\n",
    "# in VAE_0 we are using the beta cyclical annealing from Kenny's repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "    X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "    X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "    Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data \n",
    "    hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57) \"\"\n",
    "    hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57) \"\"\n",
    "    leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57) \"\"\n",
    "    print(\"Data loaded from preprocessed_SNL_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 15\n",
    "LR_PATIENCE = 10\n",
    "\n",
    "# enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "# enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fd932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "# dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "# vae = VAE_Model(enc, dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.8)\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1000)\n",
    "# vae.compile(optimizer=opt) # Not sure what weighted_mse is doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd0d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # looks like early_stopping is needed for val_loss\n",
    "# early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 14s 5ms/step - loss: 1.8206 - reconstruction_loss: 0.9840 - kl_loss: 0.0952 - beta: 0.1560 - val_loss: 1.1118 - val_reconstruction_loss: 1.0277 - val_kl_loss: 0.0842 - val_beta: 0.1560 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.4939 - reconstruction_loss: 0.3983 - kl_loss: 0.0569 - beta: 0.2120 - val_loss: 0.9851 - val_reconstruction_loss: 0.9278 - val_kl_loss: 0.0573 - val_beta: 0.2120 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      " 926/2500 [==========>...................] - ETA: 6s - loss: 0.4089 - reconstruction_loss: 0.3597 - kl_loss: 0.0405 - beta: 0.2327"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "NUM_TRAIN = 20 # For now lets just try 20\n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/VAE_trainings/attempt3/atlas_beta/\" # As of 7/10/25 should be synced with vae1_analysis\n",
    "# Last save is in attempt 1. New save should go to attempt 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: no weighted MSE, no call_backs\n",
    "# 1: adding ReduceLRonPlatueau and early_stopping and the test_step\n",
    "# 2: /atlas_beta/. now training multiple times. But other wise no different from previous. This is to differentiate between the old cyclical beta and the atlas beta schedule.\n",
    "# Changed batch size to match paper's 1024\n",
    "# 3: Clipnorm changed. \n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        vae = VAE_Model_ATLAS_beta(enc, dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.8)\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1000)\n",
    "        vae.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "        # history = vae.fit(\n",
    "        #             train_ds,\n",
    "        #             validation_data=val_ds,\n",
    "        #             epochs=NUM_EPOCHS,\n",
    "        #             callbacks=[early_stopping, reduce_lr],\n",
    "        #             verbose=2\n",
    "        #         )\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'history' is the object returned by your model.fit() call\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[43mSAVE_PATH\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_history.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         history \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(2):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reconstruction_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reconstruction_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "    plt.plot(history['beta'],label=\"beta\")\n",
    "\n",
    "    # Plot validation losses\n",
    "    plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
