{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 13:45:41.650288: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-04 13:45:44.702741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl, plot_rocs, calc_anomaly_dist, AD_score_KL, AD_score_CKL\n",
    "# from models import VAE_Model_ATLAS_beta as NNmodel\n",
    "\n",
    "\n",
    "# # Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(gpus[3], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35a016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3d397",
   "metadata": {},
   "source": [
    "### Loss plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d9f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 6 plotting complete!\n",
      "Attempt 7 plotting complete!\n",
      "Attempt 8 plotting complete!\n",
      "Attempt 9 plotting complete!\n",
      "Attempt 10 plotting complete!\n",
      "Attempt 11 plotting complete!\n",
      "Attempt 12 plotting complete!\n",
      "Attempt 13 plotting complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1847137/2148925915.py:116: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  ax.set_yscale('log')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 14 plotting complete!\n",
      "Attempt 15 plotting complete!\n",
      "Attempt 16 plotting complete!\n",
      "Attempt 17 plotting complete!\n",
      "Attempt 18 plotting complete!\n",
      "Attempt 19 plotting complete!\n",
      "Attempt 20 plotting complete!\n",
      "Attempt 21 plotting complete!\n",
      "Attempt 22 plotting complete!\n",
      "Attempt 23 plotting complete!\n",
      "Attempt 24 plotting complete!\n",
      "Attempt 25 plotting complete!\n",
      "Attempt 26 plotting complete!\n",
      "Attempt 27 plotting complete!\n",
      "Attempt 28 plotting complete!\n",
      "Attempt 29 plotting complete!\n",
      "Attempt 30 plotting complete!\n",
      "Attempt 31 plotting complete!\n",
      "Attempt 32 plotting complete!\n",
      "Attempt 33 plotting complete!\n",
      "Attempt 34 plotting complete!\n",
      "Attempt 35 plotting complete!\n",
      "Attempt 36 plotting complete!\n",
      "Attempt 37 plotting complete!\n"
     ]
    }
   ],
   "source": [
    "# History Keys\n",
    "keys = [\n",
    "        'loss'               # VAE total loss term.\n",
    "        ,'reco_loss'         # VAE loss term\n",
    "        ,'kl_loss'           # VAE Loss term\n",
    "        ,'disc_loss'         # VAE loss due to discriminator \"failure to fool disc\"\n",
    "        # # ,'raw_loss'          # Reco_loss + kl_loss\n",
    "        ,'w_kl_loss'         # kl_loss * beta\n",
    "        ,'w_disc_loss'       # disc_loss * gamma\n",
    "        ,'d_loss'\n",
    "        # Validation version\n",
    "        ,'val_loss'          \n",
    "        ,'val_reco_loss'\n",
    "        ,'val_kl_loss'\n",
    "        ,'val_disc_loss'\n",
    "        # ,'val_raw_loss'\n",
    "        ,'val_w_kl_loss'\n",
    "        ,'val_w_disc_loss'\n",
    "        ,'val_d_loss'\n",
    "        # --\n",
    "        # ,'beta'              # hyperparameter\n",
    "        # ,'gamma'             # hyperparameter\n",
    "        # ,'val_gamma'         # hyperparameter\n",
    "        # ,'val_beta'          # hyperparameter\n",
    "        # ,'lr'              # learning rate\n",
    "        ]\n",
    "\n",
    "color_key = {\n",
    "             'loss' : 'k'               # VAE total loss term.\n",
    "            ,'val_loss' : 'k'         \n",
    "            ,'reco_loss': 'tab:blue'         # VAE loss term\n",
    "            ,'val_reco_loss' : 'tab:blue'\n",
    "            ,'kl_loss': 'crimson'          # VAE Loss term\n",
    "            ,'val_kl_loss': 'crimson'\n",
    "            ,'disc_loss' : 'tab:orange'        # VAE loss due to discriminator \"failure to fool disc\"\n",
    "            ,'val_disc_loss' : 'tab:orange'\n",
    "            ,'w_kl_loss'  : 'c'        # kl_loss * beta\n",
    "            ,'val_w_kl_loss' : 'c'\n",
    "            ,'w_disc_loss'  : 'tab:green'     # disc_loss * gamma\n",
    "            ,'val_w_disc_loss' : 'tab:green'\n",
    "            ,'d_loss': 'r'\n",
    "            ,'val_d_loss' :'r'\n",
    "        }\n",
    "# d_loss : discriminator loss\n",
    "# loss : generator total loss\n",
    "# raw_loss : reconstruction and kl_loss without beta weighting\n",
    "\n",
    "\n",
    "# Generate cleaner legend\n",
    "proxy_lines = {}\n",
    "for key in keys:\n",
    "    base_key = key.replace('val_', '')  # Strip 'val_' to group them\n",
    "    if base_key not in proxy_lines and key in color_key:\n",
    "        proxy_lines[base_key] = Line2D([0], [0], \n",
    "                                    color=color_key[key], \n",
    "                                    lw=2, \n",
    "                                    label=base_key)\n",
    "clean_leg = list(proxy_lines.values())\n",
    "for att_n in range(6, 38): # plot all attempts. most recent is 18.\n",
    "    att_path = SAVE_PATH + f\"attempt{att_n}/\"\n",
    "\n",
    "    # Make folder for loss plots if it doesn't exist\n",
    "    plot_dir = os.path.join(SAVE_PATH, f\"loss_plots/attempt{att_n}/\")\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(10): # Currently only training 10 models at a time.\n",
    "        save_path = att_path + f\"n_{i}/\"\n",
    "        with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "            history = pkl.load(f)\n",
    "    \n",
    "        \n",
    "        # Plot training losses\n",
    "        # fig, (ax, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(8,10))\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])  # 3:1 means top gets 75%, bottom 25%\n",
    "\n",
    "        ax = fig.add_subplot(gs[0])\n",
    "        ax2 = fig.add_subplot(gs[1], sharex=ax)\n",
    "\n",
    "        # Calculate fractional contributions to total VAE loss\n",
    "        loss = np.array(history['loss'])\n",
    "        reco_loss = np.array(history['reco_loss'])\n",
    "        beta = np.array(history['beta'])\n",
    "        reco_loss_frac = (reco_loss * (1 - beta))/loss\n",
    "\n",
    "        w_kl_loss_frac = np.array(history['w_kl_loss'])/loss\n",
    "        w_disc_loss_frac = np.array(history['w_disc_loss'])/loss\n",
    "        ax2.plot(reco_loss_frac, label='reco_loss_frac')\n",
    "        ax2.plot(w_kl_loss_frac, label='w_kl_loss_frac')\n",
    "        ax2.plot(w_disc_loss_frac, label='w_disc_loss_frac')\n",
    "\n",
    "        # Tweak fractional plot\n",
    "        # ax2.set_ylim((0,1))\n",
    "        ax2.set_ylabel('Approximate\\nTotal VAE Loss fraction')\n",
    "        # ax2.tick_params(axis='y', labelcolor='b')\n",
    "        ax2.legend()\n",
    "        ax2.grid()\n",
    "        ax2.set_xlabel('Epoch')\n",
    "\n",
    "        for key in keys:\n",
    "            if key == 'lr' or history.get(key) == None:\n",
    "                continue\n",
    "            ax.plot(np.abs(history[key]),\n",
    "                     label=key, \n",
    "                     linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\",\n",
    "                     marker= \"x\" if key[0:3] == 'val' else \"o\",\n",
    "                     markersize=6.5,\n",
    "                     color=color_key[key])\n",
    "    \n",
    "        # Customize the plot\n",
    "        ax.set_title(f'Training and Validation Losses, Attempt: {att_n} Run: {i}')\n",
    "        ax.set_ylabel('Loss')\n",
    "        # ax.tick_params(axis='y', labelcolor='r')\n",
    "        # ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(handles=clean_leg, title=\"â—‹ = train, x = val\")\n",
    "        plt.savefig(SAVE_PATH + f\"loss_plots/attempt{att_n}/\" + f\"loss_attempt_{att_n}_run_{i}.png\", bbox_inches='tight')\n",
    "        # plt.show()\n",
    "        plt.close(fig)\n",
    "    print(f\"Attempt {att_n} plotting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_anomaly_scores(data, encoder: keras.Model, AD_metric, debug = True):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     debug: Optional bool to skip latent space vectors that produce infinities.\n",
    "#     Currently set to true as it seems only 2 specific cases are affected\n",
    "#     \"\"\"\n",
    "#     dat_encoded = np.array(encoder.predict(data))[0] # This outputs shape (3, len(X_test), 3). Can't find satisfactory explanation for this behavior. (len(X_test), 3) makes sense. (3, len, 3) does not\n",
    "#     # Kenny only uses the first list so we'll follow that convention.\n",
    "#     # has shape (len(data), 3), where col 1 is z_mean, 2 is z_log_var and z. This is by design of encoder.\n",
    "#     scores = np.zeros(len(data))\n",
    "#     bad_model = False\n",
    "\n",
    "#     for i in range(len(scores)):\n",
    "#         z_mean, z_log_var = dat_encoded[i][0], dat_encoded[i][1]\n",
    "#         score = AD_metric(z_mean, z_log_var)\n",
    "#         if debug and (np.isinf(score) or np.isnan(score)):\n",
    "#             print(\"Unstable model: inf encountered. Rejecting Model\"\n",
    "#                   + f\"z_mean: {z_mean}\\n\"\n",
    "#                   + f\"z_log_var: {z_log_var}\")\n",
    "            \n",
    "#             bad_model = True\n",
    "#             break\n",
    "#         else:\n",
    "#             scores[i] = score\n",
    "\n",
    "#     return (scores, bad_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b138a4e-60ce-4e51-8b7a-d2e85d77defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 not in [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9774118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_preprocessed_snl()\n",
    "# X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161259d",
   "metadata": {},
   "source": [
    "##### Calculate Anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1addc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_truth_and_scores(encoder, AD_metric, debug=True):\n",
    "#     bg_score, bad_model = calc_anomaly_scores(X_test, encoder, AD_metric)\n",
    "#     scores = []\n",
    "#     truths = []\n",
    "#     zeros = np.zeros(len(X_test))\n",
    "#     signal_data = [leptoquark_data, Ato4l_data, hChToTauNu_data, hToTauTau_data]\n",
    "\n",
    "#     # Generate Truth and score lists ready for ROC curve calculation\n",
    "#     if not(bad_model):\n",
    "#         for dat in signal_data:\n",
    "#             truths.append(np.concatenate((zeros, np.ones(len(dat)))))\n",
    "\n",
    "#             # Now we are actually testing for bad models as we calculate the\n",
    "#             # anomaly scores for the other datasets\n",
    "#             s, bad_model = calc_anomaly_scores(dat, encoder, AD_metric, debug=debug)\n",
    "#             if bad_model: break\n",
    "#             scores.append(np.concatenate((bg_score,s) ))\n",
    "\n",
    "#     return (truths, scores, bad_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_rocs(truths, scores, fig_title):\n",
    "#     target_fpr = 1e-5\n",
    "#     tpr_at_target = []\n",
    "#     signal_names_tex = [ # latex version\n",
    "#                     \"Leptoquark\"\n",
    "#                     , \"$A\\\\rightarrow 4\\ell$\"\n",
    "#                     , \"$h^{\\pm}\\\\rightarrow\\\\tau \\\\nu$\"\n",
    "#                     , \"$h^0\\\\rightarrow\\\\tau\\\\tau$\"\n",
    "#                     ]\n",
    "#     signal_names_hum = [ # human readable\n",
    "#                     \"Leptoquark\"\n",
    "#                     ,\"A to 4L\"\n",
    "#                     , \"h to Tau Nu\"\n",
    "#                     , \"h to Tau Tau\"\n",
    "#                     ]\n",
    "#     fig, ax = plt.subplots()\n",
    "\n",
    "#     thresholds_at_target = []\n",
    "#     for truth, score, l in zip(truths, scores, signal_names_tex):\n",
    "#         fpr, tpr, thresholds = roc_curve(truth, score)\n",
    "#         auc = sk.roc_auc_score(truth, score)\n",
    "#         ax.plot(fpr, tpr, label=l + f\": {str(round(auc, 3))}\") # plot roc curve\n",
    "\n",
    "\n",
    "#         # Find tpr at fpr target\n",
    "#         idx = np.argmin(np.abs(fpr - target_fpr))\n",
    "#         tpr_at_target.append(tpr[idx])\n",
    "#         thresholds_at_target.append(thresholds[idx])\n",
    "\n",
    "#     ax.plot(np.linspace(0, 1, 1000), np.linspace(0, 1, 1000), \"--\")\n",
    "#     ax.vlines(10**-5, 0, 1, colors=\"r\", linestyles=\"dashed\")\n",
    "\n",
    "#     # Plot teaks\n",
    "#     ax.loglog()\n",
    "#     ax.legend()\n",
    "#     ax.grid()\n",
    "#     ax.set_xlabel(\"fpr\")\n",
    "#     ax.set_ylabel(\"tpr\")\n",
    "#     ax.set_title(fig_title) \n",
    "#     plt.show()\n",
    "\n",
    "#     for i in range(len(signal_names_hum)):\n",
    "#         print(signal_names_hum[i] + \" TPR @ FPR 10e-5 (%): \" + f\"{tpr_at_target[i]*100:.2f}\\n\" + f\"Target Threshold {thresholds_at_target[i]}\")\n",
    "\n",
    "#     return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7487486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TRAIN = False\n",
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_TRAIN = 10 # number of trained models\n",
    "bad_models = []\n",
    "SAVE_FIG = True\n",
    "FILE_NAMES = \"\"\n",
    "FIG_SAVE_PATH = SAVE_PATH + \"plots/\"\n",
    "AD_metric = AD_score_CKL \n",
    "\n",
    "if TRAIN:\n",
    "    for i in range(NUM_TRAIN):\n",
    "        new_enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        new_dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        new_disc = Qmake_discriminator(INPUT_SZ, 8, 2)\n",
    "        new_VAE = VAE_GAN_Model(new_enc, new_dec, new_disc)\n",
    "        save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "\n",
    "        new_VAE.load_weights(save_path)\n",
    "\n",
    "        just_enc = new_VAE.get_layer(\"encoder\") # We only need encoder output\n",
    "        t, s, bad_model = get_truth_and_scores(just_enc, AD_metric)\n",
    "        if bad_model:\n",
    "            print(f\"Unstable mode. Inf encountered. Model number {i} \")\n",
    "            bad_models.append(i)\n",
    "        else:\n",
    "            f = plot_rocs(t, s, \"ROC Curves using CKL as Anomaly Score. Model number: \" + str(i))\n",
    "\n",
    "            if SAVE_FIG:\n",
    "                Path(FIG_SAVE_PATH+FILE_NAMES).mkdir(parents=True, exist_ok=True)\n",
    "                f.savefig(FIG_SAVE_PATH + FILE_NAMES + f\"_iter_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae308f",
   "metadata": {},
   "source": [
    "After inspecting the graphs a few notable models remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "SAVE_FIG = False\n",
    "noteable_models_iter = [\n",
    "                        # 0,\n",
    "                        # 9,\n",
    "                        # 14,\n",
    "                        16 # best when considering DKL\n",
    "                        # 17\n",
    "                        ] # mins 88, 90, 89, 79 for AUC. \n",
    "# # 16 did the best AUC and I think also has higher TPR @ target FPR\n",
    "AD_metric = AD_score_CKL\n",
    "\n",
    "# for i in noteable_models_iter:\n",
    "\n",
    "#     # Reload the Encoder ------\n",
    "#     # for i in range(NUM_TRAIN):\n",
    "#     new_enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "#     new_dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "#     new_VAE = NNmodel(new_enc, new_dec)\n",
    "#     save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "\n",
    "#     new_VAE.load_weights(save_path)\n",
    "\n",
    "#     just_enc = new_VAE.get_layer(\"encoder\") # We only need encoder output\n",
    "#     # ------\n",
    "\n",
    "#     # Plot ROC curves and optionally save -----\n",
    "#     t, s, bad_model = get_truth_and_scores(just_enc, AD_metric)\n",
    "\n",
    "#     if bad_model:\n",
    "#         print(f\"Unstable mode. Inf encountered. Model number {i} \")\n",
    "#         bad_models.append(i)\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     f = plot_rocs(t, s, \"ROC Curves using $D_{KL}$ as Anomaly Score. Model number: \" + str(i))\n",
    "\n",
    "#     if SAVE_FIG:\n",
    "#         f.savefig(FIG_SAVE_PATH + FILE_NAMES + f\"_iter_{i}.png\")\n",
    "#     # ------\n",
    "\n",
    "#     # Reload history and plot loss curves ------\n",
    "#     save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "#     with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "#         history = pkl.load(f)\n",
    "#     # Extract the loss values\n",
    "#     total_loss = history['loss']\n",
    "#     reco_loss = history['reconstruction_loss']\n",
    "#     kl_loss = history['kl_loss']\n",
    "#     val_total_loss = history['val_loss']\n",
    "#     val_reco_loss = history['val_reconstruction_loss']\n",
    "#     val_kl_loss = history['val_kl_loss']\n",
    "\n",
    "#     # Create a new figure\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "\n",
    "#     # Plot training losses\n",
    "#     plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "#     plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "#     plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "#     plt.plot(history['beta'],label=\"beta\")\n",
    "\n",
    "#     # Plot validation losses\n",
    "#     plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "#     plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "#     plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "#     # Customize the plot\n",
    "#     plt.title(f'Training and Validation Losses Run: {i}')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.yscale(\"log\")\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "#     # -----\n",
    "\n",
    "#     # Plot Anomaly score distribution -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f7cd8",
   "metadata": {},
   "source": [
    "Anamoly Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_anomaly_dist(data, encoder: keras.Model, AD_metric):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     debug: Optional bool to skip latent space vectors that produce infinities.\n",
    "#     Currently set to true as it seems only 2 specific cases are affected\n",
    "#     \"\"\"\n",
    "#     dat_encoded = np.array(encoder.predict(data))[0] # This outputs shape (3, len(X_test), 3). Can't find satisfactory explanation for this behavior. (len(X_test), 3) makes sense. (3, len, 3) does not\n",
    "#     # Kenny only uses the first list so we'll follow that convention.\n",
    "#     # has shape (len(data), 3), where col 1 is z_mean, 2 is z_log_var and z. This is by design of encoder.\n",
    "#     scores = np.zeros(len(data))\n",
    "#     for i in range(len(scores)):\n",
    "#         z_mean, z_log_var = dat_encoded[i][0], dat_encoded[i][1]\n",
    "#         scores[i] = AD_metric(z_mean, z_log_var)\n",
    "    \n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_disc = Qmake_discriminator(INPUT_SZ, 8, 2)\n",
    "new_VAE = VAE_GAN_Model(new_enc, new_dec, new_disc)\n",
    "save_path = SAVE_PATH + f\"n_{5}/\" # 5 for CKL VAE-GAN\n",
    "\n",
    "new_VAE.load_weights(save_path)\n",
    "\n",
    "just_enc = new_VAE.get_layer(\"encoder\") # We only need encoder output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182753ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = [leptoquark_data, Ato4l_data, hChToTauNu_data, hToTauTau_data\n",
    "#                , X_train\n",
    "#                , X_test\n",
    "#                ] # Already defined.\n",
    "data_names_tex = [ # latex version\n",
    "                \"Leptoquark\"\n",
    "                , \"$A\\\\rightarrow 4\\ell$\"\n",
    "                , \"$h^{\\pm}\\\\rightarrow\\\\tau \\\\nu$\"\n",
    "                , \"$h^0\\\\rightarrow\\\\tau\\\\tau$\"\n",
    "                , \"Training Set (BG)\" # Background\n",
    "                , \"Test Set (BG)\" # Background\n",
    "                ]\n",
    "\n",
    "anomaly_scores = []\n",
    "for data in data:\n",
    "    s = calc_anomaly_dist(data, just_enc, AD_score_CKL)\n",
    "    anomaly_scores.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c285ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot setting for CKL\n",
    "bin_n = 125\n",
    "xlims = (0, 40)\n",
    "ylims = (0, 0.03)\n",
    "bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "xlabel = \"Clipped KL\"\n",
    "\n",
    "\n",
    "# # Investigating around the threshold at 161\n",
    "# ckl_roc_threshold = 161.84\n",
    "# bin_n = 10\n",
    "# l_margin = 10 \n",
    "# r_margin = 300\n",
    "\n",
    "# xlims = ( ckl_roc_threshold - l_margin , ckl_roc_threshold + r_margin)\n",
    "# ylims = (0, 0.01)\n",
    "# bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "# xlabel = \"Clipped KL\"\n",
    "\n",
    "# Plot settings for KL\n",
    "# bin_n = 125\n",
    "# xlims = (0, 40)\n",
    "# ylims = (0, 0.0125)\n",
    "# bins  = np.linspace(0, xlims[1], bin_n)\n",
    "# xlabel = \"KL Divergence\"\n",
    "\n",
    "for i in range(len(data_names_tex)):\n",
    "    dat = anomaly_scores[i]\n",
    "    # print(bin_n)\n",
    "    plt.hist(dat\n",
    "             , bins = bins\n",
    "             , label=data_names_tex[i] # + \" \" + str(bin_n)\n",
    "             , histtype = \"step\"\n",
    "             , density=True\n",
    "             )\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.vlines(ckl_roc_threshold, 0, 1)\n",
    "# plt.loglog()\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid()\n",
    "plt.ylim(ylims)\n",
    "plt.xlim(xlims)\n",
    "plt.title(\"Anomaly Score Distribution Across Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b7acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
