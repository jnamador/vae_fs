{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:00:44.375779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-11 16:00:45.500309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl, plot_rocs, calc_anomaly_dist, AD_score_KL, AD_score_CKL, get_truth_and_scores, eval_rocs, SIG_KEYS\n",
    "# from models import VAE_Model_ATLAS_beta as NNmodel\n",
    "\n",
    "\n",
    "# # Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d5f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTANTS IMPORTED:\n",
      "            NUM_TRAIN      = 10 # Number of iterations to train for.\n",
      "            # VAE Architecture\n",
      "            INPUT_SZ       = 57\n",
      "            H1_SZ          = 32 # Hidden layer 1 size\n",
      "            H2_SZ          = 16 # \"          \" 2 \"  \"\n",
      "            LATENT_SZ      = 3\n",
      "            # Discriminator Architecture # 8, 2 is on ATLAS-VAE-GAN\n",
      "            DISC_H1_SZ     = 8 # Size of first hidden layer of discriminator  \n",
      "            DISC_H2_SZ     = 2 # \"\" second hidden layer \"\"\n",
      "            # Training schedule and parameters\n",
      "            NUM_EPOCHS     = 100\n",
      "            STEPS_EPOCH    = 20 # Steps per epoch\n",
      "            BATCH_SIZE     = 1024\n",
      "            STOP_PATIENCE  = 40\n",
      "            LR_PATIENCE    = 20\n",
      "            LR             = 0.001 # Learning rate\n",
      "            REDUCE_LR_FACTOR = 0.5\n",
      "            VAL_SPLIT      = 0.2 # Validation split\n",
      "            CYCLE_LEN      = 20\n",
      "            SHUFFLE_BOOL   = True\n",
      "            # Hyperparameters\n",
      "            MIN_BETA       = 0\n",
      "            MAX_BETA       = 1\n",
      "            MIN_GAMMA      = 1\n",
      "            MAX_GAMMA      = 50\n"
     ]
    }
   ],
   "source": [
    "from gan_params import *\n",
    "print_base_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35a016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3d397",
   "metadata": {},
   "source": [
    "### Loss plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d9f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History Keys\n",
    "keys = [\n",
    "        'loss'               # VAE total loss term.\n",
    "        ,'reco_loss'         # VAE loss term\n",
    "        ,'kl_loss'           # VAE Loss term\n",
    "        ,'disc_loss'         # VAE loss due to discriminator \"failure to fool disc\"\n",
    "        # # ,'raw_loss'          # Reco_loss + kl_loss\n",
    "        ,'w_kl_loss'         # kl_loss * beta\n",
    "        ,'w_disc_loss'       # disc_loss * gamma\n",
    "        ,'d_loss'\n",
    "        # Validation version\n",
    "        ,'val_loss'          \n",
    "        ,'val_reco_loss'\n",
    "        ,'val_kl_loss'\n",
    "        ,'val_disc_loss'\n",
    "        # ,'val_raw_loss'\n",
    "        ,'val_w_kl_loss'\n",
    "        ,'val_w_disc_loss'\n",
    "        ,'val_d_loss'\n",
    "        # --\n",
    "        # ,'beta'              # hyperparameter\n",
    "        # ,'gamma'             # hyperparameter\n",
    "        # ,'val_gamma'         # hyperparameter\n",
    "        # ,'val_beta'          # hyperparameter\n",
    "        # ,'lr'              # learning rate\n",
    "        ]\n",
    "\n",
    "color_key = {\n",
    "             'loss' : 'k'               # VAE total loss term.\n",
    "            ,'val_loss' : 'k'         \n",
    "            ,'reco_loss': 'tab:blue'         # VAE loss term\n",
    "            ,'val_reco_loss' : 'tab:blue'\n",
    "            ,'kl_loss': 'crimson'          # VAE Loss term\n",
    "            ,'val_kl_loss': 'crimson'\n",
    "            ,'disc_loss' : 'c'        # VAE loss due to discriminator \"failure to fool disc\"\n",
    "            ,'val_disc_loss' : 'c'\n",
    "            ,'w_kl_loss'  : 'tab:orange'        # kl_loss * beta\n",
    "            ,'val_w_kl_loss' : 'tab:orange'\n",
    "            ,'w_disc_loss'  : 'tab:green'     # disc_loss * gamma\n",
    "            ,'val_w_disc_loss' : 'tab:green'\n",
    "            ,'d_loss': 'r'\n",
    "            ,'val_d_loss' :'r'\n",
    "        }\n",
    "# d_loss : discriminator loss\n",
    "# loss : generator total loss\n",
    "# raw_loss : reconstruction and kl_loss without beta weighting\n",
    "\n",
    "\n",
    "# # Generate cleaner legend\n",
    "# proxy_lines = {}\n",
    "# for key in keys:\n",
    "#     base_key = key.replace('val_', '')  # Strip 'val_' to group them\n",
    "#     if base_key not in proxy_lines and key in color_key:\n",
    "#         proxy_lines[base_key] = Line2D([0], [0], \n",
    "#                                     color=color_key[key], \n",
    "#                                     lw=2, \n",
    "#                                     label=base_key)\n",
    "# clean_leg = list(proxy_lines.values())\n",
    "# for att_n in range(6, 38): # plot all attempts. most recent is 18.\n",
    "#     att_path = SAVE_PATH + f\"attempt{att_n}/\"\n",
    "\n",
    "#     # Make folder for loss plots if it doesn't exist\n",
    "#     plot_dir = os.path.join(SAVE_PATH, f\"loss_plots/attempt{att_n}/\")\n",
    "#     os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "#     for i in range(10): # Currently only training 10 models at a time.\n",
    "#         save_path = att_path + f\"n_{i}/\"\n",
    "#         with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "#             history = pkl.load(f)\n",
    "    \n",
    "        \n",
    "#         # Plot training losses\n",
    "#         # fig, (ax, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(8,10))\n",
    "#         fig = plt.figure(figsize=(12, 8))\n",
    "#         gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])  # 3:1 means top gets 75%, bottom 25%\n",
    "\n",
    "#         ax = fig.add_subplot(gs[0])\n",
    "#         ax2 = fig.add_subplot(gs[1], sharex=ax)\n",
    "\n",
    "#         # Calculate fractional contributions to total VAE loss\n",
    "#         loss = np.array(history['loss'])\n",
    "#         reco_loss = np.array(history['reco_loss'])\n",
    "#         beta = np.array(history['beta'])\n",
    "#         reco_loss_frac = (reco_loss * (1 - beta))/loss\n",
    "\n",
    "#         w_kl_loss_frac = np.array(history['w_kl_loss'])/loss\n",
    "#         w_disc_loss_frac = np.array(history['w_disc_loss'])/loss\n",
    "#         ax2.plot(reco_loss_frac, label='reco_loss_frac')\n",
    "#         ax2.plot(w_kl_loss_frac, label='w_kl_loss_frac')\n",
    "#         ax2.plot(w_disc_loss_frac, label='w_disc_loss_frac')\n",
    "\n",
    "#         # Tweak fractional plot\n",
    "#         # ax2.set_ylim((0,1))\n",
    "#         ax2.set_ylabel('Approximate\\nTotal VAE Loss fraction')\n",
    "#         # ax2.tick_params(axis='y', labelcolor='b')\n",
    "#         ax2.legend()\n",
    "#         ax2.grid()\n",
    "#         ax2.set_xlabel('Epoch')\n",
    "\n",
    "#         for key in keys:\n",
    "#             if key == 'lr' or history.get(key) == None:\n",
    "#                 continue\n",
    "#             ax.plot(np.abs(history[key]),\n",
    "#                      label=key, \n",
    "#                      linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\",\n",
    "#                      marker= \"x\" if key[0:3] == 'val' else \"o\",\n",
    "#                      markersize=6.5,\n",
    "#                      color=color_key[key])\n",
    "    \n",
    "#         # Customize the plot\n",
    "#         ax.set_title(f'Training and Validation Losses, Attempt: {att_n} Run: {i}')\n",
    "#         ax.set_ylabel('Loss')\n",
    "#         # ax.tick_params(axis='y', labelcolor='r')\n",
    "#         # ax.legend()\n",
    "#         ax.grid(True)\n",
    "#         ax.set_yscale('log')\n",
    "#         ax.legend(handles=clean_leg, title=\"â—‹ = train, x = val\")\n",
    "#         plt.savefig(SAVE_PATH + f\"loss_plots/attempt{att_n}/\" + f\"loss_attempt_{att_n}_run_{i}.png\", bbox_inches='tight')\n",
    "#         # plt.show()\n",
    "#         plt.close(fig)\n",
    "#     print(f\"Attempt {att_n} plotting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_preprocessed_snl()\n",
    "# X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161259d",
   "metadata": {},
   "source": [
    "##### Calculate Anomaly scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae308f",
   "metadata": {},
   "source": [
    "After inspecting the graphs a few notable models remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57801db5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # mins 88, 90, 89, 79 for AUC. \n",
    "# # 16 did the best AUC and I think also has higher TPR @ target FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d8625",
   "metadata": {},
   "source": [
    "Generate ROC Curves for all trained iterations of the model. Save them and \n",
    "generate a list of of models with their AUC to rank them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5621/25000 [=====>........................] - ETA: 13s  "
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "new_enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "new_VAE = VAE_GAN_Model(new_enc, new_dec, new_disc)\n",
    "opt = keras.optimizers.Adam(learning_rate=LR) # These help silence benign warnings and is cleaner ----\n",
    "new_VAE.compile(optimizer=opt)                # ---\n",
    "\n",
    "roc_results = {}\n",
    "\n",
    "FIG_SAVE_PATH = SAVE_PATH + \"roc_plots/\"\n",
    "\n",
    "for att_n in range(17,28): # Splitting everythingup\n",
    "    roc_results = {}\n",
    "    bad_iters = {}\n",
    "\n",
    "    # Iterate through its iterations\n",
    "    fig_save_path = FIG_SAVE_PATH + f\"attempt{att_n}/\"\n",
    "\n",
    "    for i in range(NUM_TRAIN):\n",
    "        save_path = SAVE_PATH + f\"attempt{att_n}/n_{i}/\"\n",
    "\n",
    "        new_VAE.load_weights(save_path)\n",
    "        just_enc = new_VAE.get_layer(\"encoder\") # We only need encoder output\n",
    "\n",
    "        roc_perfs = eval_rocs(just_enc, data, AD_score_CKL)\n",
    "\n",
    "        if roc_perfs is None:\n",
    "            print(f\"Bad Iteration. Attempt: {att_n}, Iteration: {i}.\")\n",
    "            bad_iters.setdefault(f\"Attempt{att_n}\", []).append(i) # Create the entry and list if it doesn't exit, otherwise append it to the current list of that entry\n",
    "            continue\n",
    "        # Save its auc performance\n",
    "        roc_results[f\"Attempt{att_n}_iter_{i}\"] = {k: roc_perfs[k]['auc'] for k in SIG_KEYS.keys()}\n",
    "\n",
    "        # Commented out because we don't need to make the plots rn\n",
    "        # Make folder for roc plots if it doesn't exist\n",
    "        # plot_dir = os.path.join(fig_save_path)\n",
    "        # os.makedirs(plot_dir, exist_ok=True)\n",
    "        # f = plot_rocs(roc_perfs, f\"ROC Curves. CKL as Anomaly Score. Attempt: {att_n}, Iter: {i}\")\n",
    "        # f.savefig(fig_save_path + f\"roc_att{att_n}_iter{i}.png\", bbox_inches = 'tight')\n",
    "        # plt.close(f)\n",
    "        # print(f\"Roc curve plotted and saved. for Attempt: {att_n}, iter: {i}!\")\n",
    "\n",
    "        print(f\"Completed Attempt: {att_n} Iter: {i}\")\n",
    "        \n",
    "    with open(SAVE_PATH + f\"temp_pkls/roc_results_att_{att_n}.pkl\", \"wb\") as f:\n",
    "        pkl.dump([bad_iters, roc_results], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182753ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = [leptoquark_data, Ato4l_data, hChToTauNu_data, hToTauTau_data\n",
    "#                , X_train\n",
    "#                , X_test\n",
    "#                ] # Already defined.\n",
    "data_names_tex = [ # latex version\n",
    "                \"Leptoquark\"\n",
    "                , \"$A\\\\rightarrow 4\\ell$\"\n",
    "                , \"$h^{\\pm}\\\\rightarrow\\\\tau \\\\nu$\"\n",
    "                , \"$h^0\\\\rightarrow\\\\tau\\\\tau$\"\n",
    "                , \"Training Set (BG)\" # Background\n",
    "                , \"Test Set (BG)\" # Background\n",
    "                ]\n",
    "\n",
    "anomaly_scores = []\n",
    "for _, dat in data.items():\n",
    "    s = calc_anomaly_dist(dat, just_enc, AD_score_CKL)\n",
    "    anomaly_scores.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c285ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot setting for CKL\n",
    "bin_n = 125\n",
    "xlims = (0, 40)\n",
    "ylims = (0, 0.03)\n",
    "bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "xlabel = \"Clipped KL\"\n",
    "\n",
    "\n",
    "# # Investigating around the threshold at 161\n",
    "# ckl_roc_threshold = 161.84\n",
    "# bin_n = 10\n",
    "# l_margin = 10 \n",
    "# r_margin = 300\n",
    "\n",
    "# xlims = ( ckl_roc_threshold - l_margin , ckl_roc_threshold + r_margin)\n",
    "# ylims = (0, 0.01)\n",
    "# bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "# xlabel = \"Clipped KL\"\n",
    "\n",
    "# Plot settings for KL\n",
    "# bin_n = 125\n",
    "# xlims = (0, 40)\n",
    "# ylims = (0, 0.0125)\n",
    "# bins  = np.linspace(0, xlims[1], bin_n)\n",
    "# xlabel = \"KL Divergence\"\n",
    "\n",
    "for i in range(len(data_names_tex)):\n",
    "    dat = anomaly_scores[i]\n",
    "    # print(bin_n)\n",
    "    plt.hist(dat\n",
    "             , bins = bins\n",
    "             , label=data_names_tex[i] # + \" \" + str(bin_n)\n",
    "             , histtype = \"step\"\n",
    "             , density=True\n",
    "             )\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.vlines(ckl_roc_threshold, 0, 1)\n",
    "# plt.loglog()\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid()\n",
    "plt.ylim(ylims)\n",
    "plt.xlim(xlims)\n",
    "plt.title(\"Anomaly Score Distribution Across Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b7acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.12.0",
   "language": "python",
   "name": "tensorflow-2.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
