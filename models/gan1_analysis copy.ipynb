{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:29:44.051706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-11 14:29:50.127409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl, plot_rocs, calc_anomaly_dist, AD_score_KL, AD_score_CKL, get_truth_and_scores, eval_rocs, SIG_KEYS\n",
    "# from models import VAE_Model_ATLAS_beta as NNmodel\n",
    "\n",
    "\n",
    "# # Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d5f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTANTS IMPORTED:\n",
      "            NUM_TRAIN      = 10 # Number of iterations to train for.\n",
      "            # VAE Architecture\n",
      "            INPUT_SZ       = 57\n",
      "            H1_SZ          = 32 # Hidden layer 1 size\n",
      "            H2_SZ          = 16 # \"          \" 2 \"  \"\n",
      "            LATENT_SZ      = 3\n",
      "            # Discriminator Architecture # 8, 2 is on ATLAS-VAE-GAN\n",
      "            DISC_H1_SZ     = 8 # Size of first hidden layer of discriminator  \n",
      "            DISC_H2_SZ     = 2 # \"\" second hidden layer \"\"\n",
      "            # Training schedule and parameters\n",
      "            NUM_EPOCHS     = 100\n",
      "            STEPS_EPOCH    = 20 # Steps per epoch\n",
      "            BATCH_SIZE     = 1024\n",
      "            STOP_PATIENCE  = 40\n",
      "            LR_PATIENCE    = 20\n",
      "            LR             = 0.001 # Learning rate\n",
      "            REDUCE_LR_FACTOR = 0.5\n",
      "            VAL_SPLIT      = 0.2 # Validation split\n",
      "            CYCLE_LEN      = 20\n",
      "            SHUFFLE_BOOL   = True\n",
      "            # Hyperparameters\n",
      "            MIN_BETA       = 0\n",
      "            MAX_BETA       = 1\n",
      "            MIN_GAMMA      = 1\n",
      "            MAX_GAMMA      = 50\n"
     ]
    }
   ],
   "source": [
    "from gan_params import *\n",
    "print_base_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35a016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3d397",
   "metadata": {},
   "source": [
    "### Loss plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d9f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History Keys\n",
    "keys = [\n",
    "        'loss'               # VAE total loss term.\n",
    "        ,'reco_loss'         # VAE loss term\n",
    "        ,'kl_loss'           # VAE Loss term\n",
    "        ,'disc_loss'         # VAE loss due to discriminator \"failure to fool disc\"\n",
    "        # # ,'raw_loss'          # Reco_loss + kl_loss\n",
    "        ,'w_kl_loss'         # kl_loss * beta\n",
    "        ,'w_disc_loss'       # disc_loss * gamma\n",
    "        ,'d_loss'\n",
    "        # Validation version\n",
    "        ,'val_loss'          \n",
    "        ,'val_reco_loss'\n",
    "        ,'val_kl_loss'\n",
    "        ,'val_disc_loss'\n",
    "        # ,'val_raw_loss'\n",
    "        ,'val_w_kl_loss'\n",
    "        ,'val_w_disc_loss'\n",
    "        ,'val_d_loss'\n",
    "        # --\n",
    "        # ,'beta'              # hyperparameter\n",
    "        # ,'gamma'             # hyperparameter\n",
    "        # ,'val_gamma'         # hyperparameter\n",
    "        # ,'val_beta'          # hyperparameter\n",
    "        # ,'lr'              # learning rate\n",
    "        ]\n",
    "\n",
    "color_key = {\n",
    "             'loss' : 'k'               # VAE total loss term.\n",
    "            ,'val_loss' : 'k'         \n",
    "            ,'reco_loss': 'tab:blue'         # VAE loss term\n",
    "            ,'val_reco_loss' : 'tab:blue'\n",
    "            ,'kl_loss': 'crimson'          # VAE Loss term\n",
    "            ,'val_kl_loss': 'crimson'\n",
    "            ,'disc_loss' : 'c'        # VAE loss due to discriminator \"failure to fool disc\"\n",
    "            ,'val_disc_loss' : 'c'\n",
    "            ,'w_kl_loss'  : 'tab:orange'        # kl_loss * beta\n",
    "            ,'val_w_kl_loss' : 'tab:orange'\n",
    "            ,'w_disc_loss'  : 'tab:green'     # disc_loss * gamma\n",
    "            ,'val_w_disc_loss' : 'tab:green'\n",
    "            ,'d_loss': 'r'\n",
    "            ,'val_d_loss' :'r'\n",
    "        }\n",
    "# d_loss : discriminator loss\n",
    "# loss : generator total loss\n",
    "# raw_loss : reconstruction and kl_loss without beta weighting\n",
    "\n",
    "\n",
    "# # Generate cleaner legend\n",
    "# proxy_lines = {}\n",
    "# for key in keys:\n",
    "#     base_key = key.replace('val_', '')  # Strip 'val_' to group them\n",
    "#     if base_key not in proxy_lines and key in color_key:\n",
    "#         proxy_lines[base_key] = Line2D([0], [0], \n",
    "#                                     color=color_key[key], \n",
    "#                                     lw=2, \n",
    "#                                     label=base_key)\n",
    "# clean_leg = list(proxy_lines.values())\n",
    "# for att_n in range(6, 38): # plot all attempts. most recent is 18.\n",
    "#     att_path = SAVE_PATH + f\"attempt{att_n}/\"\n",
    "\n",
    "#     # Make folder for loss plots if it doesn't exist\n",
    "#     plot_dir = os.path.join(SAVE_PATH, f\"loss_plots/attempt{att_n}/\")\n",
    "#     os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "#     for i in range(10): # Currently only training 10 models at a time.\n",
    "#         save_path = att_path + f\"n_{i}/\"\n",
    "#         with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "#             history = pkl.load(f)\n",
    "    \n",
    "        \n",
    "#         # Plot training losses\n",
    "#         # fig, (ax, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(8,10))\n",
    "#         fig = plt.figure(figsize=(12, 8))\n",
    "#         gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])  # 3:1 means top gets 75%, bottom 25%\n",
    "\n",
    "#         ax = fig.add_subplot(gs[0])\n",
    "#         ax2 = fig.add_subplot(gs[1], sharex=ax)\n",
    "\n",
    "#         # Calculate fractional contributions to total VAE loss\n",
    "#         loss = np.array(history['loss'])\n",
    "#         reco_loss = np.array(history['reco_loss'])\n",
    "#         beta = np.array(history['beta'])\n",
    "#         reco_loss_frac = (reco_loss * (1 - beta))/loss\n",
    "\n",
    "#         w_kl_loss_frac = np.array(history['w_kl_loss'])/loss\n",
    "#         w_disc_loss_frac = np.array(history['w_disc_loss'])/loss\n",
    "#         ax2.plot(reco_loss_frac, label='reco_loss_frac')\n",
    "#         ax2.plot(w_kl_loss_frac, label='w_kl_loss_frac')\n",
    "#         ax2.plot(w_disc_loss_frac, label='w_disc_loss_frac')\n",
    "\n",
    "#         # Tweak fractional plot\n",
    "#         # ax2.set_ylim((0,1))\n",
    "#         ax2.set_ylabel('Approximate\\nTotal VAE Loss fraction')\n",
    "#         # ax2.tick_params(axis='y', labelcolor='b')\n",
    "#         ax2.legend()\n",
    "#         ax2.grid()\n",
    "#         ax2.set_xlabel('Epoch')\n",
    "\n",
    "#         for key in keys:\n",
    "#             if key == 'lr' or history.get(key) == None:\n",
    "#                 continue\n",
    "#             ax.plot(np.abs(history[key]),\n",
    "#                      label=key, \n",
    "#                      linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\",\n",
    "#                      marker= \"x\" if key[0:3] == 'val' else \"o\",\n",
    "#                      markersize=6.5,\n",
    "#                      color=color_key[key])\n",
    "    \n",
    "#         # Customize the plot\n",
    "#         ax.set_title(f'Training and Validation Losses, Attempt: {att_n} Run: {i}')\n",
    "#         ax.set_ylabel('Loss')\n",
    "#         # ax.tick_params(axis='y', labelcolor='r')\n",
    "#         # ax.legend()\n",
    "#         ax.grid(True)\n",
    "#         ax.set_yscale('log')\n",
    "#         ax.legend(handles=clean_leg, title=\"â—‹ = train, x = val\")\n",
    "#         plt.savefig(SAVE_PATH + f\"loss_plots/attempt{att_n}/\" + f\"loss_attempt_{att_n}_run_{i}.png\", bbox_inches='tight')\n",
    "#         # plt.show()\n",
    "#         plt.close(fig)\n",
    "#     print(f\"Attempt {att_n} plotting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_preprocessed_snl()\n",
    "# X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161259d",
   "metadata": {},
   "source": [
    "##### Calculate Anomaly scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae308f",
   "metadata": {},
   "source": [
    "After inspecting the graphs a few notable models remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57801db5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # mins 88, 90, 89, 79 for AUC. \n",
    "# # 16 did the best AUC and I think also has higher TPR @ target FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d8625",
   "metadata": {},
   "source": [
    "Generate ROC Curves for all trained iterations of the model. Save them and \n",
    "generate a list of of models with their AUC to rank them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:33:26.162288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "2025-08-11 14:33:34.173779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 16s 627us/step\n",
      "1750/1750 [==============================] - 1s 648us/step\n",
      "21603/21603 [==============================] - 14s 632us/step\n",
      "23759/23759 [==============================] - 15s 627us/step\n",
      "10642/10642 [==============================] - 7s 617us/step\n",
      "Completed Attempt: 6 Iter: 0\n",
      "25000/25000 [==============================] - 16s 626us/step\n",
      "Unstable model: inf or nan encountered. Rejecting Modelz_mean: nan\n",
      "z_log_var: nan\n",
      "Bad Iteration. Attempt: 6, Iteration: 1.\n",
      "25000/25000 [==============================] - 16s 629us/step\n",
      "1750/1750 [==============================] - 1s 617us/step\n",
      "21603/21603 [==============================] - 14s 629us/step\n",
      "23759/23759 [==============================] - 15s 610us/step\n",
      "10642/10642 [==============================] - 7s 613us/step\n",
      "Completed Attempt: 6 Iter: 2\n",
      " 7193/25000 [=======>......................] - ETA: 10s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m new_VAE\u001b[38;5;241m.\u001b[39mload_weights(save_path)\n\u001b[1;32m     23\u001b[0m just_enc \u001b[38;5;241m=\u001b[39m new_VAE\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# We only need encoder output\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m roc_perfs \u001b[38;5;241m=\u001b[39m \u001b[43meval_rocs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjust_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAD_score_CKL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m roc_perfs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad Iteration. Attempt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00matt_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projs/VAE_FS/models/data_and_eval_utils.py:159\u001b[0m, in \u001b[0;36meval_rocs\u001b[0;34m(encoder, data, AD_metric, target_fpr)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_rocs\u001b[39m(encoder, data, AD_metric, target_fpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the ROC Curves for the encoder\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Returns: dict | None. dict is structured as {sig_name: {get_roc_perfomance output}}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     truths, scores, bad_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_truth_and_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAD_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     roc_perfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bad_model:\n",
      "File \u001b[0;32m~/projs/VAE_FS/models/data_and_eval_utils.py:78\u001b[0m, in \u001b[0;36mget_truth_and_scores\u001b[0;34m(encoder, ad_metric, data, debug)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# unpack once for readability\u001b[39;00m\n\u001b[1;32m     76\u001b[0m X_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 78\u001b[0m bg_score, bad_model \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_anomaly_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mad_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m truths, scores \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     81\u001b[0m zeros \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(X_test))\n",
      "File \u001b[0;32m~/projs/VAE_FS/models/data_and_eval_utils.py:105\u001b[0m, in \u001b[0;36mcalc_anomaly_scores\u001b[0;34m(data, encoder, AD_metric, debug)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_anomaly_scores\u001b[39m(data, encoder: keras\u001b[38;5;241m.\u001b[39mModel, AD_metric, debug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    -----------\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    debug: Optional bool to skip latent space vectors that produce infinities.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Currently set to true as it seems only 2 specific cases are affected\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     dat_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# This outputs shape (3, len(X_test), 3). Can't find satisfactory explanation for this behavior. (len(X_test), 3) makes sense. (3, len, 3) does not\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Kenny only uses the first list so we'll follow that convention.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# has shape (len(data), 3), where col 1 is z_mean, 2 is z_log_var and z. This is by design of encoder.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py:2382\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   2381\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2382\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2384\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:902\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    898\u001b[0m   execution_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotTraced\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m without_tracing \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraced\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m   tm\u001b[38;5;241m.\u001b[39mset_metadata(tf_function_call\u001b[38;5;241m=\u001b[39mexecution_mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m compiler,\n\u001b[1;32m    900\u001b[0m                   tracing_count\u001b[38;5;241m=\u001b[39mnew_tracing_count)\n\u001b[0;32m--> 902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    903\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m without_tracing:\n\u001b[1;32m    904\u001b[0m     _frequent_tracing_detector_manager\u001b[38;5;241m.\u001b[39mcalled_without_tracing(\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_for_call_stats)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/context.py:2224\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n\u001b[0;32m-> 2224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/context.py:978\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    977\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread_local_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_eager\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "new_enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "new_disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "new_VAE = VAE_GAN_Model(new_enc, new_dec, new_disc)\n",
    "opt = keras.optimizers.Adam(learning_rate=LR) # These help silence benign warnings and is cleaner ----\n",
    "new_VAE.compile(optimizer=opt)                # ---\n",
    "\n",
    "roc_results = {}\n",
    "\n",
    "FIG_SAVE_PATH = SAVE_PATH + \"roc_plots/\"\n",
    "\n",
    "bad_iters = {}\n",
    "for att_n in range(17,28): # Splitting everythingup\n",
    "\n",
    "    # Iterate through its iterations\n",
    "    fig_save_path = FIG_SAVE_PATH + f\"attempt{att_n}/\"\n",
    "\n",
    "    for i in range(NUM_TRAIN):\n",
    "        save_path = SAVE_PATH + f\"attempt{att_n}/n_{i}/\"\n",
    "\n",
    "        new_VAE.load_weights(save_path)\n",
    "        just_enc = new_VAE.get_layer(\"encoder\") # We only need encoder output\n",
    "\n",
    "        roc_perfs = eval_rocs(just_enc, data, AD_score_CKL)\n",
    "\n",
    "        if roc_perfs is None:\n",
    "            print(f\"Bad Iteration. Attempt: {att_n}, Iteration: {i}.\")\n",
    "            bad_iters.setdefault(f\"Attempt{att_n}\", []).append(i) # Create the entry and list if it doesn't exit, otherwise append it to the current list of that entry\n",
    "            continue\n",
    "        # Save its auc performance\n",
    "        roc_results[f\"Attempt{att_n}_iter_{i}\"] = {k: roc_perfs[k]['auc'] for k in SIG_KEYS.keys()}\n",
    "\n",
    "        # Commented out because we don't need to make the plots rn\n",
    "        # Make folder for roc plots if it doesn't exist\n",
    "        # plot_dir = os.path.join(fig_save_path)\n",
    "        # os.makedirs(plot_dir, exist_ok=True)\n",
    "        # f = plot_rocs(roc_perfs, f\"ROC Curves. CKL as Anomaly Score. Attempt: {att_n}, Iter: {i}\")\n",
    "        # f.savefig(fig_save_path + f\"roc_att{att_n}_iter{i}.png\", bbox_inches = 'tight')\n",
    "        # plt.close(f)\n",
    "        # print(f\"Roc curve plotted and saved. for Attempt: {att_n}, iter: {i}!\")\n",
    "\n",
    "        print(f\"Completed Attempt: {att_n} Iter: {i}\")\n",
    "\n",
    "temp = pd.DataFrame(roc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(SAVE_PATH + \"tempgpu1.pkl\", \"wb\") as f:\n",
    "    pkl.dump([bad_iters, roc_results], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182753ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = [leptoquark_data, Ato4l_data, hChToTauNu_data, hToTauTau_data\n",
    "#                , X_train\n",
    "#                , X_test\n",
    "#                ] # Already defined.\n",
    "data_names_tex = [ # latex version\n",
    "                \"Leptoquark\"\n",
    "                , \"$A\\\\rightarrow 4\\ell$\"\n",
    "                , \"$h^{\\pm}\\\\rightarrow\\\\tau \\\\nu$\"\n",
    "                , \"$h^0\\\\rightarrow\\\\tau\\\\tau$\"\n",
    "                , \"Training Set (BG)\" # Background\n",
    "                , \"Test Set (BG)\" # Background\n",
    "                ]\n",
    "\n",
    "anomaly_scores = []\n",
    "for _, dat in data.items():\n",
    "    s = calc_anomaly_dist(dat, just_enc, AD_score_CKL)\n",
    "    anomaly_scores.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c285ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot setting for CKL\n",
    "bin_n = 125\n",
    "xlims = (0, 40)\n",
    "ylims = (0, 0.03)\n",
    "bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "xlabel = \"Clipped KL\"\n",
    "\n",
    "\n",
    "# # Investigating around the threshold at 161\n",
    "# ckl_roc_threshold = 161.84\n",
    "# bin_n = 10\n",
    "# l_margin = 10 \n",
    "# r_margin = 300\n",
    "\n",
    "# xlims = ( ckl_roc_threshold - l_margin , ckl_roc_threshold + r_margin)\n",
    "# ylims = (0, 0.01)\n",
    "# bins  = np.linspace(xlims[0], xlims[1], bin_n)\n",
    "# xlabel = \"Clipped KL\"\n",
    "\n",
    "# Plot settings for KL\n",
    "# bin_n = 125\n",
    "# xlims = (0, 40)\n",
    "# ylims = (0, 0.0125)\n",
    "# bins  = np.linspace(0, xlims[1], bin_n)\n",
    "# xlabel = \"KL Divergence\"\n",
    "\n",
    "for i in range(len(data_names_tex)):\n",
    "    dat = anomaly_scores[i]\n",
    "    # print(bin_n)\n",
    "    plt.hist(dat\n",
    "             , bins = bins\n",
    "             , label=data_names_tex[i] # + \" \" + str(bin_n)\n",
    "             , histtype = \"step\"\n",
    "             , density=True\n",
    "             )\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.vlines(ckl_roc_threshold, 0, 1)\n",
    "# plt.loglog()\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid()\n",
    "plt.ylim(ylims)\n",
    "plt.xlim(xlims)\n",
    "plt.title(\"Anomaly Score Distribution Across Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b7acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
