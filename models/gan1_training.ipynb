{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 08:51:53.616394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-25 08:51:57.102217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 20\n",
    "LR_PATIENCE = 8\n",
    "DISC_H1_SZ = 8 # Size of first hidden layer of discriminator  # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H2_SZ = 2 # \"\" second hidden layer \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 08:53:12.240754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 08:53:14.866799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-07-25 08:53:15.112622: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f71d402c940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-25 08:53:15.112658: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-25 08:53:15.147366: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-25 08:53:15.217039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-07-25 08:53:15.408569: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 14s 4ms/step - loss: 48.9026 - reco_loss: 1.0050 - kl_loss: 0.7954 - disc_loss: 1.4964 - beta: 0.9900 - raw_loss: 1.8036 - w_kl_loss: 0.5963 - w_disc_loss: 92.9648 - gamma: 62.2500 - val_loss: 187.0480 - val_reco_loss: 3.7131 - val_kl_loss: 0.8554 - val_raw_loss: 4.5685 - val_disc_loss: 1.4782 - val_w_kl_loss: 0.8468 - val_w_disc_loss: 182.4880 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 227.2693 - reco_loss: 0.8293 - kl_loss: 0.8081 - disc_loss: 1.4650 - beta: 1.0000 - raw_loss: 1.6267 - w_kl_loss: 0.6057 - w_disc_loss: 270.5310 - gamma: 184.7500 - val_loss: 357.6013 - val_reco_loss: 3.6347 - val_kl_loss: 0.8518 - val_raw_loss: 4.4864 - val_disc_loss: 1.4357 - val_w_kl_loss: 0.8518 - val_w_disc_loss: 353.1149 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 399.9905 - reco_loss: 0.9898 - kl_loss: 0.8044 - disc_loss: 1.4402 - beta: 0.9900 - raw_loss: 1.7272 - w_kl_loss: 0.5488 - w_disc_loss: 442.3925 - gamma: 307.2500 - val_loss: 518.3881 - val_reco_loss: 3.6733 - val_kl_loss: 0.8482 - val_raw_loss: 4.5215 - val_disc_loss: 1.3947 - val_w_kl_loss: 0.8397 - val_w_disc_loss: 513.8751 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 567.4171 - reco_loss: 0.7779 - kl_loss: 0.8212 - disc_loss: 1.4180 - beta: 1.0000 - raw_loss: 1.5688 - w_kl_loss: 0.5996 - w_disc_loss: 609.2882 - gamma: 429.7500 - val_loss: 694.7997 - val_reco_loss: 3.6943 - val_kl_loss: 0.8454 - val_raw_loss: 4.5397 - val_disc_loss: 1.4060 - val_w_kl_loss: 0.8454 - val_w_disc_loss: 690.2599 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 731.0713 - reco_loss: 1.0773 - kl_loss: 0.7892 - disc_loss: 1.3982 - beta: 0.9900 - raw_loss: 1.8862 - w_kl_loss: 0.6066 - w_disc_loss: 772.0168 - gamma: 552.2500 - val_loss: 835.5444 - val_reco_loss: 3.5468 - val_kl_loss: 0.8427 - val_raw_loss: 4.3895 - val_disc_loss: 1.3549 - val_w_kl_loss: 0.8343 - val_w_disc_loss: 831.1633 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 888.7836 - reco_loss: 0.8311 - kl_loss: 0.7933 - disc_loss: 1.3775 - beta: 1.0000 - raw_loss: 1.6287 - w_kl_loss: 0.6024 - w_disc_loss: 929.3534 - gamma: 674.7500 - val_loss: 975.3682 - val_reco_loss: 3.5464 - val_kl_loss: 0.8403 - val_raw_loss: 4.3867 - val_disc_loss: 1.3194 - val_w_kl_loss: 0.8403 - val_w_disc_loss: 970.9815 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1043.0889 - reco_loss: 1.1058 - kl_loss: 0.7656 - disc_loss: 1.3584 - beta: 0.9900 - raw_loss: 1.8537 - w_kl_loss: 0.5589 - w_disc_loss: 1082.8516 - gamma: 797.2500 - val_loss: 1150.7816 - val_reco_loss: 3.4679 - val_kl_loss: 0.8381 - val_raw_loss: 4.3061 - val_disc_loss: 1.3355 - val_w_kl_loss: 0.8298 - val_w_disc_loss: 1146.4839 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1191.9003 - reco_loss: 0.7710 - kl_loss: 0.8110 - disc_loss: 1.3389 - beta: 1.0000 - raw_loss: 1.5729 - w_kl_loss: 0.6114 - w_disc_loss: 1231.3576 - gamma: 919.7500 - val_loss: 1308.4048 - val_reco_loss: 3.5068 - val_kl_loss: 0.8362 - val_raw_loss: 4.3430 - val_disc_loss: 1.3294 - val_w_kl_loss: 0.8362 - val_w_disc_loss: 1304.0618 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2490/2500 [============================>.] - ETA: 0s - loss: 1337.1190 - reco_loss: 1.1000 - kl_loss: 0.7630 - disc_loss: 1.3202 - beta: 0.8900 - raw_loss: 1.8269 - w_kl_loss: 0.5413 - w_disc_loss: 1375.5553 - gamma: 1041.9805\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1337.2792 - reco_loss: 1.0995 - kl_loss: 0.7628 - disc_loss: 1.3202 - beta: 0.9900 - raw_loss: 1.8265 - w_kl_loss: 0.5421 - w_disc_loss: 1375.8934 - gamma: 1042.2500 - val_loss: 1432.7783 - val_reco_loss: 3.5908 - val_kl_loss: 0.8342 - val_raw_loss: 4.4251 - val_disc_loss: 1.2944 - val_w_kl_loss: 0.8259 - val_w_disc_loss: 1428.3616 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1480.5494 - reco_loss: 0.8322 - kl_loss: 0.8001 - disc_loss: 1.3041 - beta: 1.0000 - raw_loss: 1.5778 - w_kl_loss: 0.5666 - w_disc_loss: 1518.9124 - gamma: 1164.7500 - val_loss: 1622.5912 - val_reco_loss: 3.5896 - val_kl_loss: 0.8334 - val_raw_loss: 4.4230 - val_disc_loss: 1.3199 - val_w_kl_loss: 0.8334 - val_w_disc_loss: 1618.1682 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1631.8664 - reco_loss: 1.0659 - kl_loss: 0.8046 - disc_loss: 1.2972 - beta: 0.9900 - raw_loss: 1.8536 - w_kl_loss: 0.5875 - w_disc_loss: 1669.7457 - gamma: 1287.2500 - val_loss: 1722.2168 - val_reco_loss: 3.6371 - val_kl_loss: 0.8324 - val_raw_loss: 4.4695 - val_disc_loss: 1.2739 - val_w_kl_loss: 0.8240 - val_w_disc_loss: 1717.7556 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1775.6452 - reco_loss: 0.8144 - kl_loss: 0.8145 - disc_loss: 1.2864 - beta: 1.0000 - raw_loss: 1.5664 - w_kl_loss: 0.5708 - w_disc_loss: 1813.4794 - gamma: 1409.7499 - val_loss: 1943.4487 - val_reco_loss: 3.6422 - val_kl_loss: 0.8317 - val_raw_loss: 4.4739 - val_disc_loss: 1.3182 - val_w_kl_loss: 0.8317 - val_w_disc_loss: 1938.9749 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1922.4228 - reco_loss: 1.1093 - kl_loss: 0.7964 - disc_loss: 1.2790 - beta: 0.9900 - raw_loss: 1.8904 - w_kl_loss: 0.5837 - w_disc_loss: 1959.7112 - gamma: 1532.2499 - val_loss: 2014.8940 - val_reco_loss: 3.6110 - val_kl_loss: 0.8311 - val_raw_loss: 4.4421 - val_disc_loss: 1.2617 - val_w_kl_loss: 0.8228 - val_w_disc_loss: 2010.4602 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2063.4564 - reco_loss: 0.8129 - kl_loss: 0.7949 - disc_loss: 1.2696 - beta: 1.0000 - raw_loss: 1.5337 - w_kl_loss: 0.5473 - w_disc_loss: 2100.8082 - gamma: 1654.7499 - val_loss: 2150.0659 - val_reco_loss: 3.5390 - val_kl_loss: 0.8305 - val_raw_loss: 4.3695 - val_disc_loss: 1.2504 - val_w_kl_loss: 0.8305 - val_w_disc_loss: 2145.6963 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2204.6388 - reco_loss: 1.0743 - kl_loss: 0.7747 - disc_loss: 1.2612 - beta: 0.9900 - raw_loss: 1.7656 - w_kl_loss: 0.5153 - w_disc_loss: 2241.5039 - gamma: 1777.2499 - val_loss: 2316.9165 - val_reco_loss: 3.5946 - val_kl_loss: 0.8300 - val_raw_loss: 4.4246 - val_disc_loss: 1.2579 - val_w_kl_loss: 0.8217 - val_w_disc_loss: 2312.5002 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2342.6359 - reco_loss: 0.8850 - kl_loss: 0.8223 - disc_loss: 1.2524 - beta: 1.0000 - raw_loss: 1.8118 - w_kl_loss: 0.6978 - w_disc_loss: 2379.1924 - gamma: 1899.7499 - val_loss: 2488.2646 - val_reco_loss: 3.5696 - val_kl_loss: 0.8291 - val_raw_loss: 4.3986 - val_disc_loss: 1.2667 - val_w_kl_loss: 0.8291 - val_w_disc_loss: 2483.8660 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2494/2500 [============================>.] - ETA: 0s - loss: 2478.1294 - reco_loss: 1.1140 - kl_loss: 0.7880 - disc_loss: 1.2434 - beta: 0.9300 - raw_loss: 1.9032 - w_kl_loss: 0.5869 - w_disc_loss: 2514.2133 - gamma: 2022.0785\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2478.2262 - reco_loss: 1.1136 - kl_loss: 0.7878 - disc_loss: 1.2434 - beta: 0.9900 - raw_loss: 1.9028 - w_kl_loss: 0.5874 - w_disc_loss: 2514.4172 - gamma: 2022.2499 - val_loss: 2565.0642 - val_reco_loss: 3.5941 - val_kl_loss: 0.8285 - val_raw_loss: 4.4226 - val_disc_loss: 1.2290 - val_w_kl_loss: 0.8202 - val_w_disc_loss: 2560.6499 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2616.8299 - reco_loss: 0.8966 - kl_loss: 0.7873 - disc_loss: 1.2370 - beta: 1.0000 - raw_loss: 1.7503 - w_kl_loss: 0.6495 - w_disc_loss: 2652.9674 - gamma: 2144.7499 - val_loss: 2761.2830 - val_reco_loss: 3.5231 - val_kl_loss: 0.8283 - val_raw_loss: 4.3514 - val_disc_loss: 1.2498 - val_w_kl_loss: 0.8283 - val_w_disc_loss: 2756.9316 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2758.0254 - reco_loss: 1.2142 - kl_loss: 0.7591 - disc_loss: 1.2323 - beta: 0.9900 - raw_loss: 1.9276 - w_kl_loss: 0.5314 - w_disc_loss: 2793.8481 - gamma: 2267.2499 - val_loss: 2913.0806 - val_reco_loss: 3.5880 - val_kl_loss: 0.8283 - val_raw_loss: 4.4163 - val_disc_loss: 1.2492 - val_w_kl_loss: 0.8200 - val_w_disc_loss: 2908.6726 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2900.0813 - reco_loss: 0.7737 - kl_loss: 0.8110 - disc_loss: 1.2286 - beta: 1.0000 - raw_loss: 1.6015 - w_kl_loss: 0.6317 - w_disc_loss: 2936.1005 - gamma: 2389.7499 - val_loss: 2992.4224 - val_reco_loss: 3.6056 - val_kl_loss: 0.8282 - val_raw_loss: 4.4338 - val_disc_loss: 1.2191 - val_w_kl_loss: 0.8282 - val_w_disc_loss: 2987.9885 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3045.3777 - reco_loss: 1.1249 - kl_loss: 0.7823 - disc_loss: 1.2264 - beta: 0.9900 - raw_loss: 1.9463 - w_kl_loss: 0.6120 - w_disc_loss: 3080.9919 - gamma: 2512.2499 - val_loss: 3150.3472 - val_reco_loss: 3.5706 - val_kl_loss: 0.8280 - val_raw_loss: 4.3986 - val_disc_loss: 1.2225 - val_w_kl_loss: 0.8197 - val_w_disc_loss: 3145.9568 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 20.2947 - reco_loss: 1.1022 - kl_loss: 0.5288 - disc_loss: 0.5936 - beta: 0.9900 - raw_loss: 1.6055 - w_kl_loss: 0.3749 - w_disc_loss: 36.8797 - gamma: 62.2500 - val_loss: 76.9971 - val_reco_loss: 4.7806 - val_kl_loss: 1.1456 - val_raw_loss: 5.9261 - val_disc_loss: 0.5758 - val_w_kl_loss: 1.1341 - val_w_disc_loss: 71.0824 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 91.0046 - reco_loss: 0.8734 - kl_loss: 0.5396 - disc_loss: 0.5815 - beta: 1.0000 - raw_loss: 1.4294 - w_kl_loss: 0.4224 - w_disc_loss: 107.3870 - gamma: 184.7500 - val_loss: 150.6682 - val_reco_loss: 4.6476 - val_kl_loss: 1.1243 - val_raw_loss: 5.7718 - val_disc_loss: 0.5891 - val_w_kl_loss: 1.1243 - val_w_disc_loss: 144.8963 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 159.9181 - reco_loss: 1.0467 - kl_loss: 0.5313 - disc_loss: 0.5728 - beta: 0.9900 - raw_loss: 1.5364 - w_kl_loss: 0.3652 - w_disc_loss: 175.9280 - gamma: 307.2500 - val_loss: 217.7052 - val_reco_loss: 4.5813 - val_kl_loss: 1.1066 - val_raw_loss: 5.6879 - val_disc_loss: 0.5755 - val_w_kl_loss: 1.0955 - val_w_disc_loss: 212.0283 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 226.7336 - reco_loss: 0.8652 - kl_loss: 0.5345 - disc_loss: 0.5647 - beta: 1.0000 - raw_loss: 1.3910 - w_kl_loss: 0.3986 - w_disc_loss: 242.6476 - gamma: 429.7500 - val_loss: 287.7434 - val_reco_loss: 4.6255 - val_kl_loss: 1.0882 - val_raw_loss: 5.7137 - val_disc_loss: 0.5745 - val_w_kl_loss: 1.0882 - val_w_disc_loss: 282.0297 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 292.0432 - reco_loss: 1.1213 - kl_loss: 0.5210 - disc_loss: 0.5569 - beta: 0.9900 - raw_loss: 1.6114 - w_kl_loss: 0.3653 - w_disc_loss: 307.4928 - gamma: 552.2500 - val_loss: 345.0408 - val_reco_loss: 4.5661 - val_kl_loss: 1.0718 - val_raw_loss: 5.6379 - val_disc_loss: 0.5533 - val_w_kl_loss: 1.0611 - val_w_disc_loss: 339.4136 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 354.8690 - reco_loss: 0.8501 - kl_loss: 0.5328 - disc_loss: 0.5488 - beta: 1.0000 - raw_loss: 1.4008 - w_kl_loss: 0.4183 - w_disc_loss: 370.2845 - gamma: 674.7500 - val_loss: 413.3268 - val_reco_loss: 4.5831 - val_kl_loss: 1.0562 - val_raw_loss: 5.6393 - val_disc_loss: 0.5540 - val_w_kl_loss: 1.0562 - val_w_disc_loss: 407.6874 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 416.4349 - reco_loss: 1.0986 - kl_loss: 0.5014 - disc_loss: 0.5412 - beta: 0.9900 - raw_loss: 1.5835 - w_kl_loss: 0.3617 - w_disc_loss: 431.4294 - gamma: 797.2500 - val_loss: 466.0541 - val_reco_loss: 4.5414 - val_kl_loss: 1.0412 - val_raw_loss: 5.5826 - val_disc_loss: 0.5364 - val_w_kl_loss: 1.0308 - val_w_disc_loss: 460.4819 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 476.0215 - reco_loss: 0.7904 - kl_loss: 0.5336 - disc_loss: 0.5340 - beta: 1.0000 - raw_loss: 1.3111 - w_kl_loss: 0.3944 - w_disc_loss: 491.0691 - gamma: 919.7500 - val_loss: 520.8947 - val_reco_loss: 4.3250 - val_kl_loss: 1.0268 - val_raw_loss: 5.3518 - val_disc_loss: 0.5256 - val_w_kl_loss: 1.0268 - val_w_disc_loss: 515.5430 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2493/2500 [============================>.] - ETA: 0s - loss: 534.3705 - reco_loss: 1.0416 - kl_loss: 0.5073 - disc_loss: 0.5268 - beta: 0.9200 - raw_loss: 1.5358 - w_kl_loss: 0.3676 - w_disc_loss: 548.9221 - gamma: 1042.0540\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 534.4151 - reco_loss: 1.0413 - kl_loss: 0.5071 - disc_loss: 0.5268 - beta: 0.9900 - raw_loss: 1.5355 - w_kl_loss: 0.3680 - w_disc_loss: 549.0184 - gamma: 1042.2500 - val_loss: 601.2720 - val_reco_loss: 4.4245 - val_kl_loss: 1.0111 - val_raw_loss: 5.4357 - val_disc_loss: 0.5400 - val_w_kl_loss: 1.0010 - val_w_disc_loss: 595.8465 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 592.1290 - reco_loss: 0.8518 - kl_loss: 0.5200 - disc_loss: 0.5209 - beta: 1.0000 - raw_loss: 1.3731 - w_kl_loss: 0.3951 - w_disc_loss: 606.7113 - gamma: 1164.7500 - val_loss: 666.6166 - val_reco_loss: 4.3801 - val_kl_loss: 1.0042 - val_raw_loss: 5.3843 - val_disc_loss: 0.5394 - val_w_kl_loss: 1.0042 - val_w_disc_loss: 661.2323 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 651.2404 - reco_loss: 1.0709 - kl_loss: 0.5007 - disc_loss: 0.5170 - beta: 0.9900 - raw_loss: 1.5451 - w_kl_loss: 0.3534 - w_disc_loss: 665.5325 - gamma: 1287.2500 - val_loss: 704.7347 - val_reco_loss: 4.5025 - val_kl_loss: 0.9967 - val_raw_loss: 5.4992 - val_disc_loss: 0.5186 - val_w_kl_loss: 0.9867 - val_w_disc_loss: 699.2455 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 708.6878 - reco_loss: 0.8184 - kl_loss: 0.5192 - disc_loss: 0.5129 - beta: 1.0000 - raw_loss: 1.3462 - w_kl_loss: 0.4004 - w_disc_loss: 723.0518 - gamma: 1409.7499 - val_loss: 773.3875 - val_reco_loss: 4.3909 - val_kl_loss: 0.9888 - val_raw_loss: 5.3797 - val_disc_loss: 0.5221 - val_w_kl_loss: 0.9888 - val_w_disc_loss: 768.0078 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 766.5369 - reco_loss: 1.0406 - kl_loss: 0.5041 - disc_loss: 0.5095 - beta: 0.9900 - raw_loss: 1.5429 - w_kl_loss: 0.3741 - w_disc_loss: 780.6017 - gamma: 1532.2499 - val_loss: 803.3342 - val_reco_loss: 4.5042 - val_kl_loss: 0.9831 - val_raw_loss: 5.4873 - val_disc_loss: 0.5007 - val_w_kl_loss: 0.9733 - val_w_disc_loss: 797.8568 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 823.7763 - reco_loss: 0.8521 - kl_loss: 0.5180 - disc_loss: 0.5064 - beta: 1.0000 - raw_loss: 1.3862 - w_kl_loss: 0.4067 - w_disc_loss: 837.8984 - gamma: 1654.7499 - val_loss: 880.3547 - val_reco_loss: 4.0826 - val_kl_loss: 0.9749 - val_raw_loss: 5.0575 - val_disc_loss: 0.5101 - val_w_kl_loss: 0.9749 - val_w_disc_loss: 875.2972 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 880.1222 - reco_loss: 1.1103 - kl_loss: 0.5019 - disc_loss: 0.5030 - beta: 0.9900 - raw_loss: 1.6073 - w_kl_loss: 0.3706 - w_disc_loss: 893.9256 - gamma: 1777.2499 - val_loss: 938.5288 - val_reco_loss: 4.1933 - val_kl_loss: 0.9680 - val_raw_loss: 5.1613 - val_disc_loss: 0.5077 - val_w_kl_loss: 0.9583 - val_w_disc_loss: 933.3773 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 935.2799 - reco_loss: 0.8109 - kl_loss: 0.5178 - disc_loss: 0.4997 - beta: 1.0000 - raw_loss: 1.3382 - w_kl_loss: 0.4012 - w_disc_loss: 949.2483 - gamma: 1899.7499 - val_loss: 1014.1768 - val_reco_loss: 4.3162 - val_kl_loss: 0.9623 - val_raw_loss: 5.2785 - val_disc_loss: 0.5145 - val_w_kl_loss: 0.9623 - val_w_disc_loss: 1008.8983 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 988.9989 - reco_loss: 1.0082 - kl_loss: 0.5060 - disc_loss: 0.4958 - beta: 0.9800 - raw_loss: 1.4940 - w_kl_loss: 0.3618 - w_disc_loss: 1002.6814 - gamma: 2022.2010\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 989.0094 - reco_loss: 1.0082 - kl_loss: 0.5059 - disc_loss: 0.4958 - beta: 0.9900 - raw_loss: 1.4939 - w_kl_loss: 0.3619 - w_disc_loss: 1002.7040 - gamma: 2022.2499 - val_loss: 1053.5201 - val_reco_loss: 4.3055 - val_kl_loss: 0.9549 - val_raw_loss: 5.2604 - val_disc_loss: 0.5031 - val_w_kl_loss: 0.9453 - val_w_disc_loss: 1048.2693 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1042.0702 - reco_loss: 0.7890 - kl_loss: 0.5200 - disc_loss: 0.4923 - beta: 1.0000 - raw_loss: 1.3044 - w_kl_loss: 0.3913 - w_disc_loss: 1055.8429 - gamma: 2144.7499 - val_loss: 1121.3502 - val_reco_loss: 4.3541 - val_kl_loss: 0.9515 - val_raw_loss: 5.3057 - val_disc_loss: 0.5059 - val_w_kl_loss: 0.9515 - val_w_disc_loss: 1116.0446 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1100.1722 - reco_loss: 1.1200 - kl_loss: 0.4960 - disc_loss: 0.4912 - beta: 0.9900 - raw_loss: 1.6272 - w_kl_loss: 0.3794 - w_disc_loss: 1113.5880 - gamma: 2267.2499 - val_loss: 1170.2635 - val_reco_loss: 4.4780 - val_kl_loss: 0.9484 - val_raw_loss: 5.4264 - val_disc_loss: 0.5003 - val_w_kl_loss: 0.9389 - val_w_disc_loss: 1164.8467 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1155.7066 - reco_loss: 0.8334 - kl_loss: 0.5106 - disc_loss: 0.4893 - beta: 1.0000 - raw_loss: 1.3462 - w_kl_loss: 0.3878 - w_disc_loss: 1169.3463 - gamma: 2389.7499 - val_loss: 1220.3199 - val_reco_loss: 4.1047 - val_kl_loss: 0.9443 - val_raw_loss: 5.0490 - val_disc_loss: 0.4958 - val_w_kl_loss: 0.9443 - val_w_disc_loss: 1215.2710 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1211.4644 - reco_loss: 1.0376 - kl_loss: 0.4922 - disc_loss: 0.4876 - beta: 0.9900 - raw_loss: 1.4990 - w_kl_loss: 0.3439 - w_disc_loss: 1224.9011 - gamma: 2512.2499 - val_loss: 1251.8329 - val_reco_loss: 4.1776 - val_kl_loss: 0.9417 - val_raw_loss: 5.1194 - val_disc_loss: 0.4845 - val_w_kl_loss: 0.9323 - val_w_disc_loss: 1246.7229 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 24.6112 - reco_loss: 1.9158 - kl_loss: 0.8253 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 2.6925 - w_kl_loss: 0.5794 - w_disc_loss: 43.1449 - gamma: 62.2500 - val_loss: 95.1346 - val_reco_loss: 7.9551 - val_kl_loss: 1.6440 - val_raw_loss: 9.5992 - val_disc_loss: 0.6930 - val_w_kl_loss: 1.6276 - val_w_disc_loss: 85.5519 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 109.0166 - reco_loss: 1.3970 - kl_loss: 0.8494 - disc_loss: 0.6930 - beta: 1.0000 - raw_loss: 2.2132 - w_kl_loss: 0.6195 - w_disc_loss: 128.0242 - gamma: 184.7500 - val_loss: 180.2487 - val_reco_loss: 8.2003 - val_kl_loss: 1.6347 - val_raw_loss: 9.8350 - val_disc_loss: 0.6929 - val_w_kl_loss: 1.6347 - val_w_disc_loss: 170.4137 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 194.3463 - reco_loss: 1.8935 - kl_loss: 0.7998 - disc_loss: 0.6928 - beta: 0.9900 - raw_loss: 2.6887 - w_kl_loss: 0.5941 - w_disc_loss: 212.8735 - gamma: 307.2500 - val_loss: 264.9466 - val_reco_loss: 8.0914 - val_kl_loss: 1.6261 - val_raw_loss: 9.7175 - val_disc_loss: 0.6928 - val_w_kl_loss: 1.6099 - val_w_disc_loss: 255.2453 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 278.6905 - reco_loss: 1.3865 - kl_loss: 0.8462 - disc_loss: 0.6927 - beta: 1.0000 - raw_loss: 2.2090 - w_kl_loss: 0.6242 - w_disc_loss: 297.6926 - gamma: 429.7500 - val_loss: 349.6354 - val_reco_loss: 7.9712 - val_kl_loss: 1.6178 - val_raw_loss: 9.5890 - val_disc_loss: 0.6926 - val_w_kl_loss: 1.6178 - val_w_disc_loss: 340.0464 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 363.8113 - reco_loss: 1.7060 - kl_loss: 0.8121 - disc_loss: 0.6926 - beta: 0.9900 - raw_loss: 2.5362 - w_kl_loss: 0.6212 - w_disc_loss: 382.4818 - gamma: 552.2500 - val_loss: 434.4545 - val_reco_loss: 8.0447 - val_kl_loss: 1.6087 - val_raw_loss: 9.6534 - val_disc_loss: 0.6925 - val_w_kl_loss: 1.5926 - val_w_disc_loss: 424.8172 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 448.1401 - reco_loss: 1.2738 - kl_loss: 0.8248 - disc_loss: 0.6925 - beta: 1.0000 - raw_loss: 2.1018 - w_kl_loss: 0.6289 - w_disc_loss: 467.2415 - gamma: 674.7500 - val_loss: 519.1617 - val_reco_loss: 8.0037 - val_kl_loss: 1.6005 - val_raw_loss: 9.6042 - val_disc_loss: 0.6924 - val_w_kl_loss: 1.6005 - val_w_disc_loss: 509.5575 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 533.3028 - reco_loss: 1.7890 - kl_loss: 0.7774 - disc_loss: 0.6923 - beta: 0.9900 - raw_loss: 2.5319 - w_kl_loss: 0.5542 - w_disc_loss: 551.9710 - gamma: 797.2500 - val_loss: 603.6333 - val_reco_loss: 7.7896 - val_kl_loss: 1.5921 - val_raw_loss: 9.3817 - val_disc_loss: 0.6923 - val_w_kl_loss: 1.5762 - val_w_disc_loss: 594.2675 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 617.6898 - reco_loss: 1.3781 - kl_loss: 0.7902 - disc_loss: 0.6922 - beta: 1.0000 - raw_loss: 2.2166 - w_kl_loss: 0.6363 - w_disc_loss: 636.6703 - gamma: 919.7500 - val_loss: 688.2347 - val_reco_loss: 7.7028 - val_kl_loss: 1.5849 - val_raw_loss: 9.2877 - val_disc_loss: 0.6921 - val_w_kl_loss: 1.5849 - val_w_disc_loss: 678.9470 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2491/2500 [============================>.] - ETA: 0s - loss: 702.5933 - reco_loss: 1.7626 - kl_loss: 0.7787 - disc_loss: 0.6921 - beta: 0.9000 - raw_loss: 2.5327 - w_kl_loss: 0.5740 - w_disc_loss: 721.1695 - gamma: 1042.0050\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 702.6773 - reco_loss: 1.7619 - kl_loss: 0.7786 - disc_loss: 0.6921 - beta: 0.9900 - raw_loss: 2.5320 - w_kl_loss: 0.5747 - w_disc_loss: 721.3389 - gamma: 1042.2500 - val_loss: 772.8315 - val_reco_loss: 7.6736 - val_kl_loss: 1.5775 - val_raw_loss: 9.2511 - val_disc_loss: 0.6920 - val_w_kl_loss: 1.5617 - val_w_disc_loss: 763.5961 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 786.9079 - reco_loss: 1.2033 - kl_loss: 0.8444 - disc_loss: 0.6920 - beta: 1.0000 - raw_loss: 2.1029 - w_kl_loss: 0.6789 - w_disc_loss: 805.9954 - gamma: 1164.7500 - val_loss: 857.7454 - val_reco_loss: 7.8811 - val_kl_loss: 1.5735 - val_raw_loss: 9.4546 - val_disc_loss: 0.6919 - val_w_kl_loss: 1.5735 - val_w_disc_loss: 848.2908 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 871.7723 - reco_loss: 1.5489 - kl_loss: 0.8239 - disc_loss: 0.6919 - beta: 0.9900 - raw_loss: 2.2770 - w_kl_loss: 0.5421 - w_disc_loss: 890.6783 - gamma: 1287.2500 - val_loss: 942.4603 - val_reco_loss: 7.9365 - val_kl_loss: 1.5692 - val_raw_loss: 9.5057 - val_disc_loss: 0.6919 - val_w_kl_loss: 1.5535 - val_w_disc_loss: 932.9703 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 956.3511 - reco_loss: 1.3384 - kl_loss: 0.7889 - disc_loss: 0.6919 - beta: 1.0000 - raw_loss: 2.1797 - w_kl_loss: 0.6356 - w_disc_loss: 975.3556 - gamma: 1409.7499 - val_loss: 1026.9198 - val_reco_loss: 7.7198 - val_kl_loss: 1.5655 - val_raw_loss: 9.2853 - val_disc_loss: 0.6918 - val_w_kl_loss: 1.5655 - val_w_disc_loss: 1017.6345 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1041.5938 - reco_loss: 1.9206 - kl_loss: 0.7834 - disc_loss: 0.6918 - beta: 0.9900 - raw_loss: 2.7619 - w_kl_loss: 0.6251 - w_disc_loss: 1060.0089 - gamma: 1532.2499 - val_loss: 1111.4309 - val_reco_loss: 7.6010 - val_kl_loss: 1.5621 - val_raw_loss: 9.1631 - val_disc_loss: 0.6918 - val_w_kl_loss: 1.5464 - val_w_disc_loss: 1102.2834 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1125.6122 - reco_loss: 1.3019 - kl_loss: 0.7912 - disc_loss: 0.6917 - beta: 1.0000 - raw_loss: 2.1353 - w_kl_loss: 0.6301 - w_disc_loss: 1144.6567 - gamma: 1654.7499 - val_loss: 1196.0114 - val_reco_loss: 7.5362 - val_kl_loss: 1.5579 - val_raw_loss: 9.0940 - val_disc_loss: 0.6917 - val_w_kl_loss: 1.5579 - val_w_disc_loss: 1186.9174 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1210.3961 - reco_loss: 1.5816 - kl_loss: 0.7816 - disc_loss: 0.6917 - beta: 0.9900 - raw_loss: 2.2892 - w_kl_loss: 0.5276 - w_disc_loss: 1229.2812 - gamma: 1777.2499 - val_loss: 1280.6863 - val_reco_loss: 7.6127 - val_kl_loss: 1.5532 - val_raw_loss: 9.1659 - val_disc_loss: 0.6916 - val_w_kl_loss: 1.5377 - val_w_disc_loss: 1271.5359 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1294.9021 - reco_loss: 1.4325 - kl_loss: 0.7829 - disc_loss: 0.6916 - beta: 1.0000 - raw_loss: 2.1804 - w_kl_loss: 0.5670 - w_disc_loss: 1313.8947 - gamma: 1899.7499 - val_loss: 1365.4515 - val_reco_loss: 7.7615 - val_kl_loss: 1.5509 - val_raw_loss: 9.3124 - val_disc_loss: 0.6916 - val_w_kl_loss: 1.5509 - val_w_disc_loss: 1356.1392 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 1379.6806 - reco_loss: 1.5974 - kl_loss: 0.7889 - disc_loss: 0.6916 - beta: 0.9900 - raw_loss: 2.3670 - w_kl_loss: 0.5744 - w_disc_loss: 1398.4735 - gamma: 2022.2255\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1379.6890 - reco_loss: 1.5973 - kl_loss: 0.7889 - disc_loss: 0.6916 - beta: 0.9900 - raw_loss: 2.3669 - w_kl_loss: 0.5745 - w_disc_loss: 1398.4904 - gamma: 2022.2499 - val_loss: 1449.8870 - val_reco_loss: 7.6276 - val_kl_loss: 1.5473 - val_raw_loss: 9.1749 - val_disc_loss: 0.6915 - val_w_kl_loss: 1.5318 - val_w_disc_loss: 1440.7275 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1464.0960 - reco_loss: 1.3715 - kl_loss: 0.7849 - disc_loss: 0.6915 - beta: 1.0000 - raw_loss: 2.1743 - w_kl_loss: 0.6093 - w_disc_loss: 1483.1056 - gamma: 2144.7499 - val_loss: 1534.4907 - val_reco_loss: 7.5755 - val_kl_loss: 1.5463 - val_raw_loss: 9.1217 - val_disc_loss: 0.6915 - val_w_kl_loss: 1.5463 - val_w_disc_loss: 1525.3690 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1548.9536 - reco_loss: 1.6403 - kl_loss: 0.7892 - disc_loss: 0.6915 - beta: 0.9900 - raw_loss: 2.3915 - w_kl_loss: 0.5610 - w_disc_loss: 1567.7261 - gamma: 2267.2499 - val_loss: 1619.1364 - val_reco_loss: 7.6052 - val_kl_loss: 1.5440 - val_raw_loss: 9.1492 - val_disc_loss: 0.6914 - val_w_kl_loss: 1.5285 - val_w_disc_loss: 1610.0027 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1633.3294 - reco_loss: 1.2021 - kl_loss: 0.8186 - disc_loss: 0.6914 - beta: 1.0000 - raw_loss: 2.1360 - w_kl_loss: 0.7042 - w_disc_loss: 1652.3558 - gamma: 2389.7499 - val_loss: 1703.7252 - val_reco_loss: 7.5540 - val_kl_loss: 1.5424 - val_raw_loss: 9.0964 - val_disc_loss: 0.6914 - val_w_kl_loss: 1.5424 - val_w_disc_loss: 1694.6288 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1718.0673 - reco_loss: 1.5877 - kl_loss: 0.7138 - disc_loss: 0.6914 - beta: 0.9900 - raw_loss: 2.2494 - w_kl_loss: 0.4935 - w_disc_loss: 1736.9937 - gamma: 2512.2499 - val_loss: 1788.2023 - val_reco_loss: 7.4300 - val_kl_loss: 1.5402 - val_raw_loss: 8.9702 - val_disc_loss: 0.6914 - val_w_kl_loss: 1.5248 - val_w_disc_loss: 1779.2474 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 25.9000 - reco_loss: 4.5910 - kl_loss: 2.5679 - disc_loss: 0.5990 - beta: 0.9900 - raw_loss: 7.0005 - w_kl_loss: 1.7971 - w_disc_loss: 37.2507 - gamma: 62.2500 - val_loss: 98.3319 - val_reco_loss: 19.4655 - val_kl_loss: 4.4349 - val_raw_loss: 23.9005 - val_disc_loss: 0.6033 - val_w_kl_loss: 4.3906 - val_w_disc_loss: 74.4758 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 97.4296 - reco_loss: 3.4852 - kl_loss: 2.6255 - disc_loss: 0.5922 - beta: 1.0000 - raw_loss: 6.1957 - w_kl_loss: 2.0614 - w_disc_loss: 109.3752 - gamma: 184.7500 - val_loss: 168.5943 - val_reco_loss: 19.5579 - val_kl_loss: 4.4144 - val_raw_loss: 23.9723 - val_disc_loss: 0.5880 - val_w_kl_loss: 4.4144 - val_w_disc_loss: 144.6219 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 168.7603 - reco_loss: 4.2362 - kl_loss: 2.5237 - disc_loss: 0.5863 - beta: 0.9900 - raw_loss: 6.6142 - w_kl_loss: 1.7719 - w_disc_loss: 180.1068 - gamma: 307.2500 - val_loss: 237.8101 - val_reco_loss: 19.3447 - val_kl_loss: 4.3967 - val_raw_loss: 23.7414 - val_disc_loss: 0.5811 - val_w_kl_loss: 4.3527 - val_w_disc_loss: 214.1127 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 237.6277 - reco_loss: 3.5199 - kl_loss: 2.5981 - disc_loss: 0.5803 - beta: 1.0000 - raw_loss: 6.0358 - w_kl_loss: 1.9131 - w_disc_loss: 249.3692 - gamma: 429.7500 - val_loss: 309.7680 - val_reco_loss: 19.3340 - val_kl_loss: 4.3850 - val_raw_loss: 23.7190 - val_disc_loss: 0.5826 - val_w_kl_loss: 4.3850 - val_w_disc_loss: 286.0489 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 306.1755 - reco_loss: 4.0441 - kl_loss: 2.5580 - disc_loss: 0.5747 - beta: 0.9900 - raw_loss: 6.4299 - w_kl_loss: 1.7780 - w_disc_loss: 317.3499 - gamma: 552.2500 - val_loss: 370.6558 - val_reco_loss: 19.2877 - val_kl_loss: 4.3759 - val_raw_loss: 23.6637 - val_disc_loss: 0.5657 - val_w_kl_loss: 4.3322 - val_w_disc_loss: 347.0358 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 372.4330 - reco_loss: 3.2265 - kl_loss: 2.5806 - disc_loss: 0.5693 - beta: 1.0000 - raw_loss: 5.7517 - w_kl_loss: 1.9195 - w_disc_loss: 384.1203 - gamma: 674.7500 - val_loss: 441.5550 - val_reco_loss: 19.1835 - val_kl_loss: 4.3693 - val_raw_loss: 23.5528 - val_disc_loss: 0.5680 - val_w_kl_loss: 4.3693 - val_w_disc_loss: 418.0022 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 438.8780 - reco_loss: 4.2497 - kl_loss: 2.4884 - disc_loss: 0.5638 - beta: 0.9900 - raw_loss: 6.6892 - w_kl_loss: 1.8219 - w_disc_loss: 449.4591 - gamma: 797.2500 - val_loss: 506.3951 - val_reco_loss: 19.2420 - val_kl_loss: 4.3665 - val_raw_loss: 23.6086 - val_disc_loss: 0.5624 - val_w_kl_loss: 4.3229 - val_w_disc_loss: 482.8302 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 502.1862 - reco_loss: 3.2147 - kl_loss: 2.6049 - disc_loss: 0.5583 - beta: 1.0000 - raw_loss: 5.8020 - w_kl_loss: 1.9653 - w_disc_loss: 513.4885 - gamma: 919.7500 - val_loss: 568.8539 - val_reco_loss: 19.0103 - val_kl_loss: 4.3647 - val_raw_loss: 23.3750 - val_disc_loss: 0.5561 - val_w_kl_loss: 4.3647 - val_w_disc_loss: 545.4789 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2494/2500 [============================>.] - ETA: 0s - loss: 565.9920 - reco_loss: 4.4991 - kl_loss: 2.3817 - disc_loss: 0.5529 - beta: 0.9300 - raw_loss: 6.7253 - w_kl_loss: 1.6574 - w_disc_loss: 576.1565 - gamma: 1042.0785\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 566.0346 - reco_loss: 4.4985 - kl_loss: 2.3850 - disc_loss: 0.5529 - beta: 0.9900 - raw_loss: 6.7251 - w_kl_loss: 1.6592 - w_disc_loss: 576.2465 - gamma: 1042.2500 - val_loss: 632.9518 - val_reco_loss: 19.4245 - val_kl_loss: 4.3620 - val_raw_loss: 23.7865 - val_disc_loss: 0.5521 - val_w_kl_loss: 4.3183 - val_w_disc_loss: 609.2090 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 627.2378 - reco_loss: 3.1569 - kl_loss: 2.6475 - disc_loss: 0.5479 - beta: 1.0000 - raw_loss: 5.8214 - w_kl_loss: 2.0192 - w_disc_loss: 638.1997 - gamma: 1164.7500 - val_loss: 683.4750 - val_reco_loss: 18.9697 - val_kl_loss: 4.3623 - val_raw_loss: 23.3320 - val_disc_loss: 0.5385 - val_w_kl_loss: 4.3623 - val_w_disc_loss: 660.1430 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 691.0004 - reco_loss: 4.5193 - kl_loss: 2.5295 - disc_loss: 0.5444 - beta: 0.9900 - raw_loss: 6.9478 - w_kl_loss: 1.8098 - w_disc_loss: 700.7257 - gamma: 1287.2500 - val_loss: 762.7845 - val_reco_loss: 19.1816 - val_kl_loss: 4.3644 - val_raw_loss: 23.5460 - val_disc_loss: 0.5482 - val_w_kl_loss: 4.3207 - val_w_disc_loss: 739.2822 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 753.3136 - reco_loss: 3.2452 - kl_loss: 2.6321 - disc_loss: 0.5419 - beta: 1.0000 - raw_loss: 5.9693 - w_kl_loss: 2.0628 - w_disc_loss: 763.9418 - gamma: 1409.7499 - val_loss: 817.8500 - val_reco_loss: 19.3925 - val_kl_loss: 4.3681 - val_raw_loss: 23.7607 - val_disc_loss: 0.5398 - val_w_kl_loss: 4.3681 - val_w_disc_loss: 794.0894 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 815.7375 - reco_loss: 4.6026 - kl_loss: 2.4773 - disc_loss: 0.5386 - beta: 0.9900 - raw_loss: 6.9557 - w_kl_loss: 1.7551 - w_disc_loss: 825.2800 - gamma: 1532.2499 - val_loss: 862.4726 - val_reco_loss: 18.6964 - val_kl_loss: 4.3683 - val_raw_loss: 23.0647 - val_disc_loss: 0.5268 - val_w_kl_loss: 4.3245 - val_w_disc_loss: 839.4516 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 876.1180 - reco_loss: 3.3857 - kl_loss: 2.5789 - disc_loss: 0.5358 - beta: 1.0000 - raw_loss: 5.9232 - w_kl_loss: 1.9311 - w_disc_loss: 886.6076 - gamma: 1654.7499 - val_loss: 952.6496 - val_reco_loss: 19.2191 - val_kl_loss: 4.3720 - val_raw_loss: 23.5911 - val_disc_loss: 0.5414 - val_w_kl_loss: 4.3720 - val_w_disc_loss: 929.0585 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 936.3971 - reco_loss: 3.9737 - kl_loss: 2.5695 - disc_loss: 0.5325 - beta: 0.9900 - raw_loss: 6.3297 - w_kl_loss: 1.7552 - w_disc_loss: 946.3791 - gamma: 1777.2499 - val_loss: 981.1135 - val_reco_loss: 19.0178 - val_kl_loss: 4.3713 - val_raw_loss: 23.3891 - val_disc_loss: 0.5210 - val_w_kl_loss: 4.3275 - val_w_disc_loss: 957.7682 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 995.1920 - reco_loss: 3.2236 - kl_loss: 2.5817 - disc_loss: 0.5293 - beta: 1.0000 - raw_loss: 5.8706 - w_kl_loss: 2.0066 - w_disc_loss: 1005.5361 - gamma: 1899.7499 - val_loss: 1055.6096 - val_reco_loss: 19.1931 - val_kl_loss: 4.3757 - val_raw_loss: 23.5688 - val_disc_loss: 0.5263 - val_w_kl_loss: 4.3757 - val_w_disc_loss: 1032.0409 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2488/2500 [============================>.] - ETA: 0s - loss: 1054.4762 - reco_loss: 4.4975 - kl_loss: 2.4901 - disc_loss: 0.5260 - beta: 0.8700 - raw_loss: 7.0063 - w_kl_loss: 1.8706 - w_disc_loss: 1063.4984 - gamma: 2021.9315\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1054.5477 - reco_loss: 4.4945 - kl_loss: 2.4871 - disc_loss: 0.5260 - beta: 0.9900 - raw_loss: 7.0032 - w_kl_loss: 1.8730 - w_disc_loss: 1063.6565 - gamma: 2022.2499 - val_loss: 1111.2642 - val_reco_loss: 19.5412 - val_kl_loss: 4.3801 - val_raw_loss: 23.9213 - val_disc_loss: 0.5219 - val_w_kl_loss: 4.3362 - val_w_disc_loss: 1087.3867 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1111.6272 - reco_loss: 3.0783 - kl_loss: 2.6425 - disc_loss: 0.5231 - beta: 1.0000 - raw_loss: 5.7479 - w_kl_loss: 2.0254 - w_disc_loss: 1121.9010 - gamma: 2144.7499 - val_loss: 1188.8203 - val_reco_loss: 19.1204 - val_kl_loss: 4.3834 - val_raw_loss: 23.5038 - val_disc_loss: 0.5283 - val_w_kl_loss: 4.3834 - val_w_disc_loss: 1165.3165 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1173.7620 - reco_loss: 4.0728 - kl_loss: 2.5428 - disc_loss: 0.5219 - beta: 0.9900 - raw_loss: 6.3671 - w_kl_loss: 1.7107 - w_disc_loss: 1183.3815 - gamma: 2267.2499 - val_loss: 1238.6207 - val_reco_loss: 18.6499 - val_kl_loss: 4.3860 - val_raw_loss: 23.0359 - val_disc_loss: 0.5221 - val_w_kl_loss: 4.3421 - val_w_disc_loss: 1215.6287 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1234.5451 - reco_loss: 3.6014 - kl_loss: 2.5769 - disc_loss: 0.5207 - beta: 1.0000 - raw_loss: 6.1779 - w_kl_loss: 1.9589 - w_disc_loss: 1244.3151 - gamma: 2389.7499 - val_loss: 1272.2827 - val_reco_loss: 19.2681 - val_kl_loss: 4.3885 - val_raw_loss: 23.6566 - val_disc_loss: 0.5094 - val_w_kl_loss: 4.3885 - val_w_disc_loss: 1248.6261 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1295.3819 - reco_loss: 4.6554 - kl_loss: 2.5031 - disc_loss: 0.5191 - beta: 0.9900 - raw_loss: 7.0729 - w_kl_loss: 1.8004 - w_disc_loss: 1304.2095 - gamma: 2512.2499 - val_loss: 1363.0002 - val_reco_loss: 19.2507 - val_kl_loss: 4.3906 - val_raw_loss: 23.6413 - val_disc_loss: 0.5205 - val_w_kl_loss: 4.3467 - val_w_disc_loss: 1339.4030 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 25.5075 - reco_loss: 1.5906 - kl_loss: 0.6626 - disc_loss: 0.7369 - beta: 0.9900 - raw_loss: 2.2312 - w_kl_loss: 0.4771 - w_disc_loss: 45.8462 - gamma: 62.2500 - val_loss: 97.4943 - val_reco_loss: 5.4281 - val_kl_loss: 0.8206 - val_raw_loss: 6.2486 - val_disc_loss: 0.7392 - val_w_kl_loss: 0.8124 - val_w_disc_loss: 91.2539 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 114.7706 - reco_loss: 1.0517 - kl_loss: 0.7176 - disc_loss: 0.7335 - beta: 1.0000 - raw_loss: 1.7393 - w_kl_loss: 0.5206 - w_disc_loss: 135.4955 - gamma: 184.7500 - val_loss: 186.5827 - val_reco_loss: 5.4934 - val_kl_loss: 0.8217 - val_raw_loss: 6.3151 - val_disc_loss: 0.7329 - val_w_kl_loss: 0.8217 - val_w_disc_loss: 180.2676 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 204.2302 - reco_loss: 1.4196 - kl_loss: 0.6914 - disc_loss: 0.7308 - beta: 0.9900 - raw_loss: 2.0872 - w_kl_loss: 0.4981 - w_disc_loss: 224.5267 - gamma: 307.2500 - val_loss: 274.0250 - val_reco_loss: 5.2333 - val_kl_loss: 0.8230 - val_raw_loss: 6.0563 - val_disc_loss: 0.7273 - val_w_kl_loss: 0.8148 - val_w_disc_loss: 267.9769 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 292.5484 - reco_loss: 1.2036 - kl_loss: 0.6992 - disc_loss: 0.7283 - beta: 1.0000 - raw_loss: 1.8937 - w_kl_loss: 0.5246 - w_disc_loss: 312.9601 - gamma: 429.7500 - val_loss: 362.9085 - val_reco_loss: 5.4795 - val_kl_loss: 0.8245 - val_raw_loss: 6.3040 - val_disc_loss: 0.7264 - val_w_kl_loss: 0.8245 - val_w_disc_loss: 356.6045 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 380.7067 - reco_loss: 1.3623 - kl_loss: 0.6889 - disc_loss: 0.7260 - beta: 0.9900 - raw_loss: 2.0218 - w_kl_loss: 0.4924 - w_disc_loss: 400.9198 - gamma: 552.2500 - val_loss: 450.5301 - val_reco_loss: 5.3426 - val_kl_loss: 0.8265 - val_raw_loss: 6.1691 - val_disc_loss: 0.7244 - val_w_kl_loss: 0.8182 - val_w_disc_loss: 444.3692 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 468.0971 - reco_loss: 1.1495 - kl_loss: 0.7101 - disc_loss: 0.7238 - beta: 1.0000 - raw_loss: 1.8833 - w_kl_loss: 0.5568 - w_disc_loss: 488.3824 - gamma: 674.7500 - val_loss: 539.3948 - val_reco_loss: 5.2991 - val_kl_loss: 0.8283 - val_raw_loss: 6.1274 - val_disc_loss: 0.7246 - val_w_kl_loss: 0.8283 - val_w_disc_loss: 533.2674 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 555.5795 - reco_loss: 1.5227 - kl_loss: 0.6701 - disc_loss: 0.7219 - beta: 0.9900 - raw_loss: 2.1420 - w_kl_loss: 0.4616 - w_disc_loss: 575.5479 - gamma: 797.2500 - val_loss: 625.6461 - val_reco_loss: 5.3606 - val_kl_loss: 0.8306 - val_raw_loss: 6.1912 - val_disc_loss: 0.7216 - val_w_kl_loss: 0.8223 - val_w_disc_loss: 619.4631 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 642.0133 - reco_loss: 1.1164 - kl_loss: 0.7124 - disc_loss: 0.7200 - beta: 1.0000 - raw_loss: 1.8355 - w_kl_loss: 0.5450 - w_disc_loss: 662.2300 - gamma: 919.7500 - val_loss: 710.0486 - val_reco_loss: 5.4946 - val_kl_loss: 0.8326 - val_raw_loss: 6.3272 - val_disc_loss: 0.7174 - val_w_kl_loss: 0.8326 - val_w_disc_loss: 703.7214 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2495/2500 [============================>.] - ETA: 0s - loss: 728.5205 - reco_loss: 1.4574 - kl_loss: 0.6852 - disc_loss: 0.7181 - beta: 0.9400 - raw_loss: 2.1066 - w_kl_loss: 0.4835 - w_disc_loss: 748.3552 - gamma: 1042.1030\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 728.5720 - reco_loss: 1.4571 - kl_loss: 0.6859 - disc_loss: 0.7181 - beta: 0.9900 - raw_loss: 2.1064 - w_kl_loss: 0.4839 - w_disc_loss: 748.4597 - gamma: 1042.2500 - val_loss: 799.8575 - val_reco_loss: 5.3791 - val_kl_loss: 0.8349 - val_raw_loss: 6.2140 - val_disc_loss: 0.7192 - val_w_kl_loss: 0.8265 - val_w_disc_loss: 793.6519 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 814.4464 - reco_loss: 1.0753 - kl_loss: 0.7205 - disc_loss: 0.7165 - beta: 1.0000 - raw_loss: 1.8246 - w_kl_loss: 0.5719 - w_disc_loss: 834.5666 - gamma: 1164.7500 - val_loss: 882.3021 - val_reco_loss: 5.4929 - val_kl_loss: 0.8359 - val_raw_loss: 6.3288 - val_disc_loss: 0.7145 - val_w_kl_loss: 0.8359 - val_w_disc_loss: 875.9733 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 901.3028 - reco_loss: 1.4328 - kl_loss: 0.6943 - disc_loss: 0.7156 - beta: 0.9900 - raw_loss: 2.0961 - w_kl_loss: 0.4949 - w_disc_loss: 921.1233 - gamma: 1287.2500 - val_loss: 971.0936 - val_reco_loss: 5.5366 - val_kl_loss: 0.8372 - val_raw_loss: 6.3738 - val_disc_loss: 0.7154 - val_w_kl_loss: 0.8288 - val_w_disc_loss: 964.7281 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 987.4071 - reco_loss: 1.1212 - kl_loss: 0.7201 - disc_loss: 0.7146 - beta: 1.0000 - raw_loss: 1.8202 - w_kl_loss: 0.5306 - w_disc_loss: 1007.4729 - gamma: 1409.7499 - val_loss: 1056.3474 - val_reco_loss: 5.4537 - val_kl_loss: 0.8384 - val_raw_loss: 6.2921 - val_disc_loss: 0.7139 - val_w_kl_loss: 0.8384 - val_w_disc_loss: 1050.0553 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1074.0355 - reco_loss: 1.3938 - kl_loss: 0.6889 - disc_loss: 0.7139 - beta: 0.9900 - raw_loss: 2.0437 - w_kl_loss: 0.4851 - w_disc_loss: 1093.8559 - gamma: 1532.2499 - val_loss: 1140.2086 - val_reco_loss: 5.4373 - val_kl_loss: 0.8395 - val_raw_loss: 6.2768 - val_disc_loss: 0.7116 - val_w_kl_loss: 0.8311 - val_w_disc_loss: 1133.9402 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1159.8787 - reco_loss: 1.1192 - kl_loss: 0.7217 - disc_loss: 0.7130 - beta: 1.0000 - raw_loss: 1.8108 - w_kl_loss: 0.5250 - w_disc_loss: 1179.9064 - gamma: 1654.7499 - val_loss: 1226.1619 - val_reco_loss: 5.3555 - val_kl_loss: 0.8408 - val_raw_loss: 6.1963 - val_disc_loss: 0.7110 - val_w_kl_loss: 0.8408 - val_w_disc_loss: 1219.9656 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1246.3227 - reco_loss: 1.4844 - kl_loss: 0.7005 - disc_loss: 0.7123 - beta: 0.9900 - raw_loss: 2.1934 - w_kl_loss: 0.5296 - w_disc_loss: 1265.9452 - gamma: 1777.2499 - val_loss: 1325.6809 - val_reco_loss: 5.4777 - val_kl_loss: 0.8420 - val_raw_loss: 6.3197 - val_disc_loss: 0.7177 - val_w_kl_loss: 0.8335 - val_w_disc_loss: 1319.3696 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1331.5166 - reco_loss: 1.1594 - kl_loss: 0.7228 - disc_loss: 0.7114 - beta: 1.0000 - raw_loss: 1.8814 - w_kl_loss: 0.5463 - w_disc_loss: 1351.4212 - gamma: 1899.7499 - val_loss: 1402.3617 - val_reco_loss: 5.3841 - val_kl_loss: 0.8431 - val_raw_loss: 6.2272 - val_disc_loss: 0.7120 - val_w_kl_loss: 0.8431 - val_w_disc_loss: 1396.1345 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2490/2500 [============================>.] - ETA: 0s - loss: 1417.5098 - reco_loss: 1.4315 - kl_loss: 0.7060 - disc_loss: 0.7107 - beta: 0.8900 - raw_loss: 2.0942 - w_kl_loss: 0.4934 - w_disc_loss: 1437.0866 - gamma: 2021.9805\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1417.6029 - reco_loss: 1.4307 - kl_loss: 0.7058 - disc_loss: 0.7107 - beta: 0.9900 - raw_loss: 2.0936 - w_kl_loss: 0.4942 - w_disc_loss: 1437.2761 - gamma: 2022.2499 - val_loss: 1488.4865 - val_reco_loss: 5.5419 - val_kl_loss: 0.8447 - val_raw_loss: 6.3865 - val_disc_loss: 0.7114 - val_w_kl_loss: 0.8362 - val_w_disc_loss: 1482.1084 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1502.9236 - reco_loss: 1.1946 - kl_loss: 0.7127 - disc_loss: 0.7100 - beta: 1.0000 - raw_loss: 1.9578 - w_kl_loss: 0.5820 - w_disc_loss: 1522.7092 - gamma: 2144.7499 - val_loss: 1567.1173 - val_reco_loss: 5.4829 - val_kl_loss: 0.8453 - val_raw_loss: 6.3281 - val_disc_loss: 0.7075 - val_w_kl_loss: 0.8453 - val_w_disc_loss: 1560.7892 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1589.1042 - reco_loss: 1.3983 - kl_loss: 0.7094 - disc_loss: 0.7096 - beta: 0.9900 - raw_loss: 2.0609 - w_kl_loss: 0.4943 - w_disc_loss: 1608.7756 - gamma: 2267.2499 - val_loss: 1659.9327 - val_reco_loss: 5.4954 - val_kl_loss: 0.8459 - val_raw_loss: 6.3414 - val_disc_loss: 0.7102 - val_w_kl_loss: 0.8375 - val_w_disc_loss: 1653.5999 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1675.4453 - reco_loss: 1.1850 - kl_loss: 0.6980 - disc_loss: 0.7094 - beta: 1.0000 - raw_loss: 1.9064 - w_kl_loss: 0.5488 - w_disc_loss: 1695.2629 - gamma: 2389.7499 - val_loss: 1742.2601 - val_reco_loss: 5.3592 - val_kl_loss: 0.8464 - val_raw_loss: 6.2056 - val_disc_loss: 0.7083 - val_w_kl_loss: 0.8464 - val_w_disc_loss: 1736.0546 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1761.7997 - reco_loss: 1.4726 - kl_loss: 0.7003 - disc_loss: 0.7091 - beta: 0.9900 - raw_loss: 2.1838 - w_kl_loss: 0.5298 - w_disc_loss: 1781.3310 - gamma: 2512.2499 - val_loss: 1827.9893 - val_reco_loss: 5.4080 - val_kl_loss: 0.8470 - val_raw_loss: 6.2550 - val_disc_loss: 0.7079 - val_w_kl_loss: 0.8385 - val_w_disc_loss: 1821.7427 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 23.6507 - reco_loss: 1.2897 - kl_loss: 0.9143 - disc_loss: 0.6816 - beta: 0.9900 - raw_loss: 2.1221 - w_kl_loss: 0.6212 - w_disc_loss: 42.4067 - gamma: 62.2500 - val_loss: 88.9098 - val_reco_loss: 4.6784 - val_kl_loss: 0.9718 - val_raw_loss: 5.6502 - val_disc_loss: 0.6745 - val_w_kl_loss: 0.9621 - val_w_disc_loss: 83.2693 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 106.5183 - reco_loss: 1.0746 - kl_loss: 0.9228 - disc_loss: 0.6780 - beta: 1.0000 - raw_loss: 2.0363 - w_kl_loss: 0.7302 - w_disc_loss: 125.2477 - gamma: 184.7500 - val_loss: 171.3280 - val_reco_loss: 4.5503 - val_kl_loss: 0.9705 - val_raw_loss: 5.5208 - val_disc_loss: 0.6741 - val_w_kl_loss: 0.9705 - val_w_disc_loss: 165.8072 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 189.0823 - reco_loss: 1.3500 - kl_loss: 0.8930 - disc_loss: 0.6756 - beta: 0.9900 - raw_loss: 2.1966 - w_kl_loss: 0.6313 - w_disc_loss: 207.5784 - gamma: 307.2500 - val_loss: 253.3813 - val_reco_loss: 4.6032 - val_kl_loss: 0.9705 - val_raw_loss: 5.5737 - val_disc_loss: 0.6726 - val_w_kl_loss: 0.9608 - val_w_disc_loss: 247.8173 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 270.8749 - reco_loss: 1.0159 - kl_loss: 0.9386 - disc_loss: 0.6737 - beta: 1.0000 - raw_loss: 1.9967 - w_kl_loss: 0.7433 - w_disc_loss: 289.5124 - gamma: 429.7500 - val_loss: 335.3137 - val_reco_loss: 4.4428 - val_kl_loss: 0.9709 - val_raw_loss: 5.4137 - val_disc_loss: 0.6720 - val_w_kl_loss: 0.9709 - val_w_disc_loss: 329.9000 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 352.6534 - reco_loss: 1.3876 - kl_loss: 0.8993 - disc_loss: 0.6718 - beta: 0.9900 - raw_loss: 2.2326 - w_kl_loss: 0.6301 - w_disc_loss: 370.9954 - gamma: 552.2500 - val_loss: 416.6790 - val_reco_loss: 4.7530 - val_kl_loss: 0.9717 - val_raw_loss: 5.7247 - val_disc_loss: 0.6699 - val_w_kl_loss: 0.9619 - val_w_disc_loss: 410.9640 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 433.5751 - reco_loss: 0.9799 - kl_loss: 0.9518 - disc_loss: 0.6701 - beta: 1.0000 - raw_loss: 1.9456 - w_kl_loss: 0.7326 - w_disc_loss: 452.1530 - gamma: 674.7500 - val_loss: 497.7352 - val_reco_loss: 4.7166 - val_kl_loss: 0.9728 - val_raw_loss: 5.6894 - val_disc_loss: 0.6686 - val_w_kl_loss: 0.9728 - val_w_disc_loss: 492.0459 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 514.6814 - reco_loss: 1.3063 - kl_loss: 0.9278 - disc_loss: 0.6685 - beta: 0.9900 - raw_loss: 2.1695 - w_kl_loss: 0.6450 - w_disc_loss: 532.9878 - gamma: 797.2500 - val_loss: 577.1260 - val_reco_loss: 4.6878 - val_kl_loss: 0.9739 - val_raw_loss: 5.6617 - val_disc_loss: 0.6657 - val_w_kl_loss: 0.9642 - val_w_disc_loss: 571.4739 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 594.9682 - reco_loss: 1.0354 - kl_loss: 0.9432 - disc_loss: 0.6670 - beta: 1.0000 - raw_loss: 1.9709 - w_kl_loss: 0.7098 - w_disc_loss: 613.4243 - gamma: 919.7500 - val_loss: 658.8400 - val_reco_loss: 4.6344 - val_kl_loss: 0.9759 - val_raw_loss: 5.6102 - val_disc_loss: 0.6659 - val_w_kl_loss: 0.9759 - val_w_disc_loss: 653.2298 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2494/2500 [============================>.] - ETA: 0s - loss: 675.1680 - reco_loss: 1.3744 - kl_loss: 0.9042 - disc_loss: 0.6652 - beta: 0.9300 - raw_loss: 2.2553 - w_kl_loss: 0.6564 - w_disc_loss: 693.2308 - gamma: 1042.0785\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 675.2235 - reco_loss: 1.3739 - kl_loss: 0.9041 - disc_loss: 0.6652 - beta: 0.9900 - raw_loss: 2.2549 - w_kl_loss: 0.6570 - w_disc_loss: 693.3437 - gamma: 1042.2500 - val_loss: 735.5498 - val_reco_loss: 4.3812 - val_kl_loss: 0.9769 - val_raw_loss: 5.3582 - val_disc_loss: 0.6617 - val_w_kl_loss: 0.9672 - val_w_disc_loss: 730.2014 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 755.0143 - reco_loss: 1.2011 - kl_loss: 0.9167 - disc_loss: 0.6638 - beta: 1.0000 - raw_loss: 2.1391 - w_kl_loss: 0.7121 - w_disc_loss: 773.2055 - gamma: 1164.7500 - val_loss: 819.6354 - val_reco_loss: 4.4256 - val_kl_loss: 0.9782 - val_raw_loss: 5.4037 - val_disc_loss: 0.6642 - val_w_kl_loss: 0.9782 - val_w_disc_loss: 814.2316 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 835.1914 - reco_loss: 1.3265 - kl_loss: 0.9193 - disc_loss: 0.6629 - beta: 0.9900 - raw_loss: 2.2044 - w_kl_loss: 0.6559 - w_disc_loss: 853.2886 - gamma: 1287.2500 - val_loss: 895.1565 - val_reco_loss: 4.6362 - val_kl_loss: 0.9790 - val_raw_loss: 5.6153 - val_disc_loss: 0.6597 - val_w_kl_loss: 0.9692 - val_w_disc_loss: 889.5510 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 914.8306 - reco_loss: 1.0260 - kl_loss: 0.9423 - disc_loss: 0.6619 - beta: 1.0000 - raw_loss: 2.0075 - w_kl_loss: 0.7474 - w_disc_loss: 933.0947 - gamma: 1409.7499 - val_loss: 981.1445 - val_reco_loss: 4.4588 - val_kl_loss: 0.9797 - val_raw_loss: 5.4386 - val_disc_loss: 0.6633 - val_w_kl_loss: 0.9797 - val_w_disc_loss: 975.7060 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 995.0502 - reco_loss: 1.5325 - kl_loss: 0.8920 - disc_loss: 0.6611 - beta: 0.9900 - raw_loss: 2.3718 - w_kl_loss: 0.6254 - w_disc_loss: 1012.9254 - gamma: 1532.2499 - val_loss: 1063.3805 - val_reco_loss: 4.4440 - val_kl_loss: 0.9811 - val_raw_loss: 5.4252 - val_disc_loss: 0.6639 - val_w_kl_loss: 0.9713 - val_w_disc_loss: 1057.9651 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1074.4190 - reco_loss: 1.1979 - kl_loss: 0.9186 - disc_loss: 0.6603 - beta: 1.0000 - raw_loss: 2.0632 - w_kl_loss: 0.6571 - w_disc_loss: 1092.5769 - gamma: 1654.7499 - val_loss: 1134.5957 - val_reco_loss: 4.6786 - val_kl_loss: 0.9816 - val_raw_loss: 5.6602 - val_disc_loss: 0.6579 - val_w_kl_loss: 0.9816 - val_w_disc_loss: 1128.9354 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1153.6426 - reco_loss: 1.2906 - kl_loss: 0.9007 - disc_loss: 0.6593 - beta: 0.9900 - raw_loss: 2.1480 - w_kl_loss: 0.6389 - w_disc_loss: 1171.6855 - gamma: 1777.2499 - val_loss: 1208.4318 - val_reco_loss: 4.7781 - val_kl_loss: 0.9824 - val_raw_loss: 5.7605 - val_disc_loss: 0.6542 - val_w_kl_loss: 0.9725 - val_w_disc_loss: 1202.6810 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1232.6241 - reco_loss: 1.0460 - kl_loss: 0.9384 - disc_loss: 0.6584 - beta: 1.0000 - raw_loss: 1.9932 - w_kl_loss: 0.7204 - w_disc_loss: 1250.7948 - gamma: 1899.7499 - val_loss: 1292.1725 - val_reco_loss: 4.8421 - val_kl_loss: 0.9838 - val_raw_loss: 5.8259 - val_disc_loss: 0.6560 - val_w_kl_loss: 0.9838 - val_w_disc_loss: 1286.3466 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2497/2500 [============================>.] - ETA: 0s - loss: 1311.4847 - reco_loss: 1.3039 - kl_loss: 0.9306 - disc_loss: 0.6574 - beta: 0.9600 - raw_loss: 2.1620 - w_kl_loss: 0.6390 - w_disc_loss: 1329.4265 - gamma: 2022.1520\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1311.5163 - reco_loss: 1.3037 - kl_loss: 0.9305 - disc_loss: 0.6574 - beta: 0.9900 - raw_loss: 2.1619 - w_kl_loss: 0.6395 - w_disc_loss: 1329.4903 - gamma: 2022.2499 - val_loss: 1363.4114 - val_reco_loss: 4.6332 - val_kl_loss: 0.9850 - val_raw_loss: 5.6181 - val_disc_loss: 0.6517 - val_w_kl_loss: 0.9751 - val_w_disc_loss: 1357.8031 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1390.3922 - reco_loss: 1.1139 - kl_loss: 0.9303 - disc_loss: 0.6567 - beta: 1.0000 - raw_loss: 1.9945 - w_kl_loss: 0.6683 - w_disc_loss: 1408.5102 - gamma: 2144.7499 - val_loss: 1452.3562 - val_reco_loss: 4.5040 - val_kl_loss: 0.9856 - val_raw_loss: 5.4896 - val_disc_loss: 0.6559 - val_w_kl_loss: 0.9856 - val_w_disc_loss: 1446.8666 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1470.1734 - reco_loss: 1.4764 - kl_loss: 0.8981 - disc_loss: 0.6563 - beta: 0.9900 - raw_loss: 2.3217 - w_kl_loss: 0.6289 - w_disc_loss: 1487.9512 - gamma: 2267.2499 - val_loss: 1533.3247 - val_reco_loss: 4.6180 - val_kl_loss: 0.9862 - val_raw_loss: 5.6042 - val_disc_loss: 0.6561 - val_w_kl_loss: 0.9764 - val_w_disc_loss: 1527.7303 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1548.9035 - reco_loss: 1.0989 - kl_loss: 0.9476 - disc_loss: 0.6557 - beta: 1.0000 - raw_loss: 2.0235 - w_kl_loss: 0.7033 - w_disc_loss: 1566.9605 - gamma: 2389.7499 - val_loss: 1617.5498 - val_reco_loss: 4.5707 - val_kl_loss: 0.9863 - val_raw_loss: 5.5570 - val_disc_loss: 0.6577 - val_w_kl_loss: 0.9863 - val_w_disc_loss: 1611.9928 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1628.5766 - reco_loss: 1.4155 - kl_loss: 0.8967 - disc_loss: 0.6553 - beta: 0.9900 - raw_loss: 2.3304 - w_kl_loss: 0.6819 - w_disc_loss: 1646.3148 - gamma: 2512.2499 - val_loss: 1684.8743 - val_reco_loss: 4.6732 - val_kl_loss: 0.9871 - val_raw_loss: 5.6603 - val_disc_loss: 0.6525 - val_w_kl_loss: 0.9772 - val_w_disc_loss: 1679.2239 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 21.4908 - reco_loss: 1.8705 - kl_loss: 0.8857 - disc_loss: 0.5941 - beta: 0.9900 - raw_loss: 2.7254 - w_kl_loss: 0.6378 - w_disc_loss: 36.9652 - gamma: 62.2500 - val_loss: 80.1735 - val_reco_loss: 5.7119 - val_kl_loss: 1.0066 - val_raw_loss: 6.7184 - val_disc_loss: 0.5951 - val_w_kl_loss: 0.9965 - val_w_disc_loss: 73.4652 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 93.4320 - reco_loss: 1.6123 - kl_loss: 0.8623 - disc_loss: 0.5902 - beta: 1.0000 - raw_loss: 2.4755 - w_kl_loss: 0.6566 - w_disc_loss: 109.0355 - gamma: 184.7500 - val_loss: 151.7080 - val_reco_loss: 6.1531 - val_kl_loss: 1.0098 - val_raw_loss: 7.1629 - val_disc_loss: 0.5877 - val_w_kl_loss: 1.0098 - val_w_disc_loss: 144.5451 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 165.6789 - reco_loss: 2.4553 - kl_loss: 0.8003 - disc_loss: 0.5874 - beta: 0.9900 - raw_loss: 3.2174 - w_kl_loss: 0.5682 - w_disc_loss: 180.4517 - gamma: 307.2500 - val_loss: 221.8737 - val_reco_loss: 5.6809 - val_kl_loss: 1.0130 - val_raw_loss: 6.6939 - val_disc_loss: 0.5840 - val_w_kl_loss: 1.0029 - val_w_disc_loss: 215.1899 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 235.7665 - reco_loss: 1.5986 - kl_loss: 0.8583 - disc_loss: 0.5845 - beta: 1.0000 - raw_loss: 2.4906 - w_kl_loss: 0.6779 - w_disc_loss: 251.1792 - gamma: 429.7500 - val_loss: 293.2929 - val_reco_loss: 5.8833 - val_kl_loss: 1.0180 - val_raw_loss: 6.9013 - val_disc_loss: 0.5833 - val_w_kl_loss: 1.0180 - val_w_disc_loss: 286.3916 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 306.3851 - reco_loss: 2.0958 - kl_loss: 0.8274 - disc_loss: 0.5819 - beta: 0.9900 - raw_loss: 2.8596 - w_kl_loss: 0.5691 - w_disc_loss: 321.3482 - gamma: 552.2500 - val_loss: 362.6233 - val_reco_loss: 5.5832 - val_kl_loss: 1.0219 - val_raw_loss: 6.6051 - val_disc_loss: 0.5804 - val_w_kl_loss: 1.0117 - val_w_disc_loss: 356.0284 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 375.6243 - reco_loss: 1.8221 - kl_loss: 0.8530 - disc_loss: 0.5790 - beta: 1.0000 - raw_loss: 2.6965 - w_kl_loss: 0.6625 - w_disc_loss: 390.6613 - gamma: 674.7500 - val_loss: 428.8749 - val_reco_loss: 5.7720 - val_kl_loss: 1.0267 - val_raw_loss: 6.7987 - val_disc_loss: 0.5735 - val_w_kl_loss: 1.0267 - val_w_disc_loss: 422.0763 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 444.7422 - reco_loss: 2.1152 - kl_loss: 0.8660 - disc_loss: 0.5763 - beta: 0.9900 - raw_loss: 2.9531 - w_kl_loss: 0.6249 - w_disc_loss: 459.4414 - gamma: 797.2500 - val_loss: 494.3441 - val_reco_loss: 5.9016 - val_kl_loss: 1.0320 - val_raw_loss: 6.9336 - val_disc_loss: 0.5678 - val_w_kl_loss: 1.0217 - val_w_disc_loss: 487.4208 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 512.5620 - reco_loss: 1.6179 - kl_loss: 0.9229 - disc_loss: 0.5736 - beta: 1.0000 - raw_loss: 2.5964 - w_kl_loss: 0.7449 - w_disc_loss: 527.5332 - gamma: 919.7500 - val_loss: 568.4184 - val_reco_loss: 5.7322 - val_kl_loss: 1.0360 - val_raw_loss: 6.7681 - val_disc_loss: 0.5726 - val_w_kl_loss: 1.0360 - val_w_disc_loss: 561.6503 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 580.5043 - reco_loss: 2.0226 - kl_loss: 0.8880 - disc_loss: 0.5711 - beta: 0.9800 - raw_loss: 2.8316 - w_kl_loss: 0.6024 - w_disc_loss: 595.1501 - gamma: 1042.2010\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 580.5176 - reco_loss: 2.0225 - kl_loss: 0.8879 - disc_loss: 0.5711 - beta: 0.9900 - raw_loss: 2.8315 - w_kl_loss: 0.6026 - w_disc_loss: 595.1773 - gamma: 1042.2500 - val_loss: 633.1994 - val_reco_loss: 5.7835 - val_kl_loss: 1.0409 - val_raw_loss: 6.8244 - val_disc_loss: 0.5677 - val_w_kl_loss: 1.0305 - val_w_disc_loss: 626.3854 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 647.3761 - reco_loss: 1.5540 - kl_loss: 0.9397 - disc_loss: 0.5687 - beta: 1.0000 - raw_loss: 2.4570 - w_kl_loss: 0.6861 - w_disc_loss: 662.3356 - gamma: 1164.7500 - val_loss: 693.7057 - val_reco_loss: 6.0851 - val_kl_loss: 1.0434 - val_raw_loss: 7.1285 - val_disc_loss: 0.5600 - val_w_kl_loss: 1.0434 - val_w_disc_loss: 686.5773 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 715.6577 - reco_loss: 2.1777 - kl_loss: 0.8773 - disc_loss: 0.5671 - beta: 0.9900 - raw_loss: 3.0121 - w_kl_loss: 0.6225 - w_disc_loss: 730.0139 - gamma: 1287.2500 - val_loss: 767.3870 - val_reco_loss: 5.7224 - val_kl_loss: 1.0462 - val_raw_loss: 6.7686 - val_disc_loss: 0.5641 - val_w_kl_loss: 1.0357 - val_w_disc_loss: 760.6289 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 782.9008 - reco_loss: 1.6602 - kl_loss: 0.8926 - disc_loss: 0.5659 - beta: 1.0000 - raw_loss: 2.5236 - w_kl_loss: 0.6571 - w_disc_loss: 797.7074 - gamma: 1409.7499 - val_loss: 835.7490 - val_reco_loss: 5.6221 - val_kl_loss: 1.0486 - val_raw_loss: 6.6707 - val_disc_loss: 0.5636 - val_w_kl_loss: 1.0486 - val_w_disc_loss: 829.0784 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 850.8238 - reco_loss: 2.2030 - kl_loss: 0.9218 - disc_loss: 0.5646 - beta: 0.9900 - raw_loss: 3.0555 - w_kl_loss: 0.6344 - w_disc_loss: 865.0604 - gamma: 1532.2499 - val_loss: 902.4442 - val_reco_loss: 5.9981 - val_kl_loss: 1.0512 - val_raw_loss: 7.0493 - val_disc_loss: 0.5619 - val_w_kl_loss: 1.0407 - val_w_disc_loss: 895.4055 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 917.2284 - reco_loss: 1.6352 - kl_loss: 0.9201 - disc_loss: 0.5632 - beta: 1.0000 - raw_loss: 2.5578 - w_kl_loss: 0.7006 - w_disc_loss: 931.9197 - gamma: 1654.7499 - val_loss: 976.8626 - val_reco_loss: 5.6933 - val_kl_loss: 1.0540 - val_raw_loss: 6.7472 - val_disc_loss: 0.5654 - val_w_kl_loss: 1.0540 - val_w_disc_loss: 970.1154 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 984.0104 - reco_loss: 2.0293 - kl_loss: 0.8873 - disc_loss: 0.5617 - beta: 0.9900 - raw_loss: 2.9180 - w_kl_loss: 0.6650 - w_disc_loss: 998.2968 - gamma: 1777.2499 - val_loss: 1032.2472 - val_reco_loss: 5.9845 - val_kl_loss: 1.0568 - val_raw_loss: 7.0412 - val_disc_loss: 0.5577 - val_w_kl_loss: 1.0462 - val_w_disc_loss: 1025.2166 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1049.9498 - reco_loss: 1.6505 - kl_loss: 0.9241 - disc_loss: 0.5604 - beta: 1.0000 - raw_loss: 2.5745 - w_kl_loss: 0.7009 - w_disc_loss: 1064.5360 - gamma: 1899.7499 - val_loss: 1113.9318 - val_reco_loss: 5.9082 - val_kl_loss: 1.0590 - val_raw_loss: 6.9672 - val_disc_loss: 0.5645 - val_w_kl_loss: 1.0590 - val_w_disc_loss: 1106.9646 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2491/2500 [============================>.] - ETA: 0s - loss: 1116.4072 - reco_loss: 2.0470 - kl_loss: 0.9237 - disc_loss: 0.5591 - beta: 0.9000 - raw_loss: 2.9050 - w_kl_loss: 0.6392 - w_disc_loss: 1130.5584 - gamma: 2022.0050\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1116.4712 - reco_loss: 2.0461 - kl_loss: 0.9233 - disc_loss: 0.5591 - beta: 0.9900 - raw_loss: 2.9043 - w_kl_loss: 0.6401 - w_disc_loss: 1130.6915 - gamma: 2022.2499 - val_loss: 1160.2153 - val_reco_loss: 6.1643 - val_kl_loss: 1.0616 - val_raw_loss: 7.2259 - val_disc_loss: 0.5534 - val_w_kl_loss: 1.0510 - val_w_disc_loss: 1153.0000 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1182.4859 - reco_loss: 1.6213 - kl_loss: 0.9353 - disc_loss: 0.5581 - beta: 1.0000 - raw_loss: 2.6515 - w_kl_loss: 0.7837 - w_disc_loss: 1196.9264 - gamma: 2144.7499 - val_loss: 1241.4493 - val_reco_loss: 5.8595 - val_kl_loss: 1.0625 - val_raw_loss: 6.9221 - val_disc_loss: 0.5596 - val_w_kl_loss: 1.0625 - val_w_disc_loss: 1234.5272 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1249.1656 - reco_loss: 1.9229 - kl_loss: 0.9199 - disc_loss: 0.5573 - beta: 0.9900 - raw_loss: 2.7861 - w_kl_loss: 0.6434 - w_disc_loss: 1263.4463 - gamma: 2267.2499 - val_loss: 1306.9073 - val_reco_loss: 5.8817 - val_kl_loss: 1.0640 - val_raw_loss: 6.9456 - val_disc_loss: 0.5583 - val_w_kl_loss: 1.0533 - val_w_disc_loss: 1299.9724 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1315.0980 - reco_loss: 1.7233 - kl_loss: 0.9158 - disc_loss: 0.5563 - beta: 1.0000 - raw_loss: 2.6915 - w_kl_loss: 0.7342 - w_disc_loss: 1329.4450 - gamma: 2389.7499 - val_loss: 1365.4821 - val_reco_loss: 6.0287 - val_kl_loss: 1.0654 - val_raw_loss: 7.0941 - val_disc_loss: 0.5542 - val_w_kl_loss: 1.0654 - val_w_disc_loss: 1358.3879 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1383.0412 - reco_loss: 2.0018 - kl_loss: 0.9269 - disc_loss: 0.5562 - beta: 0.9900 - raw_loss: 2.8883 - w_kl_loss: 0.6607 - w_disc_loss: 1397.1850 - gamma: 2512.2499 - val_loss: 1421.0596 - val_reco_loss: 6.3081 - val_kl_loss: 1.0659 - val_raw_loss: 7.3740 - val_disc_loss: 0.5493 - val_w_kl_loss: 1.0553 - val_w_disc_loss: 1413.6962 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 7 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 15.5695 - reco_loss: 1.5147 - kl_loss: 1.0547 - disc_loss: 0.4148 - beta: 0.9900 - raw_loss: 2.5312 - w_kl_loss: 0.7577 - w_disc_loss: 25.7522 - gamma: 62.2500 - val_loss: 58.5895 - val_reco_loss: 5.9028 - val_kl_loss: 1.3663 - val_raw_loss: 7.2691 - val_disc_loss: 0.4158 - val_w_kl_loss: 1.3526 - val_w_disc_loss: 51.3340 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 64.3674 - reco_loss: 1.2068 - kl_loss: 1.0994 - disc_loss: 0.4029 - beta: 1.0000 - raw_loss: 2.3332 - w_kl_loss: 0.8528 - w_disc_loss: 74.3806 - gamma: 184.7500 - val_loss: 102.3210 - val_reco_loss: 5.8366 - val_kl_loss: 1.3604 - val_raw_loss: 7.1971 - val_disc_loss: 0.3868 - val_w_kl_loss: 1.3604 - val_w_disc_loss: 95.1239 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 111.3952 - reco_loss: 1.5601 - kl_loss: 1.0362 - disc_loss: 0.3937 - beta: 0.9900 - raw_loss: 2.5324 - w_kl_loss: 0.7244 - w_disc_loss: 120.9278 - gamma: 307.2500 - val_loss: 143.8439 - val_reco_loss: 6.0223 - val_kl_loss: 1.3571 - val_raw_loss: 7.3794 - val_disc_loss: 0.3704 - val_w_kl_loss: 1.3435 - val_w_disc_loss: 136.4781 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 156.0301 - reco_loss: 1.2856 - kl_loss: 1.0700 - disc_loss: 0.3852 - beta: 1.0000 - raw_loss: 2.3461 - w_kl_loss: 0.8110 - w_disc_loss: 165.4890 - gamma: 429.7500 - val_loss: 194.1821 - val_reco_loss: 5.8687 - val_kl_loss: 1.3549 - val_raw_loss: 7.2236 - val_disc_loss: 0.3808 - val_w_kl_loss: 1.3549 - val_w_disc_loss: 186.9585 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 199.3869 - reco_loss: 1.5899 - kl_loss: 1.0341 - disc_loss: 0.3774 - beta: 0.9900 - raw_loss: 2.5896 - w_kl_loss: 0.7451 - w_disc_loss: 208.3630 - gamma: 552.2500 - val_loss: 233.6889 - val_reco_loss: 5.9682 - val_kl_loss: 1.3520 - val_raw_loss: 7.3202 - val_disc_loss: 0.3690 - val_w_kl_loss: 1.3385 - val_w_disc_loss: 226.3822 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 239.9871 - reco_loss: 1.2413 - kl_loss: 1.0527 - disc_loss: 0.3691 - beta: 1.0000 - raw_loss: 2.2653 - w_kl_loss: 0.7780 - w_disc_loss: 249.0295 - gamma: 674.7500 - val_loss: 275.7547 - val_reco_loss: 5.8635 - val_kl_loss: 1.3496 - val_raw_loss: 7.2131 - val_disc_loss: 0.3649 - val_w_kl_loss: 1.3496 - val_w_disc_loss: 268.5416 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 280.3513 - reco_loss: 1.6019 - kl_loss: 1.0196 - disc_loss: 0.3623 - beta: 0.9900 - raw_loss: 2.6215 - w_kl_loss: 0.7619 - w_disc_loss: 288.8286 - gamma: 797.2500 - val_loss: 315.1091 - val_reco_loss: 5.8545 - val_kl_loss: 1.3471 - val_raw_loss: 7.2016 - val_disc_loss: 0.3587 - val_w_kl_loss: 1.3336 - val_w_disc_loss: 307.9210 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 318.3086 - reco_loss: 1.1822 - kl_loss: 1.0610 - disc_loss: 0.3554 - beta: 1.0000 - raw_loss: 2.3251 - w_kl_loss: 0.8687 - w_disc_loss: 326.8722 - gamma: 919.7500 - val_loss: 347.3116 - val_reco_loss: 5.8237 - val_kl_loss: 1.3451 - val_raw_loss: 7.1688 - val_disc_loss: 0.3467 - val_w_kl_loss: 1.3451 - val_w_disc_loss: 340.1429 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2492/2500 [============================>.] - ETA: 0s - loss: 355.3000 - reco_loss: 1.5936 - kl_loss: 1.0352 - disc_loss: 0.3487 - beta: 0.9100 - raw_loss: 2.6491 - w_kl_loss: 0.7881 - w_disc_loss: 363.2952 - gamma: 1042.0295\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 355.3314 - reco_loss: 1.5928 - kl_loss: 1.0351 - disc_loss: 0.3487 - beta: 0.9900 - raw_loss: 2.6483 - w_kl_loss: 0.7888 - w_disc_loss: 363.3657 - gamma: 1042.2500 - val_loss: 394.1608 - val_reco_loss: 5.8913 - val_kl_loss: 1.3430 - val_raw_loss: 7.2342 - val_disc_loss: 0.3507 - val_w_kl_loss: 1.3295 - val_w_disc_loss: 386.9400 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 391.4965 - reco_loss: 1.1967 - kl_loss: 1.0596 - disc_loss: 0.3433 - beta: 1.0000 - raw_loss: 2.1965 - w_kl_loss: 0.7594 - w_disc_loss: 399.8147 - gamma: 1164.7500 - val_loss: 434.7743 - val_reco_loss: 5.8047 - val_kl_loss: 1.3416 - val_raw_loss: 7.1463 - val_disc_loss: 0.3488 - val_w_kl_loss: 1.3416 - val_w_disc_loss: 427.6280 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 429.6398 - reco_loss: 1.5984 - kl_loss: 1.0242 - disc_loss: 0.3399 - beta: 0.9900 - raw_loss: 2.5619 - w_kl_loss: 0.7185 - w_disc_loss: 437.4872 - gamma: 1287.2500 - val_loss: 453.4303 - val_reco_loss: 5.8822 - val_kl_loss: 1.3404 - val_raw_loss: 7.2226 - val_disc_loss: 0.3309 - val_w_kl_loss: 1.3270 - val_w_disc_loss: 446.2210 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 466.0830 - reco_loss: 1.3115 - kl_loss: 1.0152 - disc_loss: 0.3363 - beta: 1.0000 - raw_loss: 2.3397 - w_kl_loss: 0.7809 - w_disc_loss: 474.0444 - gamma: 1409.7499 - val_loss: 494.6375 - val_reco_loss: 5.7594 - val_kl_loss: 1.3397 - val_raw_loss: 7.0991 - val_disc_loss: 0.3314 - val_w_kl_loss: 1.3397 - val_w_disc_loss: 487.5384 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 503.7120 - reco_loss: 1.5579 - kl_loss: 1.0012 - disc_loss: 0.3338 - beta: 0.9900 - raw_loss: 2.5345 - w_kl_loss: 0.7284 - w_disc_loss: 511.4024 - gamma: 1532.2499 - val_loss: 533.7123 - val_reco_loss: 5.7382 - val_kl_loss: 1.3390 - val_raw_loss: 7.0773 - val_disc_loss: 0.3305 - val_w_kl_loss: 1.3256 - val_w_disc_loss: 526.6485 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 538.0170 - reco_loss: 1.1647 - kl_loss: 1.0655 - disc_loss: 0.3299 - beta: 1.0000 - raw_loss: 2.2136 - w_kl_loss: 0.7954 - w_disc_loss: 545.9121 - gamma: 1654.7499 - val_loss: 565.6017 - val_reco_loss: 5.8060 - val_kl_loss: 1.3381 - val_raw_loss: 7.1441 - val_disc_loss: 0.3255 - val_w_kl_loss: 1.3381 - val_w_disc_loss: 558.4576 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 573.7530 - reco_loss: 1.6674 - kl_loss: 1.0096 - disc_loss: 0.3270 - beta: 0.9900 - raw_loss: 2.6666 - w_kl_loss: 0.7447 - w_disc_loss: 581.1023 - gamma: 1777.2499 - val_loss: 614.0201 - val_reco_loss: 5.7693 - val_kl_loss: 1.3368 - val_raw_loss: 7.1061 - val_disc_loss: 0.3301 - val_w_kl_loss: 1.3235 - val_w_disc_loss: 606.9273 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 607.7359 - reco_loss: 1.2584 - kl_loss: 1.0293 - disc_loss: 0.3239 - beta: 1.0000 - raw_loss: 2.3164 - w_kl_loss: 0.8020 - w_disc_loss: 615.3432 - gamma: 1899.7499 - val_loss: 638.0132 - val_reco_loss: 5.6881 - val_kl_loss: 1.3360 - val_raw_loss: 7.0241 - val_disc_loss: 0.3218 - val_w_kl_loss: 1.3360 - val_w_disc_loss: 630.9891 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: 640.5667 - reco_loss: 1.5296 - kl_loss: 1.0346 - disc_loss: 0.3204 - beta: 0.9700 - raw_loss: 2.4446 - w_kl_loss: 0.6813 - w_disc_loss: 647.9232 - gamma: 2022.1765\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 640.5767 - reco_loss: 1.5294 - kl_loss: 1.0351 - disc_loss: 0.3204 - beta: 0.9900 - raw_loss: 2.4445 - w_kl_loss: 0.6817 - w_disc_loss: 647.9450 - gamma: 2022.2499 - val_loss: 668.9342 - val_reco_loss: 5.7956 - val_kl_loss: 1.3356 - val_raw_loss: 7.1312 - val_disc_loss: 0.3177 - val_w_kl_loss: 1.3222 - val_w_disc_loss: 661.8163 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 674.5473 - reco_loss: 1.2150 - kl_loss: 1.0421 - disc_loss: 0.3180 - beta: 1.0000 - raw_loss: 2.3148 - w_kl_loss: 0.8367 - w_disc_loss: 681.9726 - gamma: 2144.7499 - val_loss: 680.1377 - val_reco_loss: 5.6205 - val_kl_loss: 1.3352 - val_raw_loss: 6.9557 - val_disc_loss: 0.3052 - val_w_kl_loss: 1.3352 - val_w_disc_loss: 673.1819 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 711.3289 - reco_loss: 1.4587 - kl_loss: 1.0390 - disc_loss: 0.3169 - beta: 0.9900 - raw_loss: 2.5082 - w_kl_loss: 0.7811 - w_disc_loss: 718.5293 - gamma: 2267.2499 - val_loss: 735.0770 - val_reco_loss: 5.8969 - val_kl_loss: 1.3345 - val_raw_loss: 7.2314 - val_disc_loss: 0.3126 - val_w_kl_loss: 1.3212 - val_w_disc_loss: 727.8589 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 745.6313 - reco_loss: 1.2476 - kl_loss: 1.0345 - disc_loss: 0.3151 - beta: 1.0000 - raw_loss: 2.3029 - w_kl_loss: 0.8036 - w_disc_loss: 752.9798 - gamma: 2389.7499 - val_loss: 755.4877 - val_reco_loss: 5.8585 - val_kl_loss: 1.3340 - val_raw_loss: 7.1925 - val_disc_loss: 0.3053 - val_w_kl_loss: 1.3340 - val_w_disc_loss: 748.2952 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 779.8006 - reco_loss: 1.4955 - kl_loss: 1.0270 - disc_loss: 0.3132 - beta: 0.9900 - raw_loss: 2.4640 - w_kl_loss: 0.7225 - w_disc_loss: 786.9323 - gamma: 2512.2499 - val_loss: 788.7131 - val_reco_loss: 5.7094 - val_kl_loss: 1.3334 - val_raw_loss: 7.0429 - val_disc_loss: 0.3037 - val_w_kl_loss: 1.3201 - val_w_disc_loss: 781.6837 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 8 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 88.1638 - reco_loss: 1.8759 - kl_loss: 1.4859 - disc_loss: 2.7060 - beta: 0.9900 - raw_loss: 3.3405 - w_kl_loss: 1.0924 - w_disc_loss: 167.7735 - gamma: 62.2500 - val_loss: 358.5932 - val_reco_loss: 7.5119 - val_kl_loss: 3.0117 - val_raw_loss: 10.5236 - val_disc_loss: 2.8197 - val_w_kl_loss: 2.9816 - val_w_disc_loss: 348.0997 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 404.6245 - reco_loss: 1.3874 - kl_loss: 1.5410 - disc_loss: 2.6091 - beta: 1.0000 - raw_loss: 2.9187 - w_kl_loss: 1.1638 - w_disc_loss: 481.6259 - gamma: 184.7500 - val_loss: 662.7338 - val_reco_loss: 7.3833 - val_kl_loss: 2.9865 - val_raw_loss: 10.3698 - val_disc_loss: 2.6524 - val_w_kl_loss: 2.9865 - val_w_disc_loss: 652.3640 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 706.1387 - reco_loss: 1.7249 - kl_loss: 1.4835 - disc_loss: 2.5428 - beta: 0.9900 - raw_loss: 3.0829 - w_kl_loss: 1.0114 - w_disc_loss: 781.0046 - gamma: 307.2500 - val_loss: 985.8992 - val_reco_loss: 7.3199 - val_kl_loss: 2.9658 - val_raw_loss: 10.2857 - val_disc_loss: 2.6480 - val_w_kl_loss: 2.9361 - val_w_disc_loss: 975.6432 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 992.8511 - reco_loss: 1.3244 - kl_loss: 1.5316 - disc_loss: 2.4811 - beta: 1.0000 - raw_loss: 2.8953 - w_kl_loss: 1.1887 - w_disc_loss: 1066.0066 - gamma: 429.7500 - val_loss: 1257.1791 - val_reco_loss: 7.2436 - val_kl_loss: 2.9462 - val_raw_loss: 10.1897 - val_disc_loss: 2.5399 - val_w_kl_loss: 2.9462 - val_w_disc_loss: 1246.9894 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1267.1198 - reco_loss: 1.7512 - kl_loss: 1.4485 - disc_loss: 2.4239 - beta: 0.9900 - raw_loss: 3.0775 - w_kl_loss: 0.9889 - w_disc_loss: 1338.3223 - gamma: 552.2500 - val_loss: 1535.8033 - val_reco_loss: 7.1984 - val_kl_loss: 2.9271 - val_raw_loss: 10.1255 - val_disc_loss: 2.4871 - val_w_kl_loss: 2.8978 - val_w_disc_loss: 1525.7072 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1526.7844 - reco_loss: 1.3981 - kl_loss: 1.5000 - disc_loss: 2.3663 - beta: 1.0000 - raw_loss: 2.8162 - w_kl_loss: 1.0771 - w_disc_loss: 1596.4690 - gamma: 674.7500 - val_loss: 1855.3217 - val_reco_loss: 7.2281 - val_kl_loss: 2.9079 - val_raw_loss: 10.1360 - val_disc_loss: 2.5072 - val_w_kl_loss: 2.9079 - val_w_disc_loss: 1845.1857 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1779.3685 - reco_loss: 1.9522 - kl_loss: 1.4346 - disc_loss: 2.3172 - beta: 0.9900 - raw_loss: 3.2909 - w_kl_loss: 0.9972 - w_disc_loss: 1847.0714 - gamma: 797.2500 - val_loss: 2066.3335 - val_reco_loss: 7.1071 - val_kl_loss: 2.8911 - val_raw_loss: 9.9982 - val_disc_loss: 2.3954 - val_w_kl_loss: 2.8622 - val_w_disc_loss: 2056.3643 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2014.3808 - reco_loss: 1.5043 - kl_loss: 1.4295 - disc_loss: 2.2625 - beta: 1.0000 - raw_loss: 2.9957 - w_kl_loss: 1.1348 - w_disc_loss: 2080.7032 - gamma: 919.7500 - val_loss: 2303.9116 - val_reco_loss: 7.0649 - val_kl_loss: 2.8728 - val_raw_loss: 9.9377 - val_disc_loss: 2.3385 - val_w_kl_loss: 2.8728 - val_w_disc_loss: 2293.9739 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 2245.7442 - reco_loss: 1.7010 - kl_loss: 1.4226 - disc_loss: 2.2172 - beta: 0.9800 - raw_loss: 3.0493 - w_kl_loss: 1.0040 - w_disc_loss: 2310.6011 - gamma: 1042.2010\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2245.7893 - reco_loss: 1.7009 - kl_loss: 1.4225 - disc_loss: 2.2172 - beta: 0.9900 - raw_loss: 3.0493 - w_kl_loss: 1.0043 - w_disc_loss: 2310.7003 - gamma: 1042.2500 - val_loss: 2535.0989 - val_reco_loss: 7.0095 - val_kl_loss: 2.8570 - val_raw_loss: 9.8665 - val_disc_loss: 2.2885 - val_w_kl_loss: 2.8284 - val_w_disc_loss: 2525.2610 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2472.3583 - reco_loss: 1.3943 - kl_loss: 1.4676 - disc_loss: 2.1775 - beta: 1.0000 - raw_loss: 2.8653 - w_kl_loss: 1.1147 - w_disc_loss: 2536.2302 - gamma: 1164.7500 - val_loss: 2831.9148 - val_reco_loss: 6.9348 - val_kl_loss: 2.8482 - val_raw_loss: 9.7830 - val_disc_loss: 2.3020 - val_w_kl_loss: 2.8482 - val_w_disc_loss: 2822.1318 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2703.6411 - reco_loss: 1.7353 - kl_loss: 1.4035 - disc_loss: 2.1490 - beta: 0.9900 - raw_loss: 3.1248 - w_kl_loss: 1.0354 - w_disc_loss: 2766.3876 - gamma: 1287.2500 - val_loss: 3113.1387 - val_reco_loss: 7.0488 - val_kl_loss: 2.8398 - val_raw_loss: 9.8886 - val_disc_loss: 2.3014 - val_w_kl_loss: 2.8114 - val_w_disc_loss: 3103.2786 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2938.1430 - reco_loss: 1.3380 - kl_loss: 1.4804 - disc_loss: 2.1284 - beta: 1.0000 - raw_loss: 2.8510 - w_kl_loss: 1.1481 - w_disc_loss: 3000.4752 - gamma: 1409.7499 - val_loss: 3310.9265 - val_reco_loss: 7.0033 - val_kl_loss: 2.8319 - val_raw_loss: 9.8352 - val_disc_loss: 2.2442 - val_w_kl_loss: 2.8319 - val_w_disc_loss: 3301.0913 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 3164.7831 - reco_loss: 1.6752 - kl_loss: 1.3952 - disc_loss: 2.1056 - beta: 0.9900 - raw_loss: 2.9830 - w_kl_loss: 0.9765 - w_disc_loss: 3226.3185 - gamma: 1532.2499 - val_loss: 3537.0349 - val_reco_loss: 6.9760 - val_kl_loss: 2.8228 - val_raw_loss: 9.7988 - val_disc_loss: 2.2136 - val_w_kl_loss: 2.7946 - val_w_disc_loss: 3527.2644 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3387.3031 - reco_loss: 1.2194 - kl_loss: 1.4897 - disc_loss: 2.0840 - beta: 1.0000 - raw_loss: 2.7916 - w_kl_loss: 1.1980 - w_disc_loss: 3448.3503 - gamma: 1654.7499 - val_loss: 3748.0383 - val_reco_loss: 6.9299 - val_kl_loss: 2.8156 - val_raw_loss: 9.7455 - val_disc_loss: 2.1786 - val_w_kl_loss: 2.8156 - val_w_disc_loss: 3738.2927 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3610.8655 - reco_loss: 1.8059 - kl_loss: 1.3934 - disc_loss: 2.0657 - beta: 0.9900 - raw_loss: 3.1761 - w_kl_loss: 1.0232 - w_disc_loss: 3670.9592 - gamma: 1777.2499 - val_loss: 3858.0767 - val_reco_loss: 6.8578 - val_kl_loss: 2.8081 - val_raw_loss: 9.6659 - val_disc_loss: 2.0933 - val_w_kl_loss: 2.7800 - val_w_disc_loss: 3848.4390 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3818.7382 - reco_loss: 1.3658 - kl_loss: 1.4482 - disc_loss: 2.0416 - beta: 1.0000 - raw_loss: 2.9170 - w_kl_loss: 1.1734 - w_disc_loss: 3878.3724 - gamma: 1899.7499 - val_loss: 4195.8086 - val_reco_loss: 6.8534 - val_kl_loss: 2.8002 - val_raw_loss: 9.6536 - val_disc_loss: 2.1348 - val_w_kl_loss: 2.8002 - val_w_disc_loss: 4186.1553 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2493/2500 [============================>.] - ETA: 0s - loss: 4025.3503 - reco_loss: 1.6720 - kl_loss: 1.4086 - disc_loss: 2.0198 - beta: 0.9200 - raw_loss: 3.0517 - w_kl_loss: 1.0279 - w_disc_loss: 4083.9677 - gamma: 2022.0540\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4025.5028 - reco_loss: 1.6715 - kl_loss: 1.4080 - disc_loss: 2.0198 - beta: 0.9900 - raw_loss: 3.0514 - w_kl_loss: 1.0290 - w_disc_loss: 4084.3178 - gamma: 2022.2499 - val_loss: 4453.1616 - val_reco_loss: 6.8612 - val_kl_loss: 2.7916 - val_raw_loss: 9.6528 - val_disc_loss: 2.1328 - val_w_kl_loss: 2.7637 - val_w_disc_loss: 4443.5366 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4231.1197 - reco_loss: 1.4914 - kl_loss: 1.4145 - disc_loss: 2.0000 - beta: 1.0000 - raw_loss: 3.0122 - w_kl_loss: 1.1551 - w_disc_loss: 4289.3769 - gamma: 2144.7499 - val_loss: 4587.8330 - val_reco_loss: 6.8681 - val_kl_loss: 2.7882 - val_raw_loss: 9.6563 - val_disc_loss: 2.0754 - val_w_kl_loss: 2.7882 - val_w_disc_loss: 4578.1768 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4454.5405 - reco_loss: 1.7969 - kl_loss: 1.3973 - disc_loss: 1.9903 - beta: 0.9900 - raw_loss: 3.1276 - w_kl_loss: 0.9920 - w_disc_loss: 4512.3505 - gamma: 2267.2499 - val_loss: 4869.4863 - val_reco_loss: 6.7954 - val_kl_loss: 2.7867 - val_raw_loss: 9.5820 - val_disc_loss: 2.0872 - val_w_kl_loss: 2.7588 - val_w_disc_loss: 4859.9321 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4674.3860 - reco_loss: 1.2417 - kl_loss: 1.4686 - disc_loss: 1.9803 - beta: 1.0000 - raw_loss: 2.7218 - w_kl_loss: 1.1283 - w_disc_loss: 4732.3040 - gamma: 2389.7499 - val_loss: 5150.9771 - val_reco_loss: 6.8363 - val_kl_loss: 2.7839 - val_raw_loss: 9.6202 - val_disc_loss: 2.0977 - val_w_kl_loss: 2.7839 - val_w_disc_loss: 5141.3569 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 4892.6668 - reco_loss: 1.9141 - kl_loss: 1.3748 - disc_loss: 1.9703 - beta: 0.9900 - raw_loss: 3.2644 - w_kl_loss: 1.0081 - w_disc_loss: 4949.7692 - gamma: 2512.2499 - val_loss: 5314.3857 - val_reco_loss: 6.9557 - val_kl_loss: 2.7803 - val_raw_loss: 9.7360 - val_disc_loss: 2.0613 - val_w_kl_loss: 2.7525 - val_w_disc_loss: 5304.6777 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 9 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 23.2853 - reco_loss: 1.4654 - kl_loss: 0.6859 - disc_loss: 0.6715 - beta: 0.9900 - raw_loss: 2.1382 - w_kl_loss: 0.5012 - w_disc_loss: 41.7205 - gamma: 62.2500 - val_loss: 86.5849 - val_reco_loss: 5.2600 - val_kl_loss: 0.8915 - val_raw_loss: 6.1515 - val_disc_loss: 0.6516 - val_w_kl_loss: 0.8826 - val_w_disc_loss: 80.4423 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 103.5482 - reco_loss: 1.1560 - kl_loss: 0.7018 - disc_loss: 0.6597 - beta: 1.0000 - raw_loss: 1.9271 - w_kl_loss: 0.5887 - w_disc_loss: 121.8301 - gamma: 184.7500 - val_loss: 167.7991 - val_reco_loss: 5.4118 - val_kl_loss: 0.8912 - val_raw_loss: 6.3030 - val_disc_loss: 0.6566 - val_w_kl_loss: 0.8912 - val_w_disc_loss: 161.4960 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 182.1484 - reco_loss: 1.4447 - kl_loss: 0.6913 - disc_loss: 0.6510 - beta: 0.9900 - raw_loss: 2.0825 - w_kl_loss: 0.4757 - w_disc_loss: 200.0121 - gamma: 307.2500 - val_loss: 245.2265 - val_reco_loss: 5.2380 - val_kl_loss: 0.8912 - val_raw_loss: 6.1292 - val_disc_loss: 0.6489 - val_w_kl_loss: 0.8823 - val_w_disc_loss: 239.1062 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 259.1503 - reco_loss: 1.1081 - kl_loss: 0.7246 - disc_loss: 0.6447 - beta: 1.0000 - raw_loss: 1.8747 - w_kl_loss: 0.5807 - w_disc_loss: 277.0251 - gamma: 429.7500 - val_loss: 314.0783 - val_reco_loss: 5.1001 - val_kl_loss: 0.8912 - val_raw_loss: 5.9913 - val_disc_loss: 0.6275 - val_w_kl_loss: 0.8912 - val_w_disc_loss: 308.0871 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 334.8754 - reco_loss: 1.5509 - kl_loss: 0.6737 - disc_loss: 0.6379 - beta: 0.9900 - raw_loss: 2.1941 - w_kl_loss: 0.4797 - w_disc_loss: 352.2258 - gamma: 552.2500 - val_loss: 389.3248 - val_reco_loss: 5.0856 - val_kl_loss: 0.8913 - val_raw_loss: 5.9769 - val_disc_loss: 0.6249 - val_w_kl_loss: 0.8824 - val_w_disc_loss: 383.3568 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 408.8571 - reco_loss: 1.1653 - kl_loss: 0.7050 - disc_loss: 0.6319 - beta: 1.0000 - raw_loss: 1.9029 - w_kl_loss: 0.5599 - w_disc_loss: 426.3120 - gamma: 674.7500 - val_loss: 466.9854 - val_reco_loss: 5.2275 - val_kl_loss: 0.8916 - val_raw_loss: 6.1191 - val_disc_loss: 0.6262 - val_w_kl_loss: 0.8916 - val_w_disc_loss: 460.8662 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 481.6854 - reco_loss: 1.4149 - kl_loss: 0.6893 - disc_loss: 0.6257 - beta: 0.9900 - raw_loss: 2.0491 - w_kl_loss: 0.4730 - w_disc_loss: 498.8027 - gamma: 797.2500 - val_loss: 550.1667 - val_reco_loss: 5.1850 - val_kl_loss: 0.8918 - val_raw_loss: 6.0767 - val_disc_loss: 0.6338 - val_w_kl_loss: 0.8829 - val_w_disc_loss: 544.0989 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 552.9482 - reco_loss: 1.1477 - kl_loss: 0.7110 - disc_loss: 0.6198 - beta: 1.0000 - raw_loss: 1.8670 - w_kl_loss: 0.5450 - w_disc_loss: 570.0662 - gamma: 919.7500 - val_loss: 629.6570 - val_reco_loss: 5.2722 - val_kl_loss: 0.8921 - val_raw_loss: 6.1643 - val_disc_loss: 0.6356 - val_w_kl_loss: 0.8921 - val_w_disc_loss: 623.4927 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-07\n",
      "Epoch 9/100\n",
      "2492/2500 [============================>.] - ETA: 0s - loss: 624.5219 - reco_loss: 1.7324 - kl_loss: 0.6488 - disc_loss: 0.6151 - beta: 0.9100 - raw_loss: 2.3701 - w_kl_loss: 0.4747 - w_disc_loss: 640.9246 - gamma: 1042.0295\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 5.000000058430487e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 624.5835 - reco_loss: 1.7313 - kl_loss: 0.6488 - disc_loss: 0.6151 - beta: 0.9900 - raw_loss: 2.3690 - w_kl_loss: 0.4752 - w_disc_loss: 641.0549 - gamma: 1042.2500 - val_loss: 685.8773 - val_reco_loss: 5.1298 - val_kl_loss: 0.8926 - val_raw_loss: 6.0224 - val_disc_loss: 0.6161 - val_w_kl_loss: 0.8837 - val_w_disc_loss: 679.8638 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-07\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 694.6334 - reco_loss: 1.1495 - kl_loss: 0.7063 - disc_loss: 0.6108 - beta: 1.0000 - raw_loss: 1.8841 - w_kl_loss: 0.5603 - w_disc_loss: 711.4597 - gamma: 1164.7500 - val_loss: 755.9510 - val_reco_loss: 5.1389 - val_kl_loss: 0.8929 - val_raw_loss: 6.0317 - val_disc_loss: 0.6117 - val_w_kl_loss: 0.8929 - val_w_disc_loss: 749.9193 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 766.0840 - reco_loss: 1.4827 - kl_loss: 0.6864 - disc_loss: 0.6079 - beta: 0.9900 - raw_loss: 2.1441 - w_kl_loss: 0.4933 - w_disc_loss: 782.5587 - gamma: 1287.2500 - val_loss: 834.8508 - val_reco_loss: 5.0933 - val_kl_loss: 0.8929 - val_raw_loss: 5.9863 - val_disc_loss: 0.6147 - val_w_kl_loss: 0.8840 - val_w_disc_loss: 828.8735 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 837.1963 - reco_loss: 1.1637 - kl_loss: 0.7167 - disc_loss: 0.6057 - beta: 1.0000 - raw_loss: 1.8941 - w_kl_loss: 0.5569 - w_disc_loss: 853.8529 - gamma: 1409.7499 - val_loss: 891.9570 - val_reco_loss: 5.2593 - val_kl_loss: 0.8931 - val_raw_loss: 6.1525 - val_disc_loss: 0.6022 - val_w_kl_loss: 0.8931 - val_w_disc_loss: 885.8045 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 907.7035 - reco_loss: 1.4369 - kl_loss: 0.7027 - disc_loss: 0.6031 - beta: 0.9900 - raw_loss: 2.1009 - w_kl_loss: 0.4953 - w_disc_loss: 924.0755 - gamma: 1532.2499 - val_loss: 962.3071 - val_reco_loss: 5.0405 - val_kl_loss: 0.8937 - val_raw_loss: 5.9342 - val_disc_loss: 0.6002 - val_w_kl_loss: 0.8848 - val_w_disc_loss: 956.3818 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 978.0151 - reco_loss: 1.0892 - kl_loss: 0.7226 - disc_loss: 0.6011 - beta: 1.0000 - raw_loss: 1.7450 - w_kl_loss: 0.4980 - w_disc_loss: 994.6805 - gamma: 1654.7499 - val_loss: 1035.7841 - val_reco_loss: 5.0705 - val_kl_loss: 0.8939 - val_raw_loss: 5.9644 - val_disc_loss: 0.6001 - val_w_kl_loss: 0.8939 - val_w_disc_loss: 1029.8197 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1046.9904 - reco_loss: 1.3845 - kl_loss: 0.6838 - disc_loss: 0.5983 - beta: 0.9900 - raw_loss: 2.0519 - w_kl_loss: 0.4985 - w_disc_loss: 1063.2637 - gamma: 1777.2499 - val_loss: 1095.0753 - val_reco_loss: 5.2289 - val_kl_loss: 0.8941 - val_raw_loss: 6.1230 - val_disc_loss: 0.5923 - val_w_kl_loss: 0.8852 - val_w_disc_loss: 1088.9613 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1116.1126 - reco_loss: 1.2474 - kl_loss: 0.6932 - disc_loss: 0.5961 - beta: 1.0000 - raw_loss: 1.9835 - w_kl_loss: 0.5605 - w_disc_loss: 1132.3901 - gamma: 1899.7499 - val_loss: 1161.7417 - val_reco_loss: 5.0090 - val_kl_loss: 0.8947 - val_raw_loss: 5.9036 - val_disc_loss: 0.5894 - val_w_kl_loss: 0.8947 - val_w_disc_loss: 1155.8380 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-08\n",
      "Epoch 17/100\n",
      "2499/2500 [============================>.] - ETA: 0s - loss: 1184.8094 - reco_loss: 1.4300 - kl_loss: 0.7072 - disc_loss: 0.5939 - beta: 0.9800 - raw_loss: 2.0941 - w_kl_loss: 0.4948 - w_disc_loss: 1200.8900 - gamma: 2022.2010\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.5000000292152436e-08.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1184.8229 - reco_loss: 1.4299 - kl_loss: 0.7072 - disc_loss: 0.5939 - beta: 0.9900 - raw_loss: 2.0940 - w_kl_loss: 0.4950 - w_disc_loss: 1200.9180 - gamma: 2022.2499 - val_loss: 1253.2753 - val_reco_loss: 5.1394 - val_kl_loss: 0.8947 - val_raw_loss: 6.0341 - val_disc_loss: 0.5986 - val_w_kl_loss: 0.8857 - val_w_disc_loss: 1247.2501 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-08\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1252.5624 - reco_loss: 1.1052 - kl_loss: 0.7195 - disc_loss: 0.5916 - beta: 1.0000 - raw_loss: 1.8081 - w_kl_loss: 0.5330 - w_disc_loss: 1268.8746 - gamma: 2144.7499 - val_loss: 1315.1851 - val_reco_loss: 5.2448 - val_kl_loss: 0.8950 - val_raw_loss: 6.1399 - val_disc_loss: 0.5934 - val_w_kl_loss: 0.8950 - val_w_disc_loss: 1309.0452 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1323.0217 - reco_loss: 1.4877 - kl_loss: 0.7077 - disc_loss: 0.5906 - beta: 0.9900 - raw_loss: 2.1592 - w_kl_loss: 0.5013 - w_disc_loss: 1338.9461 - gamma: 2267.2499 - val_loss: 1355.4027 - val_reco_loss: 5.1314 - val_kl_loss: 0.8950 - val_raw_loss: 6.0264 - val_disc_loss: 0.5795 - val_w_kl_loss: 0.8860 - val_w_disc_loss: 1349.3853 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1392.7586 - reco_loss: 1.1522 - kl_loss: 0.7013 - disc_loss: 0.5896 - beta: 1.0000 - raw_loss: 1.9065 - w_kl_loss: 0.5724 - w_disc_loss: 1408.9085 - gamma: 2389.7499 - val_loss: 1495.7745 - val_reco_loss: 4.9787 - val_kl_loss: 0.8950 - val_raw_loss: 5.8737 - val_disc_loss: 0.6079 - val_w_kl_loss: 0.8950 - val_w_disc_loss: 1489.9009 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-08\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1462.8019 - reco_loss: 1.4087 - kl_loss: 0.7099 - disc_loss: 0.5886 - beta: 0.9900 - raw_loss: 2.0861 - w_kl_loss: 0.5060 - w_disc_loss: 1478.7427 - gamma: 2512.2499 - val_loss: 1521.7777 - val_reco_loss: 5.1246 - val_kl_loss: 0.8953 - val_raw_loss: 6.0199 - val_disc_loss: 0.5890 - val_w_kl_loss: 0.8863 - val_w_disc_loss: 1515.7667 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-08\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "NUM_TRAIN = 10 \n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt9/\" #\n",
    "\n",
    "lr = 0.0000001\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "# 7: Learning rate set to 0.00001, \n",
    "# 8: Learning rate set to 0.000001\n",
    "# 9: Learning rate set to 0.0000001\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ec6e1",
   "metadata": {},
   "source": [
    "Moving over to large parametric sweep to find something that will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# NUM_TRAIN = 4 # Train just once for now\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/attempt6/\" #\n",
    "train = False\n",
    "save = True\n",
    "NUM_TRAIN = 3 # Train just once for now\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "parameters = [0]\n",
    "parameters_key = \"max_gamma\"\n",
    "SAVE_PATH = SAVE_PATH + parameters_key + \"/\"\n",
    "\n",
    "n_train = 3 # Train at least 3 models per parameter\n",
    "\n",
    "if train:\n",
    "    for param in parameters:\n",
    "        for i in range(n_train):\n",
    "            save_path = SAVE_PATH + f\"{parameters_key}_{param}/\"\n",
    "\n",
    "            # Manually make the directories and file. Python can do it, but its cleaner to do it manually\n",
    "            with open(SAVE_PATH +\"out.txt\", \"a\") as f:\n",
    "                print(f\"Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n\", file=f)\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            disc = Qmake_discriminator(INPUT_SZ, 8, 2) # Testing out these values for now\n",
    "\n",
    "            steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "            vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "            history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "            # Make loss plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            # Plot training losses\n",
    "            for key, val in history.history.items():\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                plt.plot(val, label=key, \n",
    "                        linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "                \n",
    "            # Customize the plot\n",
    "            plt.title(f'Variant: {parameters_key} = {param} Training and Validation Losses Run: {i}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.semilogy()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "            # Iterative training. \n",
    "            # save_path = save_path + f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "            if save:\n",
    "                iter_save_path = save_path +  f\"n_{i}/\"\n",
    "\n",
    "                # Save progress to main out file\n",
    "                with open(SAVE_PATH + \"out.txt\", \"a\") as f: \n",
    "                    print(f\"SAVING Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n Save path: {iter_save_path}\\n\", file=f, flush = True)\n",
    "\n",
    "                # Save weights to iter specific folder\n",
    "                vae.save_weights(filepath=iter_save_path , save_format='tf')\n",
    "                # Now save the histories\n",
    "                with open(iter_save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                    pkl.dump(history.history, f)\n",
    "                plt.savefig(iter_save_path + parameters_key + f\"_{param}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reco_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reco_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "    gamma = history['gamma']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "    # plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    # plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    # plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "    # plt.plot(history['beta'],label=\"beta\")\n",
    "    # plt.plot(history['gamma'], label=\"$\\gamma$\")\n",
    "\n",
    "    # # Plot validation losses\n",
    "    # plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    # plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    # plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want as a AD metric? the discriminator or latent space vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
