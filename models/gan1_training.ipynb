{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:24:12.382386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-14 16:24:13.682316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7933052",
   "metadata": {},
   "source": [
    "Instead of importing we will define the model in house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ac8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmake_discriminator(input_dim, h_dim_1, h_dim_2):\n",
    "    l2_factor = 1e-3\n",
    "    inputs = keras.Input(shape=(input_dim))\n",
    "    x = Dense(h_dim_1,\n",
    "              activation='relu',\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(inputs)\n",
    "    x = Dense(h_dim_2,\n",
    "              activation='relu',\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(x)\n",
    "    x = Dense(1,\n",
    "              activation='sigmoid',  # Output probability\n",
    "              kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "              bias_initializer=keras.initializers.Zeros())(x)\n",
    "    discriminator = keras.Model(inputs, x, name='discriminator')\n",
    "    return discriminator\n",
    "def custom_mse_loss_with_multi_index_scaling(masked_data, masked_reconstruction):\n",
    "    jet_scale = 1\n",
    "    tau_scale = 1\n",
    "    muon_sacle = 1\n",
    "    met_scale = 1\n",
    "\n",
    "    # Define the indices and their corresponding scale factors\n",
    "    scale_dict = {\n",
    "        0: jet_scale,\n",
    "        3: jet_scale,\n",
    "        6: jet_scale,\n",
    "        9: jet_scale,\n",
    "        12: jet_scale,\n",
    "        15: jet_scale,\n",
    "        18: tau_scale,\n",
    "        21: tau_scale,\n",
    "        24: tau_scale,\n",
    "        27: tau_scale,\n",
    "        30: muon_sacle,\n",
    "        33: muon_sacle,\n",
    "        36: muon_sacle,\n",
    "        39: muon_sacle,\n",
    "        42: met_scale\n",
    "    }\n",
    "    \n",
    "    # Create the scaling tensor\n",
    "    scale_tensor = tf.ones_like(masked_data)\n",
    "    \n",
    "    for index, factor in scale_dict.items():\n",
    "        index_mask = tf.one_hot(index, depth=tf.shape(masked_data)[-1])\n",
    "        scale_tensor += index_mask * (factor - 1)\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaled_data = masked_data * scale_tensor\n",
    "    scaled_reconstruction = masked_reconstruction * scale_tensor\n",
    "    \n",
    "#     # Hardcoded lists for eta and phi indices\n",
    "#     eta_indices = [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40]\n",
    "#     phi_indices = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 43]\n",
    "\n",
    "#     batch_size = tf.shape(scaled_reconstruction)[0]\n",
    "    \n",
    "#     # Apply constraints to eta\n",
    "#     for i in eta_indices:\n",
    "#         indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "#         updates = 3 * tf.tanh(scaled_reconstruction[:, i] / 3)\n",
    "#         scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "#     # Apply constraints to phi\n",
    "#     for i in phi_indices:\n",
    "#         indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "#         updates = 3.14159265258979*(10/8) * tf.tanh(scaled_reconstruction[:, i] / (3.14159265258979*(10/8)))\n",
    "#         scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    # Calculate MSE using keras.losses.mse\n",
    "    mse = keras.losses.mse(scaled_data, scaled_reconstruction)\n",
    "    \n",
    "    # Take the mean across all dimensions\n",
    "    return tf.reduce_mean(mse)\n",
    "\n",
    "class VAE_GAN_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator, steps_per_epoch=20,cycle_length=20, min_beta=0, max_beta=1,min_gamma=0, max_gamma=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        # per keras VAE example https://keras.io/examples/generative/vae/\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.gamma_tracker = keras.metrics.Mean(name=\"gamma\")\n",
    "\n",
    "\n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "\n",
    "        self.min_gamma = tf.cast(min_gamma, tf.float32)\n",
    "        self.max_gamma = tf.cast(max_gamma, tf.float32)\n",
    "        self.gamma = tf.Variable(min_gamma, dtype=tf.float32)\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super(VAE_GAN_Model, self).compile(**kwargs)\n",
    "        # Set the optimizer for the entire model (encoder + decoder + discriminator)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Collect trainable variables from encoder, decoder, and discriminator\n",
    "        trainable_variables = (\n",
    "            self.encoder.trainable_weights + \n",
    "            self.decoder.trainable_weights + \n",
    "            self.discriminator.trainable_weights\n",
    "        )\n",
    "        # Build the optimizer with the full variable list\n",
    "        self.optimizer.build(trainable_variables)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        # For first half (x < 0.5), scale 2x from 0 to 1\n",
    "        # For second half (x >= 0.5), stay at 1\n",
    "        scaled_x = tf.where(x < 0.5, 2.0 * x, 1.0)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * scaled_x\n",
    "    \n",
    "    def get_gamma_schedule(self, epoch):\n",
    "        # Convert to float32 for TF operations\n",
    "        epoch = tf.cast(epoch, tf.float32)\n",
    "        \n",
    "        # Calculate annealing progress\n",
    "        anneal_progress = (epoch - 50.0) / 50.0\n",
    "        gamma_anneal = self.min_gamma + (self.max_gamma - self.min_gamma) * anneal_progress\n",
    "        \n",
    "        # Implement the conditions using tf.where\n",
    "        gamma = tf.where(epoch < 50.0, \n",
    "                        0.0,  # if epoch < 50\n",
    "                        tf.where(epoch >= 100.0,\n",
    "                                self.max_gamma,  # if epoch >= 100\n",
    "                                gamma_anneal))   # if 50 <= epoch < 100\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Is this the beta tuning?\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "        self.gamma.assign(self.get_gamma_schedule(epoch))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the Discriminator\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape_disc:\n",
    "            # Generate reconstructed data\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Get discriminator predictions\n",
    "            real_output = self.discriminator(data)\n",
    "            fake_output = self.discriminator(reconstruction * mask)\n",
    "            \n",
    "            # Labels for real and fake data\n",
    "            real_labels = tf.ones_like(real_output)\n",
    "            fake_labels = tf.zeros_like(fake_output)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "            d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        grads_disc = tape_disc.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_weights))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the VAE (Generator)\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # here we shove in our custom reconstructionn loss function\n",
    "            # Ignore zero-padded entries. \n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx()) \n",
    "            reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*reconstruction, mask*data)\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            # This is just standard Kullback-Leibler diversion loss. I think this can stay.\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "\n",
    "            # Generator (VAE) wants to fool the discriminator\n",
    "            fake_output = self.discriminator(mask*reconstruction)\n",
    "            valid_labels = tf.ones_like(fake_output)  # Try to make discriminator think reconstructions are real\n",
    "            g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "            g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        # ----- Review for differences\n",
    "        # grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # self.total_loss_tracker.update_state(total_loss)\n",
    "        # self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        # self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        grads_vae = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_vae, self.encoder.trainable_weights + self.decoder.trainable_weights)) # This line is different\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.discriminator_loss_tracker.update_state(g_loss_adv)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"disc_loss\": self.discriminator_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "            \"raw_loss\": self.reconstruction_loss_tracker.result() + self.kl_loss_tracker.result(),\n",
    "            \"w_kl_loss\": self.kl_loss_tracker.result() * self.beta,\n",
    "            \"w_disc_loss\": self.discriminator_loss_tracker.result() * self.gamma,\n",
    "        }\n",
    "    \n",
    "    # Since we overrode train_step we need test_step\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        # Discriminator loss (only for monitoring)\n",
    "        # pass both data and reconstruction through D to get generator adversarial loss\n",
    "        real_output = self.discriminator(data)\n",
    "        fake_output = self.discriminator(mask*reconstruction)\n",
    "        real_labels = tf.ones_like(real_output)\n",
    "        fake_labels = tf.zeros_like(fake_output)\n",
    "        d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "        d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        # Generator adversarial loss\n",
    "        valid_labels = tf.ones_like(fake_output)\n",
    "        g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "        g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "        total_loss = reconstruction_loss + kl_loss * self.beta + g_loss_adv * self.gamma\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"raw_loss\": reconstruction_loss + kl_loss,\n",
    "            \"disc_loss\": g_loss_adv,\n",
    "            \"w_kl_loss\": kl_loss * self.beta,\n",
    "            \"w_disc_loss\": g_loss_adv * self.gamma\n",
    "        }\n",
    "\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "    X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "    X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "    Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data \n",
    "    hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57) \"\"\n",
    "    hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57) \"\"\n",
    "    leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57) \"\"\n",
    "    print(\"Data loaded from preprocessed_SNL_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 15\n",
    "LR_PATIENCE = 10\n",
    "\n",
    "# enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "# enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fd932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "# dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "# vae = VAE_Model(enc, dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.8)\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1000)\n",
    "# vae.compile(optimizer=opt) # Not sure what weighted_mse is doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd0d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # looks like early_stopping is needed for val_loss\n",
    "# early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ff3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:24:18.840668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37946 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:24:20.952502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-07-14 16:24:21.325445: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x5557561e8350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-14 16:24:21.325463: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-14 16:24:21.359860: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-14 16:24:21.441241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-07-14 16:24:21.628961: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497/2500 [============================>.] - ETA: 0s - loss: 1.7393 - reco_loss: 1.7081 - kl_loss: 0.0481 - disc_loss: 1.5266 - beta: 0.3195 - raw_loss: 1.7393 - w_kl_loss: 0.0062 - w_disc_loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     24\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# history = vae.fit(\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#             train_ds,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#             validation_data=val_ds,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#             verbose=2\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Iterative training. \u001b[39;00m\n\u001b[1;32m     36\u001b[0m save_path \u001b[38;5;241m=\u001b[39m SAVE_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# As of 7/8/25. Should be synced with vae0_analysis\u001b[39;00m\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1716\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1717\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[0;32m-> 1729\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1744\u001b[0m }\n\u001b[1;32m   1745\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py:2067\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 2067\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   2068\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2069\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2070\u001b[0m         ):\n\u001b[1;32m   2071\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1376\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1377\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1380\u001b[0m )\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    646\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    649\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "NUM_TRAIN = 10 # For now lets just try 20\n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt0/\" #\n",
    "# Last save is in attempt 1. New save should go to attempt 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, H1_SZ, H2_SZ) # Testing out these values for now\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        vae = VAE_GAN_Model(enc, dec, disc,  steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0, max_beta=1)\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1000)\n",
    "        vae.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "        # history = vae.fit(\n",
    "        #             train_ds,\n",
    "        #             validation_data=val_ds,\n",
    "        #             epochs=NUM_EPOCHS,\n",
    "        #             callbacks=[early_stopping, reduce_lr],\n",
    "        #             verbose=2\n",
    "        #         )\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'history' is the object returned by your model.fit() call\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[43mSAVE_PATH\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_history.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         history \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(2):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reconstruction_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reconstruction_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "    plt.plot(history['beta'],label=\"beta\")\n",
    "\n",
    "    # Plot validation losses\n",
    "    plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
