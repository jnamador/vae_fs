{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:11:06.792654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-22 16:11:08.168701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "# file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "# with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "#     X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "#     X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "#     Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data \n",
    "#     hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57) \"\"\n",
    "#     hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57) \"\"\n",
    "#     leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57) \"\"\n",
    "#     print(\"Data loaded from preprocessed_SNL_data.h5\")\n",
    "\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 50\n",
    "LR_PATIENCE = 25\n",
    "DISC_H1_SZ = 8 # Size of first hidden layer of discriminator  # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H2_SZ = 2 # \"\" second hidden layer \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:12:01.986966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:12:04.866467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-07-22 16:12:05.775847: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x5560de729a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-22 16:12:05.775867: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-22 16:12:05.818012: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-22 16:12:06.004979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-07-22 16:12:06.249509: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 16s 4ms/step - loss: 14.5306 - reco_loss: 2.5644 - kl_loss: 4.1223 - disc_loss: 0.4466 - beta: 0.9900 - raw_loss: 4.0641 - w_kl_loss: 1.1099 - w_disc_loss: 24.4377 - gamma: 62.2500 - val_loss: 135.9002 - val_reco_loss: 62.4710 - val_kl_loss: 15.3244 - val_raw_loss: 77.7954 - val_disc_loss: 0.4719 - val_w_kl_loss: 15.1711 - val_w_disc_loss: 58.2581 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 131.0623 - reco_loss: 8.5728 - kl_loss: 4.1979 - disc_loss: 0.7339 - beta: 1.0000 - raw_loss: 16.0968 - w_kl_loss: 5.7104 - w_disc_loss: 137.5611 - gamma: 184.7500 - val_loss: 192.4345 - val_reco_loss: 3.8718 - val_kl_loss: 0.9293 - val_raw_loss: 4.8011 - val_disc_loss: 0.7629 - val_w_kl_loss: 0.9293 - val_w_disc_loss: 187.6334 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 164.8552 - reco_loss: 12.1514 - kl_loss: 9.6981 - disc_loss: 0.5330 - beta: 0.9900 - raw_loss: 18.1962 - w_kl_loss: 4.4871 - w_disc_loss: 162.3587 - gamma: 307.2500 - val_loss: 371.0398 - val_reco_loss: 67.0691 - val_kl_loss: 11.2628 - val_raw_loss: 78.3319 - val_disc_loss: 0.7947 - val_w_kl_loss: 11.1502 - val_w_disc_loss: 292.8205 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 316.9941 - reco_loss: 7.0070 - kl_loss: 2.8367 - disc_loss: 0.7655 - beta: 1.0000 - raw_loss: 12.0589 - w_kl_loss: 3.8426 - w_disc_loss: 328.4487 - gamma: 429.7500 - val_loss: 351.9121 - val_reco_loss: 3.3786 - val_kl_loss: 0.9654 - val_raw_loss: 4.3439 - val_disc_loss: 0.7079 - val_w_kl_loss: 0.9654 - val_w_disc_loss: 347.5682 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 364.1248 - reco_loss: 1.9954 - kl_loss: 2.5725 - disc_loss: 0.6938 - beta: 0.9900 - raw_loss: 2.7867 - w_kl_loss: 0.5836 - w_disc_loss: 382.7178 - gamma: 552.2500 - val_loss: 538.9998 - val_reco_loss: 84.6512 - val_kl_loss: 8.4683 - val_raw_loss: 93.1195 - val_disc_loss: 0.7270 - val_w_kl_loss: 8.3836 - val_w_disc_loss: 445.9650 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 474.4550 - reco_loss: 9.7335 - kl_loss: 2.6812 - disc_loss: 0.7152 - beta: 1.0000 - raw_loss: 14.0239 - w_kl_loss: 3.2698 - w_disc_loss: 482.3611 - gamma: 674.7500 - val_loss: 516.9847 - val_reco_loss: 4.2599 - val_kl_loss: 1.5352 - val_raw_loss: 5.7952 - val_disc_loss: 0.6946 - val_w_kl_loss: 1.5352 - val_w_disc_loss: 511.1896 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 533.4902 - reco_loss: 0.8881 - kl_loss: 0.6939 - disc_loss: 0.6937 - beta: 0.9900 - raw_loss: 1.6888 - w_kl_loss: 0.5975 - w_disc_loss: 553.0437 - gamma: 797.2500 - val_loss: 597.8975 - val_reco_loss: 2.1020 - val_kl_loss: 0.7687 - val_raw_loss: 2.8707 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.7610 - val_w_disc_loss: 595.0345 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 617.0783 - reco_loss: 0.3612 - kl_loss: 0.3766 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.7853 - w_kl_loss: 0.3220 - w_disc_loss: 637.5211 - gamma: 919.7500 - val_loss: 682.2755 - val_reco_loss: 1.8801 - val_kl_loss: 0.4496 - val_raw_loss: 2.3296 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.4496 - val_w_disc_loss: 679.9459 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 701.8562 - reco_loss: 0.4136 - kl_loss: 0.2193 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.6529 - w_kl_loss: 0.1787 - w_disc_loss: 722.4309 - gamma: 1042.2500 - val_loss: 766.6776 - val_reco_loss: 1.5692 - val_kl_loss: 0.2699 - val_raw_loss: 1.8391 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.2672 - val_w_disc_loss: 764.8412 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 786.5703 - reco_loss: 0.3179 - kl_loss: 0.1157 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.4557 - w_kl_loss: 0.1048 - w_disc_loss: 807.3454 - gamma: 1164.7500 - val_loss: 851.3383 - val_reco_loss: 1.4708 - val_kl_loss: 0.1032 - val_raw_loss: 1.5739 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1032 - val_w_disc_loss: 849.7644 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 871.4133 - reco_loss: 0.3322 - kl_loss: 0.0464 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.3872 - w_kl_loss: 0.0412 - w_disc_loss: 892.2574 - gamma: 1287.2500 - val_loss: 936.0309 - val_reco_loss: 1.3047 - val_kl_loss: 0.0518 - val_raw_loss: 1.3565 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0513 - val_w_disc_loss: 934.6750 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 956.1991 - reco_loss: 0.2361 - kl_loss: 0.0216 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2625 - w_kl_loss: 0.0202 - w_disc_loss: 977.1679 - gamma: 1409.7499 - val_loss: 1020.8593 - val_reco_loss: 1.2415 - val_kl_loss: 0.0323 - val_raw_loss: 1.2738 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0323 - val_w_disc_loss: 1019.5855 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1041.1690 - reco_loss: 0.3078 - kl_loss: 0.0130 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.3212 - w_kl_loss: 0.0100 - w_disc_loss: 1062.0804 - gamma: 1532.2499 - val_loss: 1105.7225 - val_reco_loss: 1.2070 - val_kl_loss: 0.0197 - val_raw_loss: 1.2267 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0195 - val_w_disc_loss: 1104.4960 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1126.0039 - reco_loss: 0.2366 - kl_loss: 0.0085 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2461 - w_kl_loss: 0.0072 - w_disc_loss: 1146.9868 - gamma: 1654.7499 - val_loss: 1190.6174 - val_reco_loss: 1.1981 - val_kl_loss: 0.0127 - val_raw_loss: 1.2109 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0127 - val_w_disc_loss: 1189.4065 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1210.9883 - reco_loss: 0.2859 - kl_loss: 0.0057 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.2916 - w_kl_loss: 0.0042 - w_disc_loss: 1231.9281 - gamma: 1777.2499 - val_loss: 1222.0116 - val_reco_loss: 1.1992 - val_kl_loss: 0.0394 - val_raw_loss: 1.2385 - val_disc_loss: 0.6640 - val_w_kl_loss: 0.0390 - val_w_disc_loss: 1220.7734 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 960.9284 - reco_loss: 13.4945 - kl_loss: 9.2283 - disc_loss: 0.4991 - beta: 1.0000 - raw_loss: 22.3057 - w_kl_loss: 6.6517 - w_disc_loss: 953.4305 - gamma: 1899.7499 - val_loss: 1465.8362 - val_reco_loss: 25.3308 - val_kl_loss: 6.9184 - val_raw_loss: 32.2492 - val_disc_loss: 0.7311 - val_w_kl_loss: 6.9184 - val_w_disc_loss: 1433.5870 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1440.3135 - reco_loss: 5.0690 - kl_loss: 2.1720 - disc_loss: 0.7192 - beta: 0.9900 - raw_loss: 8.1254 - w_kl_loss: 2.2836 - w_disc_loss: 1454.2457 - gamma: 2022.2499 - val_loss: 1454.0005 - val_reco_loss: 3.3957 - val_kl_loss: 0.8161 - val_raw_loss: 4.2118 - val_disc_loss: 0.6959 - val_w_kl_loss: 0.8080 - val_w_disc_loss: 1449.7969 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1468.7248 - reco_loss: 0.5509 - kl_loss: 0.2412 - disc_loss: 0.6943 - beta: 1.0000 - raw_loss: 0.9376 - w_kl_loss: 0.2938 - w_disc_loss: 1489.0541 - gamma: 2144.7499 - val_loss: 1529.7444 - val_reco_loss: 1.5304 - val_kl_loss: 0.2324 - val_raw_loss: 1.7627 - val_disc_loss: 0.6927 - val_w_kl_loss: 0.2324 - val_w_disc_loss: 1527.9817 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1550.6790 - reco_loss: 0.3572 - kl_loss: 0.2317 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.5472 - w_kl_loss: 0.1416 - w_disc_loss: 1571.3560 - gamma: 2267.2499 - val_loss: 1615.5107 - val_reco_loss: 1.3705 - val_kl_loss: 0.6337 - val_raw_loss: 2.0042 - val_disc_loss: 0.6930 - val_w_kl_loss: 0.6274 - val_w_disc_loss: 1613.5128 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1635.7115 - reco_loss: 0.2776 - kl_loss: 0.1427 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 0.4901 - w_kl_loss: 0.1622 - w_disc_loss: 1656.4558 - gamma: 2389.7499 - val_loss: 1700.2604 - val_reco_loss: 1.2979 - val_kl_loss: 0.0787 - val_raw_loss: 1.3767 - val_disc_loss: 0.6932 - val_w_kl_loss: 0.0787 - val_w_disc_loss: 1698.8837 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 1720.5367 - reco_loss: 0.3712 - kl_loss: 0.0218 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.3998 - w_kl_loss: 0.0214 - w_disc_loss: 1741.3665 - gamma: 2512.2499 - val_loss: 1785.0918 - val_reco_loss: 1.2780 - val_kl_loss: 0.0141 - val_raw_loss: 1.2921 - val_disc_loss: 0.6932 - val_w_kl_loss: 0.0139 - val_w_disc_loss: 1783.7999 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1799.7045 - reco_loss: 0.2481 - kl_loss: 0.3757 - disc_loss: 0.6911 - beta: 1.0000 - raw_loss: 0.2714 - w_kl_loss: 0.0198 - w_disc_loss: 1820.6563 - gamma: 2634.7499 - val_loss: 286.1976 - val_reco_loss: 28.5403 - val_kl_loss: 9.4501 - val_raw_loss: 37.9904 - val_disc_loss: 0.0921 - val_w_kl_loss: 9.4501 - val_w_disc_loss: 248.2072 - val_gamma: 2695.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 2757.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 2818.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 2879.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 2940.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3002.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3063.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2493/2500 [============================>.] - ETA: 0s - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3124.5539\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3124.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3185.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3247.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3308.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3369.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3430.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3492.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3553.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3614.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3675.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3737.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3798.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3859.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3920.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3982.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4043.4509 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4104.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4165.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4227.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4288.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4349.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4410.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4472.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4533.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4594.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4655.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4717.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4778.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4839.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4900.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4962.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5023.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5084.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5145.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5207.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5268.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5329.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5390.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5452.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5513.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5574.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5635.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5697.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5758.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5819.7498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5880.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5942.2498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6003.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6064.7498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6125.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 51/100\n",
      "2498/2500 [============================>.] - ETA: 0s - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9700 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6187.1764\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6187.2498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6248.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(history\u001b[38;5;241m.\u001b[39mhistory, f)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainging_history_FULL.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'weakref' object"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "NUM_TRAIN = 10 \n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt7/\" #\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "# 7: Learning rate set to 0.0001\n",
    "# 8: Changed gamma linear schedule to actually stop at max gamma. previously it went up to 100 times greater than max gamma. \n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ec6e1",
   "metadata": {},
   "source": [
    "Moving over to large parametric sweep to find something that will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# NUM_TRAIN = 4 # Train just once for now\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/attempt6/\" #\n",
    "train = False\n",
    "save = True\n",
    "NUM_TRAIN = 3 # Train just once for now\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "parameters = [0]\n",
    "parameters_key = \"max_gamma\"\n",
    "SAVE_PATH = SAVE_PATH + parameters_key + \"/\"\n",
    "\n",
    "n_train = 3 # Train at least 3 models per parameter\n",
    "\n",
    "if train:\n",
    "    for param in parameters:\n",
    "        for i in range(n_train):\n",
    "            save_path = SAVE_PATH + f\"{parameters_key}_{param}/\"\n",
    "\n",
    "            # Manually make the directories and file. Python can do it, but its cleaner to do it manually\n",
    "            with open(SAVE_PATH +\"out.txt\", \"a\") as f:\n",
    "                print(f\"Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n\", file=f)\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            disc = Qmake_discriminator(INPUT_SZ, 8, 2) # Testing out these values for now\n",
    "\n",
    "            steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "            vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "            history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "            # Make loss plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            # Plot training losses\n",
    "            for key, val in history.history.items():\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                plt.plot(val, label=key, \n",
    "                        linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "                \n",
    "            # Customize the plot\n",
    "            plt.title(f'Variant: {parameters_key} = {param} Training and Validation Losses Run: {i}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.semilogy()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "            # Iterative training. \n",
    "            # save_path = save_path + f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "            if save:\n",
    "                iter_save_path = save_path +  f\"n_{i}/\"\n",
    "\n",
    "                # Save progress to main out file\n",
    "                with open(SAVE_PATH + \"out.txt\", \"a\") as f: \n",
    "                    print(f\"SAVING Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n Save path: {iter_save_path}\\n\", file=f, flush = True)\n",
    "\n",
    "                # Save weights to iter specific folder\n",
    "                vae.save_weights(filepath=iter_save_path , save_format='tf')\n",
    "                # Now save the histories\n",
    "                with open(iter_save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                    pkl.dump(history.history, f)\n",
    "                plt.savefig(iter_save_path + parameters_key + f\"_{param}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reco_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reco_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "    gamma = history['gamma']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "    # plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    # plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    # plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "    # plt.plot(history['beta'],label=\"beta\")\n",
    "    # plt.plot(history['gamma'], label=\"$\\gamma$\")\n",
    "\n",
    "    # # Plot validation losses\n",
    "    # plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    # plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    # plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want as a AD metric? the discriminator or latent space vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
