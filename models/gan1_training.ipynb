{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:11:06.792654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-22 16:11:08.168701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "# file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "# with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "#     X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "#     X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "#     Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data \n",
    "#     hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57) \"\"\n",
    "#     hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57) \"\"\n",
    "#     leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57) \"\"\n",
    "#     print(\"Data loaded from preprocessed_SNL_data.h5\")\n",
    "\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 20\n",
    "LR_PATIENCE = 8\n",
    "DISC_H1_SZ = 8 # Size of first hidden layer of discriminator  # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H2_SZ = 2 # \"\" second hidden layer \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 9.0534 - reco_loss: 2.3548 - kl_loss: 2.1883 - disc_loss: 0.2491 - beta: 0.9900 - raw_loss: 3.4383 - w_kl_loss: 0.8037 - w_disc_loss: 13.3029 - gamma: 62.2500 - val_loss: 94.9703 - val_reco_loss: 66.2805 - val_kl_loss: 7.0429 - val_raw_loss: 73.3234 - val_disc_loss: 0.1759 - val_w_kl_loss: 6.9725 - val_w_disc_loss: 21.7174 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 125.5876 - reco_loss: 9.8342 - kl_loss: 2.6100 - disc_loss: 0.6998 - beta: 1.0000 - raw_loss: 14.3310 - w_kl_loss: 3.4068 - w_disc_loss: 133.1361 - gamma: 184.7500 - val_loss: 197.5084 - val_reco_loss: 4.6928 - val_kl_loss: 0.9019 - val_raw_loss: 5.5947 - val_disc_loss: 0.7803 - val_w_kl_loss: 0.9019 - val_w_disc_loss: 191.9137 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 211.6213 - reco_loss: 1.1195 - kl_loss: 0.2979 - disc_loss: 0.7611 - beta: 0.9900 - raw_loss: 1.5032 - w_kl_loss: 0.2865 - w_disc_loss: 233.4867 - gamma: 307.2500 - val_loss: 265.7164 - val_reco_loss: 2.6746 - val_kl_loss: 0.2786 - val_raw_loss: 2.9532 - val_disc_loss: 0.7132 - val_w_kl_loss: 0.2758 - val_w_disc_loss: 262.7660 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 281.8180 - reco_loss: 0.5142 - kl_loss: 0.1141 - disc_loss: 0.7048 - beta: 1.0000 - raw_loss: 0.6534 - w_kl_loss: 0.1058 - w_disc_loss: 302.7658 - gamma: 429.7500 - val_loss: 342.5437 - val_reco_loss: 1.9420 - val_kl_loss: 0.1545 - val_raw_loss: 2.0965 - val_disc_loss: 0.6934 - val_w_kl_loss: 0.1545 - val_w_disc_loss: 340.4472 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 362.1807 - reco_loss: 0.4629 - kl_loss: 0.0642 - disc_loss: 0.6933 - beta: 0.9900 - raw_loss: 0.5334 - w_kl_loss: 0.0527 - w_disc_loss: 382.8806 - gamma: 552.2500 - val_loss: 426.9312 - val_reco_loss: 1.5187 - val_kl_loss: 0.0883 - val_raw_loss: 1.6070 - val_disc_loss: 0.6933 - val_w_kl_loss: 0.0874 - val_w_disc_loss: 425.3251 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 446.9511 - reco_loss: 0.2738 - kl_loss: 0.0354 - disc_loss: 0.6934 - beta: 1.0000 - raw_loss: 0.3141 - w_kl_loss: 0.0306 - w_disc_loss: 467.8720 - gamma: 674.7500 - val_loss: 511.7934 - val_reco_loss: 1.3210 - val_kl_loss: 0.0441 - val_raw_loss: 1.3651 - val_disc_loss: 0.6936 - val_w_kl_loss: 0.0441 - val_w_disc_loss: 510.4283 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 555.0418 - reco_loss: 2.6954 - kl_loss: 17.6685 - disc_loss: 0.6885 - beta: 0.9900 - raw_loss: 26.9133 - w_kl_loss: 17.9261 - w_disc_loss: 549.1909 - gamma: 797.2500 - val_loss: 621.1872 - val_reco_loss: 3.3104 - val_kl_loss: 1.0587 - val_raw_loss: 4.3691 - val_disc_loss: 0.7185 - val_w_kl_loss: 1.0481 - val_w_disc_loss: 616.8287 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 784.5650 - reco_loss: 2.4409 - kl_loss: 3.4333 - disc_loss: 0.8748 - beta: 1.0000 - raw_loss: 5.5853 - w_kl_loss: 2.3861 - w_disc_loss: 806.6109 - gamma: 919.7500 - val_loss: 686.4083 - val_reco_loss: 11.3094 - val_kl_loss: 3.6335 - val_raw_loss: 14.9429 - val_disc_loss: 0.6845 - val_w_kl_loss: 3.6335 - val_w_disc_loss: 671.4654 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 699.4196 - reco_loss: 3.3123 - kl_loss: 3.1912 - disc_loss: 0.6853 - beta: 0.9900 - raw_loss: 6.1294 - w_kl_loss: 2.1017 - w_disc_loss: 714.2705 - gamma: 1042.2500 - val_loss: 783.6082 - val_reco_loss: 8.6276 - val_kl_loss: 2.8850 - val_raw_loss: 11.5127 - val_disc_loss: 0.6997 - val_w_kl_loss: 2.8562 - val_w_disc_loss: 772.1244 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 788.9743 - reco_loss: 1.8924 - kl_loss: 1.8424 - disc_loss: 0.6921 - beta: 1.0000 - raw_loss: 4.0019 - w_kl_loss: 1.6014 - w_disc_loss: 806.1678 - gamma: 1164.7500 - val_loss: 856.4830 - val_reco_loss: 5.0766 - val_kl_loss: 1.6859 - val_raw_loss: 6.7624 - val_disc_loss: 0.6931 - val_w_kl_loss: 1.6859 - val_w_disc_loss: 849.7205 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 873.2029 - reco_loss: 1.2105 - kl_loss: 0.8389 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 2.2150 - w_kl_loss: 0.7499 - w_disc_loss: 892.2069 - gamma: 1287.2500 - val_loss: 937.4754 - val_reco_loss: 2.2540 - val_kl_loss: 0.5598 - val_raw_loss: 2.8137 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.5542 - val_w_disc_loss: 934.6672 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 956.7010 - reco_loss: 0.4247 - kl_loss: 0.2425 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.7758 - w_kl_loss: 0.2664 - w_disc_loss: 977.1507 - gamma: 1409.7499 - val_loss: 1021.2576 - val_reco_loss: 1.5268 - val_kl_loss: 0.1458 - val_raw_loss: 1.6727 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1458 - val_w_disc_loss: 1019.5849 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1041.3708 - reco_loss: 0.4269 - kl_loss: 0.0806 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.5175 - w_kl_loss: 0.0678 - w_disc_loss: 1062.0809 - gamma: 1532.2499 - val_loss: 1106.0209 - val_reco_loss: 1.4181 - val_kl_loss: 0.1073 - val_raw_loss: 1.5254 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1063 - val_w_disc_loss: 1104.4966 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1126.4557 - reco_loss: 0.2517 - kl_loss: 0.0675 - disc_loss: 0.6934 - beta: 1.0000 - raw_loss: 0.3067 - w_kl_loss: 0.0420 - w_disc_loss: 1147.3788 - gamma: 1654.7499 - val_loss: 1250.2019 - val_reco_loss: 1.5520 - val_kl_loss: 0.3559 - val_raw_loss: 1.9080 - val_disc_loss: 0.7275 - val_w_kl_loss: 0.3559 - val_w_disc_loss: 1248.2939 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1219.1395 - reco_loss: 0.5157 - kl_loss: 0.7978 - disc_loss: 0.6973 - beta: 0.9900 - raw_loss: 1.3469 - w_kl_loss: 0.6070 - w_disc_loss: 1239.1378 - gamma: 1777.2499 - val_loss: 1280.1019 - val_reco_loss: 4.0791 - val_kl_loss: 1.2863 - val_raw_loss: 5.3654 - val_disc_loss: 0.6934 - val_w_kl_loss: 1.2734 - val_w_disc_loss: 1274.7494 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1297.3406 - reco_loss: 0.8307 - kl_loss: 0.8254 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 1.7590 - w_kl_loss: 0.7049 - w_disc_loss: 1316.8083 - gamma: 1899.7499 - val_loss: 1362.8768 - val_reco_loss: 2.8228 - val_kl_loss: 0.8210 - val_raw_loss: 3.6439 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.8210 - val_w_disc_loss: 1359.2329 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1381.9261 - reco_loss: 0.6788 - kl_loss: 0.4309 - disc_loss: 0.6933 - beta: 0.9900 - raw_loss: 1.1584 - w_kl_loss: 0.3581 - w_disc_loss: 1402.0000 - gamma: 2022.2499 - val_loss: 1471.4364 - val_reco_loss: 6.1464 - val_kl_loss: 2.1815 - val_raw_loss: 8.3279 - val_disc_loss: 0.7023 - val_w_kl_loss: 2.1597 - val_w_disc_loss: 1463.1302 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1433.6258 - reco_loss: 1.5484 - kl_loss: 42.1629 - disc_loss: 0.6720 - beta: 1.0000 - raw_loss: 14.9829 - w_kl_loss: 10.2458 - w_disc_loss: 1439.7828 - gamma: 2144.7499 - val_loss: 1105.4481 - val_reco_loss: 61.0864 - val_kl_loss: 16.7761 - val_raw_loss: 77.8626 - val_disc_loss: 0.4658 - val_w_kl_loss: 16.7761 - val_w_disc_loss: 1027.5856 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1530.6349 - reco_loss: 29.6524 - kl_loss: 11.6985 - disc_loss: 0.6643 - beta: 0.9900 - raw_loss: 41.4576 - w_kl_loss: 8.7752 - w_disc_loss: 1509.4986 - gamma: 2267.2499 - val_loss: 1892.2637 - val_reco_loss: 103.9568 - val_kl_loss: 15.8143 - val_raw_loss: 119.7711 - val_disc_loss: 0.7613 - val_w_kl_loss: 15.6560 - val_w_disc_loss: 1772.6509 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1765.3718 - reco_loss: 18.7922 - kl_loss: 7.4388 - disc_loss: 0.7374 - beta: 1.0000 - raw_loss: 26.5500 - w_kl_loss: 5.8913 - w_disc_loss: 1761.6204 - gamma: 2389.7499 - val_loss: 206.5988 - val_reco_loss: 141.5479 - val_kl_loss: 14.9205 - val_raw_loss: 156.4684 - val_disc_loss: 0.0205 - val_w_kl_loss: 14.9205 - val_w_disc_loss: 50.1304 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 139.0544 - reco_loss: 53.8878 - kl_loss: 27.7782 - disc_loss: 0.0279 - beta: 0.9900 - raw_loss: 69.2416 - w_kl_loss: 11.3945 - w_disc_loss: 70.5326 - gamma: 2512.2499 - val_loss: 2557.8162 - val_reco_loss: 1498.1133 - val_kl_loss: 102.0381 - val_raw_loss: 1600.1514 - val_disc_loss: 0.3725 - val_w_kl_loss: 101.0167 - val_w_disc_loss: 958.6862 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2319.7267 - reco_loss: 391.0721 - kl_loss: 52.6437 - disc_loss: 0.7108 - beta: 1.0000 - raw_loss: 464.4418 - w_kl_loss: 55.5686 - w_disc_loss: 1876.9770 - gamma: 2634.7499 - val_loss: 2641.0254 - val_reco_loss: 313.5169 - val_kl_loss: 28.9948 - val_raw_loss: 342.5116 - val_disc_loss: 0.8526 - val_w_kl_loss: 28.9948 - val_w_disc_loss: 2298.5137 - val_gamma: 2695.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2298.2943 - reco_loss: 63.4829 - kl_loss: 10.3604 - disc_loss: 0.8148 - beta: 0.9900 - raw_loss: 77.1920 - w_kl_loss: 10.2386 - w_disc_loss: 2246.1373 - gamma: 2757.2499 - val_loss: 2151.6052 - val_reco_loss: 40.3730 - val_kl_loss: 5.8518 - val_raw_loss: 46.2248 - val_disc_loss: 0.7470 - val_w_kl_loss: 5.7933 - val_w_disc_loss: 2105.4390 - val_gamma: 2818.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2090.6183 - reco_loss: 5.6446 - kl_loss: 1.7768 - disc_loss: 0.7309 - beta: 1.0000 - raw_loss: 8.5238 - w_kl_loss: 2.1863 - w_disc_loss: 2104.5200 - gamma: 2879.7499 - val_loss: 2060.1572 - val_reco_loss: 6.0651 - val_kl_loss: 0.8940 - val_raw_loss: 6.9591 - val_disc_loss: 0.6981 - val_w_kl_loss: 0.8940 - val_w_disc_loss: 2053.1980 - val_gamma: 2940.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2067.6518 - reco_loss: 0.8692 - kl_loss: 0.2389 - disc_loss: 0.6954 - beta: 0.9900 - raw_loss: 1.1661 - w_kl_loss: 0.2217 - w_disc_loss: 2087.7829 - gamma: 3002.2499 - val_loss: 2125.5513 - val_reco_loss: 1.7889 - val_kl_loss: 0.3465 - val_raw_loss: 2.1354 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.3430 - val_w_disc_loss: 2123.4192 - val_gamma: 3063.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2497/2500 [============================>.] - ETA: 0s - loss: 2145.1875 - reco_loss: 0.4034 - kl_loss: 0.1222 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.5430 - w_kl_loss: 0.1062 - w_disc_loss: 2165.8378 - gamma: 3124.6519\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2145.2214 - reco_loss: 0.4034 - kl_loss: 0.1222 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.5430 - w_kl_loss: 0.1062 - w_disc_loss: 2165.9057 - gamma: 3124.7499 - val_loss: 2210.2815 - val_reco_loss: 1.7472 - val_kl_loss: 0.1998 - val_raw_loss: 1.9470 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1998 - val_w_disc_loss: 2208.3345 - val_gamma: 3185.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2230.1019 - reco_loss: 0.4189 - kl_loss: 0.0882 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.5080 - w_kl_loss: 0.0666 - w_disc_loss: 2250.8107 - gamma: 3247.2499 - val_loss: 2294.9246 - val_reco_loss: 1.5160 - val_kl_loss: 0.1684 - val_raw_loss: 1.6844 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1667 - val_w_disc_loss: 2293.2419 - val_gamma: 3308.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2314.8959 - reco_loss: 0.3159 - kl_loss: 0.0699 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.3902 - w_kl_loss: 0.0565 - w_disc_loss: 2335.7374 - gamma: 3369.7499 - val_loss: 2379.6694 - val_reco_loss: 1.3871 - val_kl_loss: 0.1259 - val_raw_loss: 1.5130 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1259 - val_w_disc_loss: 2378.1565 - val_gamma: 3430.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 2399.8290 - reco_loss: 0.3500 - kl_loss: 0.0523 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.4023 - w_kl_loss: 0.0391 - w_disc_loss: 2420.6577 - gamma: 3492.2499 - val_loss: 2464.5132 - val_reco_loss: 1.3600 - val_kl_loss: 0.0884 - val_raw_loss: 1.4484 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0875 - val_w_disc_loss: 2463.0657 - val_gamma: 3553.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2484.6710 - reco_loss: 0.2879 - kl_loss: 0.0375 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 0.3305 - w_kl_loss: 0.0323 - w_disc_loss: 2505.5667 - gamma: 3614.7499 - val_loss: 2549.2795 - val_reco_loss: 1.2526 - val_kl_loss: 0.0512 - val_raw_loss: 1.3038 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0512 - val_w_disc_loss: 2547.9758 - val_gamma: 3675.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2569.5672 - reco_loss: 0.2890 - kl_loss: 0.0216 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.3130 - w_kl_loss: 0.0179 - w_disc_loss: 2590.4802 - gamma: 3737.2499 - val_loss: 2634.1265 - val_reco_loss: 1.2191 - val_kl_loss: 0.0242 - val_raw_loss: 1.2434 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0240 - val_w_disc_loss: 2632.8833 - val_gamma: 3798.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2654.4342 - reco_loss: 0.2355 - kl_loss: 0.0147 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 0.2519 - w_kl_loss: 0.0125 - w_disc_loss: 2675.4033 - gamma: 3859.7499 - val_loss: 2719.1968 - val_reco_loss: 1.2164 - val_kl_loss: 0.0184 - val_raw_loss: 1.2348 - val_disc_loss: 0.6932 - val_w_kl_loss: 0.0184 - val_w_disc_loss: 2717.9619 - val_gamma: 3920.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2734.0056 - reco_loss: 0.3176 - kl_loss: 0.2027 - disc_loss: 0.6918 - beta: 0.9901 - raw_loss: 0.3394 - w_kl_loss: 0.0158 - w_disc_loss: 2754.8922 - gamma: 3982.2499 - val_loss: 667.6054 - val_reco_loss: 6.5227 - val_kl_loss: 2.4611 - val_raw_loss: 8.9838 - val_disc_loss: 0.1629 - val_w_kl_loss: 2.4367 - val_w_disc_loss: 658.6461 - val_gamma: 4043.4509 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1628.3669 - reco_loss: 50.9126 - kl_loss: 69.2369 - disc_loss: 0.3746 - beta: 1.0000 - raw_loss: 95.2632 - w_kl_loss: 33.9652 - w_disc_loss: 1543.7096 - gamma: 4104.7499 - val_loss: 3230.9290 - val_reco_loss: 80.5989 - val_kl_loss: 16.9955 - val_raw_loss: 97.5945 - val_disc_loss: 0.7521 - val_w_kl_loss: 16.9955 - val_w_disc_loss: 3133.3345 - val_gamma: 4165.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3163.6596 - reco_loss: 23.3498 - kl_loss: 18.1256 - disc_loss: 0.7432 - beta: 0.9901 - raw_loss: 45.0609 - w_kl_loss: 16.1540 - w_disc_loss: 3141.3904 - gamma: 4227.2499 - val_loss: 3146.8821 - val_reco_loss: 58.4993 - val_kl_loss: 15.1408 - val_raw_loss: 73.6401 - val_disc_loss: 0.7167 - val_w_kl_loss: 14.9901 - val_w_disc_loss: 3073.3926 - val_gamma: 4288.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3091.1019 - reco_loss: 13.3571 - kl_loss: 12.7769 - disc_loss: 0.7096 - beta: 1.0000 - raw_loss: 26.5424 - w_kl_loss: 10.0253 - w_disc_loss: 3086.3088 - gamma: 4349.7499 - val_loss: 3107.4600 - val_reco_loss: 29.1048 - val_kl_loss: 11.7307 - val_raw_loss: 40.8355 - val_disc_loss: 0.6952 - val_w_kl_loss: 11.7307 - val_w_disc_loss: 3066.6245 - val_gamma: 4410.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3098.1993 - reco_loss: 7.4828 - kl_loss: 8.3041 - disc_loss: 0.6941 - beta: 0.9901 - raw_loss: 15.4265 - w_kl_loss: 5.9236 - w_disc_loss: 3104.0236 - gamma: 4472.2499 - val_loss: 3158.5947 - val_reco_loss: 8.8107 - val_kl_loss: 7.5124 - val_raw_loss: 16.3231 - val_disc_loss: 0.6931 - val_w_kl_loss: 7.4377 - val_w_disc_loss: 3142.3464 - val_gamma: 4533.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3170.7553 - reco_loss: 1.6383 - kl_loss: 4.6975 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 7.1373 - w_kl_loss: 4.1771 - w_disc_loss: 3184.8464 - gamma: 4594.7499 - val_loss: 3234.1672 - val_reco_loss: 3.0088 - val_kl_loss: 3.9104 - val_raw_loss: 6.9192 - val_disc_loss: 0.6931 - val_w_kl_loss: 3.9104 - val_w_disc_loss: 3227.2480 - val_gamma: 4655.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3251.8514 - reco_loss: 0.7069 - kl_loss: 2.3323 - disc_loss: 0.6932 - beta: 0.9901 - raw_loss: 3.3306 - w_kl_loss: 1.9590 - w_disc_loss: 3269.7667 - gamma: 4717.2499 - val_loss: 3315.8254 - val_reco_loss: 1.7854 - val_kl_loss: 1.8888 - val_raw_loss: 3.6742 - val_disc_loss: 0.6931 - val_w_kl_loss: 1.8700 - val_w_disc_loss: 3312.1699 - val_gamma: 4778.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3335.0802 - reco_loss: 0.3732 - kl_loss: 0.9374 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 1.6476 - w_kl_loss: 0.9671 - w_disc_loss: 3354.6687 - gamma: 4839.7499 - val_loss: 3398.9607 - val_reco_loss: 1.3470 - val_kl_loss: 0.5344 - val_raw_loss: 1.8814 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.5344 - val_w_disc_loss: 3397.0793 - val_gamma: 4900.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3419.0414 - reco_loss: 0.3247 - kl_loss: 0.2533 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.6974 - w_kl_loss: 0.2772 - w_disc_loss: 3439.5868 - gamma: 4962.2499 - val_loss: 3483.4094 - val_reco_loss: 1.2636 - val_kl_loss: 0.1527 - val_raw_loss: 1.4163 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1512 - val_w_disc_loss: 3481.9946 - val_gamma: 5023.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3503.6210 - reco_loss: 0.2396 - kl_loss: 0.1180 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.3658 - w_kl_loss: 0.0958 - w_disc_loss: 3524.4906 - gamma: 5084.7499 - val_loss: 3568.1360 - val_reco_loss: 1.2508 - val_kl_loss: 0.1069 - val_raw_loss: 1.3577 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.1069 - val_w_disc_loss: 3566.7783 - val_gamma: 5145.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3581.2222 - reco_loss: 1.0979 - kl_loss: 2.4708 - disc_loss: 0.6914 - beta: 0.9900 - raw_loss: 2.0534 - w_kl_loss: 0.7066 - w_disc_loss: 3600.3339 - gamma: 5207.2499 - val_loss: 3676.0042 - val_reco_loss: 16.2610 - val_kl_loss: 5.3320 - val_raw_loss: 21.5930 - val_disc_loss: 0.6937 - val_w_kl_loss: 5.2786 - val_w_disc_loss: 3654.4646 - val_gamma: 5268.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3701.0202 - reco_loss: 3.7366 - kl_loss: 14.8490 - disc_loss: 0.6933 - beta: 1.0000 - raw_loss: 26.8904 - w_kl_loss: 17.4306 - w_disc_loss: 3695.3668 - gamma: 5329.7499 - val_loss: 3753.3335 - val_reco_loss: 12.1098 - val_kl_loss: 4.4980 - val_raw_loss: 16.6078 - val_disc_loss: 0.6931 - val_w_kl_loss: 4.4980 - val_w_disc_loss: 3736.7258 - val_gamma: 5390.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3764.4762 - reco_loss: 3.2846 - kl_loss: 3.0935 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 6.4767 - w_kl_loss: 2.3812 - w_disc_loss: 3779.2292 - gamma: 5452.2499 - val_loss: 3831.0122 - val_reco_loss: 6.2441 - val_kl_loss: 3.1585 - val_raw_loss: 9.4026 - val_disc_loss: 0.6931 - val_w_kl_loss: 3.1269 - val_w_disc_loss: 3821.6414 - val_gamma: 5513.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3846.5048 - reco_loss: 1.1830 - kl_loss: 1.9444 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 3.4965 - w_kl_loss: 1.7568 - w_disc_loss: 3864.2429 - gamma: 5574.7499 - val_loss: 3911.2810 - val_reco_loss: 2.6630 - val_kl_loss: 1.5642 - val_raw_loss: 4.2272 - val_disc_loss: 0.6932 - val_w_kl_loss: 1.5642 - val_w_disc_loss: 3907.0540 - val_gamma: 5635.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 3929.8157 - reco_loss: 0.6732 - kl_loss: 1.2122 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 1.6786 - w_kl_loss: 0.7493 - w_disc_loss: 3949.3672 - gamma: 5697.2499 - val_loss: 4057.5146 - val_reco_loss: 8.6893 - val_kl_loss: 4.4378 - val_raw_loss: 13.1271 - val_disc_loss: 0.7023 - val_w_kl_loss: 4.3933 - val_w_disc_loss: 4044.4319 - val_gamma: 5758.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4034.4466 - reco_loss: 2.0263 - kl_loss: 3.5136 - disc_loss: 0.6959 - beta: 1.0000 - raw_loss: 5.8690 - w_kl_loss: 2.9190 - w_disc_loss: 4049.8620 - gamma: 5819.7498 - val_loss: 4086.1296 - val_reco_loss: 6.2953 - val_kl_loss: 3.3794 - val_raw_loss: 9.6747 - val_disc_loss: 0.6932 - val_w_kl_loss: 3.3794 - val_w_disc_loss: 4076.4551 - val_gamma: 5880.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4101.7799 - reco_loss: 1.7082 - kl_loss: 2.1000 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 4.0308 - w_kl_loss: 1.7335 - w_disc_loss: 4118.9835 - gamma: 5942.2498 - val_loss: 4166.6875 - val_reco_loss: 3.2865 - val_kl_loss: 1.7102 - val_raw_loss: 4.9967 - val_disc_loss: 0.6932 - val_w_kl_loss: 1.6930 - val_w_disc_loss: 4161.7080 - val_gamma: 6003.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4184.7137 - reco_loss: 0.6250 - kl_loss: 0.8737 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 1.7931 - w_kl_loss: 0.8871 - w_disc_loss: 4204.1488 - gamma: 6064.7498 - val_loss: 4249.2920 - val_reco_loss: 1.7350 - val_kl_loss: 0.4945 - val_raw_loss: 2.2295 - val_disc_loss: 0.6933 - val_w_kl_loss: 0.4945 - val_w_disc_loss: 4247.0625 - val_gamma: 6125.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 51/100\n",
      "2490/2500 [============================>.] - ETA: 0s - loss: 4269.7155 - reco_loss: 0.4036 - kl_loss: 0.2529 - disc_loss: 0.6934 - beta: 0.8900 - raw_loss: 0.6933 - w_kl_loss: 0.2161 - w_disc_loss: 4290.1623 - gamma: 6186.9804\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 4269.8106 - reco_loss: 0.4033 - kl_loss: 0.2530 - disc_loss: 0.6934 - beta: 0.9900 - raw_loss: 0.6929 - w_kl_loss: 0.2162 - w_disc_loss: 4290.3513 - gamma: 6187.2498 - val_loss: 4328.1748 - val_reco_loss: 1.8675 - val_kl_loss: 0.5702 - val_raw_loss: 2.4377 - val_disc_loss: 0.6923 - val_w_kl_loss: 0.5645 - val_w_disc_loss: 4325.7427 - val_gamma: 6248.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 12s 4ms/step - loss: 23.1332 - reco_loss: 1.4169 - kl_loss: 1.2487 - disc_loss: 0.6312 - beta: 0.9900 - raw_loss: 2.2326 - w_kl_loss: 0.6009 - w_disc_loss: 39.9417 - gamma: 62.2500 - val_loss: 91.3227 - val_reco_loss: 4.7520 - val_kl_loss: 0.4440 - val_raw_loss: 5.1960 - val_disc_loss: 0.6977 - val_w_kl_loss: 0.4395 - val_w_disc_loss: 86.1312 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 108.3581 - reco_loss: 0.8517 - kl_loss: 0.2646 - disc_loss: 0.6953 - beta: 1.0000 - raw_loss: 1.2315 - w_kl_loss: 0.2889 - w_disc_loss: 128.4240 - gamma: 184.7500 - val_loss: 172.3531 - val_reco_loss: 1.8077 - val_kl_loss: 0.0669 - val_raw_loss: 1.8747 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0669 - val_w_disc_loss: 170.4785 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 192.2394 - reco_loss: 0.3948 - kl_loss: 0.0927 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.4987 - w_kl_loss: 0.0774 - w_disc_loss: 212.9658 - gamma: 307.2500 - val_loss: 256.7425 - val_reco_loss: 1.3257 - val_kl_loss: 0.0283 - val_raw_loss: 1.3540 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0280 - val_w_disc_loss: 255.3888 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 276.9671 - reco_loss: 0.2441 - kl_loss: 0.0616 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.3156 - w_kl_loss: 0.0545 - w_disc_loss: 297.8806 - gamma: 429.7500 - val_loss: 341.5533 - val_reco_loss: 1.2352 - val_kl_loss: 0.0237 - val_raw_loss: 1.2589 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0237 - val_w_disc_loss: 340.2945 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 361.8745 - reco_loss: 0.2812 - kl_loss: 0.0366 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.3125 - w_kl_loss: 0.0233 - w_disc_loss: 382.7916 - gamma: 552.2500 - val_loss: 426.4445 - val_reco_loss: 1.2067 - val_kl_loss: 0.0263 - val_raw_loss: 1.2329 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0260 - val_w_disc_loss: 425.2119 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 446.7217 - reco_loss: 0.2147 - kl_loss: 0.0256 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2486 - w_kl_loss: 0.0266 - w_disc_loss: 467.7024 - gamma: 674.7500 - val_loss: 511.3362 - val_reco_loss: 1.1973 - val_kl_loss: 0.0169 - val_raw_loss: 1.2142 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0169 - val_w_disc_loss: 510.1220 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 531.7160 - reco_loss: 0.3186 - kl_loss: 0.0135 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.3319 - w_kl_loss: 0.0100 - w_disc_loss: 552.6143 - gamma: 797.2500 - val_loss: 596.2345 - val_reco_loss: 1.1900 - val_kl_loss: 0.0116 - val_raw_loss: 1.2017 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0115 - val_w_disc_loss: 595.0330 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 616.5427 - reco_loss: 0.2386 - kl_loss: 0.0090 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2484 - w_kl_loss: 0.0074 - w_disc_loss: 637.5246 - gamma: 919.7500 - val_loss: 681.1420 - val_reco_loss: 1.1905 - val_kl_loss: 0.0081 - val_raw_loss: 1.1986 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0081 - val_w_disc_loss: 679.9434 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 701.4904 - reco_loss: 0.2782 - kl_loss: 0.0068 - disc_loss: 0.6932 - beta: 0.9900 - raw_loss: 0.2854 - w_kl_loss: 0.0054 - w_disc_loss: 722.4361 - gamma: 1042.2500 - val_loss: 766.0489 - val_reco_loss: 1.1914 - val_kl_loss: 0.0055 - val_raw_loss: 1.1969 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0054 - val_w_disc_loss: 764.8521 - val_gamma: 1103.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 786.3470 - reco_loss: 0.2259 - kl_loss: 0.0051 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2313 - w_kl_loss: 0.0041 - w_disc_loss: 807.3452 - gamma: 1164.7500 - val_loss: 850.9641 - val_reco_loss: 1.1959 - val_kl_loss: 0.0037 - val_raw_loss: 1.1996 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0037 - val_w_disc_loss: 849.7645 - val_gamma: 1225.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 871.2323 - reco_loss: 0.2944 - kl_loss: 0.1661 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.3213 - w_kl_loss: 0.0194 - w_disc_loss: 892.1380 - gamma: 1287.2500 - val_loss: 937.8146 - val_reco_loss: 2.7190 - val_kl_loss: 0.4099 - val_raw_loss: 3.1288 - val_disc_loss: 0.6932 - val_w_kl_loss: 0.4058 - val_w_disc_loss: 934.6899 - val_gamma: 1348.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 961.6116 - reco_loss: 2.3757 - kl_loss: 1.3901 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 5.7541 - w_kl_loss: 2.5595 - w_disc_loss: 977.0872 - gamma: 1409.7499 - val_loss: 1004.2769 - val_reco_loss: 5.3780 - val_kl_loss: 1.2813 - val_raw_loss: 6.6593 - val_disc_loss: 0.6782 - val_w_kl_loss: 1.2813 - val_w_disc_loss: 997.6177 - val_gamma: 1470.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1041.6275 - reco_loss: 1.7758 - kl_loss: 0.8013 - disc_loss: 0.6918 - beta: 0.9900 - raw_loss: 2.7638 - w_kl_loss: 0.7376 - w_disc_loss: 1060.0507 - gamma: 1532.2499 - val_loss: 1107.7305 - val_reco_loss: 2.7264 - val_kl_loss: 0.5211 - val_raw_loss: 3.2474 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.5159 - val_w_disc_loss: 1104.4883 - val_gamma: 1593.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1126.5206 - reco_loss: 0.4585 - kl_loss: 0.2268 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.7699 - w_kl_loss: 0.2363 - w_disc_loss: 1146.9641 - gamma: 1654.7499 - val_loss: 1191.0398 - val_reco_loss: 1.5367 - val_kl_loss: 0.0998 - val_raw_loss: 1.6365 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0998 - val_w_disc_loss: 1189.4032 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1211.0763 - reco_loss: 0.3561 - kl_loss: 0.0413 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.4109 - w_kl_loss: 0.0409 - w_disc_loss: 1231.8937 - gamma: 1777.2499 - val_loss: 1275.6101 - val_reco_loss: 1.2807 - val_kl_loss: 0.0154 - val_raw_loss: 1.2961 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0152 - val_w_disc_loss: 1274.3142 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1295.8356 - reco_loss: 0.2375 - kl_loss: 0.0157 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2579 - w_kl_loss: 0.0158 - w_disc_loss: 1316.8090 - gamma: 1899.7499 - val_loss: 1360.4774 - val_reco_loss: 1.2411 - val_kl_loss: 0.0089 - val_raw_loss: 1.2500 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0089 - val_w_disc_loss: 1359.2274 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1380.8158 - reco_loss: 0.3224 - kl_loss: 0.1008 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.3545 - w_kl_loss: 0.0237 - w_disc_loss: 1401.6979 - gamma: 2022.2499 - val_loss: 1444.6991 - val_reco_loss: 2.3363 - val_kl_loss: 0.2184 - val_raw_loss: 2.5548 - val_disc_loss: 0.6922 - val_w_kl_loss: 0.2163 - val_w_disc_loss: 1442.1465 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1465.5347 - reco_loss: 0.3897 - kl_loss: 0.0627 - disc_loss: 0.6930 - beta: 1.0000 - raw_loss: 0.4916 - w_kl_loss: 0.0775 - w_disc_loss: 1486.2652 - gamma: 2144.7499 - val_loss: 1531.8328 - val_reco_loss: 1.4884 - val_kl_loss: 0.0262 - val_raw_loss: 1.5147 - val_disc_loss: 0.6937 - val_w_kl_loss: 0.0262 - val_w_disc_loss: 1530.3181 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1550.7072 - reco_loss: 0.3689 - kl_loss: 0.0213 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.3980 - w_kl_loss: 0.0218 - w_disc_loss: 1571.5301 - gamma: 2267.2499 - val_loss: 1615.4421 - val_reco_loss: 1.3551 - val_kl_loss: 0.0108 - val_raw_loss: 1.3659 - val_disc_loss: 0.6932 - val_w_kl_loss: 0.0107 - val_w_disc_loss: 1614.0763 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1635.6505 - reco_loss: 0.2801 - kl_loss: 0.0183 - disc_loss: 0.6932 - beta: 1.0000 - raw_loss: 0.3040 - w_kl_loss: 0.0180 - w_disc_loss: 1656.5702 - gamma: 2389.7499 - val_loss: 1700.1857 - val_reco_loss: 1.3078 - val_kl_loss: 0.0080 - val_raw_loss: 1.3158 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0080 - val_w_disc_loss: 1698.8699 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1720.4490 - reco_loss: 0.3154 - kl_loss: 0.0068 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.3223 - w_kl_loss: 0.0052 - w_disc_loss: 1741.3519 - gamma: 2512.2499 - val_loss: 1785.0249 - val_reco_loss: 1.2371 - val_kl_loss: 0.0065 - val_raw_loss: 1.2436 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0064 - val_w_disc_loss: 1783.7814 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1805.2704 - reco_loss: 0.2233 - kl_loss: 0.0050 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2292 - w_kl_loss: 0.0045 - w_disc_loss: 1826.2752 - gamma: 2634.7499 - val_loss: 1869.8955 - val_reco_loss: 1.2048 - val_kl_loss: 0.0035 - val_raw_loss: 1.2083 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0035 - val_w_disc_loss: 1868.6873 - val_gamma: 2695.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1890.2326 - reco_loss: 0.2779 - kl_loss: 0.0029 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.2808 - w_kl_loss: 0.0022 - w_disc_loss: 1911.1835 - gamma: 2757.2499 - val_loss: 1954.7988 - val_reco_loss: 1.1952 - val_kl_loss: 0.0038 - val_raw_loss: 1.1990 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0037 - val_w_disc_loss: 1953.5999 - val_gamma: 2818.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 1975.0749 - reco_loss: 0.2102 - kl_loss: 0.0024 - disc_loss: 0.6931 - beta: 1.0000 - raw_loss: 0.2127 - w_kl_loss: 0.0019 - w_disc_loss: 1996.0959 - gamma: 2879.7499 - val_loss: 2039.7104 - val_reco_loss: 1.1913 - val_kl_loss: 0.0017 - val_raw_loss: 1.1930 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0017 - val_w_disc_loss: 2038.5175 - val_gamma: 2940.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 2060.0729 - reco_loss: 0.2976 - kl_loss: 0.0019 - disc_loss: 0.6931 - beta: 0.9900 - raw_loss: 0.2995 - w_kl_loss: 0.0015 - w_disc_loss: 2080.9982 - gamma: 3002.2499 - val_loss: 2124.6179 - val_reco_loss: 1.1904 - val_kl_loss: 0.0020 - val_raw_loss: 1.1924 - val_disc_loss: 0.6931 - val_w_kl_loss: 0.0020 - val_w_disc_loss: 2123.4255 - val_gamma: 3063.4509 - val_beta: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2491/2500 [============================>.] - ETA: 0s - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3124.5049\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3124.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3185.9509 - val_beta: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3247.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3308.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3369.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3430.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3492.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3553.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3614.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3675.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3737.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3798.4509 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3859.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 3920.9509 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 3982.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4043.4509 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4104.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4165.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4227.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4288.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4349.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4410.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4472.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4533.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4594.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4655.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9901 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4717.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4778.4512 - val_beta: 0.9901 - lr: 5.0000e-05\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4839.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 4900.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 4962.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5023.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5084.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5145.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5207.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5268.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5329.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5390.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5452.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5513.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5574.7499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5635.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5697.2499 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5758.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5819.7498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 5880.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 5942.2498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6003.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 1.0000 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6064.7498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6125.9512 - val_beta: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 51/100\n",
      "2492/2500 [============================>.] - ETA: 0s - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9099 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6187.0294\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: nan - reco_loss: nan - kl_loss: nan - disc_loss: nan - beta: 0.9900 - raw_loss: nan - w_kl_loss: nan - w_disc_loss: nan - gamma: 6187.2498 - val_loss: nan - val_reco_loss: nan - val_kl_loss: nan - val_raw_loss: nan - val_disc_loss: nan - val_w_kl_loss: nan - val_w_disc_loss: nan - val_gamma: 6248.4512 - val_beta: 0.9900 - lr: 5.0000e-05\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "1223/2500 [=============>................] - ETA: 4s - loss: 10.1502 - reco_loss: 1.2968 - kl_loss: 0.9723 - disc_loss: 1.1347 - beta: 0.2200 - raw_loss: 1.8052 - w_kl_loss: 0.3829 - w_disc_loss: 24.5927 - gamma: 30.9390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# --\u001b[39;00m\n\u001b[1;32m     38\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt)\n\u001b[0;32m---> 39\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Iterative training. \u001b[39;00m\n\u001b[1;32m     43\u001b[0m save_path \u001b[38;5;241m=\u001b[39m SAVE_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:142\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m--> 142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:342\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_define_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs):\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Gets a function for these inputs, defining it if necessary.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m  Caller must hold self._lock.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m      shape relaxation retracing.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m   args, kwargs, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 342\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    344\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature, \u001b[38;5;241m*\u001b[39margs[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature):])\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:407\u001b[0m, in \u001b[0;36mFunctionSpec.canonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_pure:\n\u001b[1;32m    406\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[0;32m--> 407\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m cast_inputs(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature)\n\u001b[1;32m    409\u001b[0m filtered_flat_args \u001b[38;5;241m=\u001b[39m filter_function_inputs(args, kwargs)\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:433\u001b[0m, in \u001b[0;36mFunctionSpec.bind_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\u001b[38;5;241m.\u001b[39margs, \u001b[43mbound_arguments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/inspect.py:2675\u001b[0m, in \u001b[0;36mBoundArguments.kwargs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2673\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2674\u001b[0m kwargs_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2675\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_started:\n\u001b[1;32m   2677\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "NUM_TRAIN = 10 \n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt8/\" #\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "# 7: Learning rate set to 0.00001, \n",
    "# 8: Learning rate set to 0.000001\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.000001)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ec6e1",
   "metadata": {},
   "source": [
    "Moving over to large parametric sweep to find something that will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# NUM_TRAIN = 4 # Train just once for now\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/attempt6/\" #\n",
    "train = False\n",
    "save = True\n",
    "NUM_TRAIN = 3 # Train just once for now\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "parameters = [0]\n",
    "parameters_key = \"max_gamma\"\n",
    "SAVE_PATH = SAVE_PATH + parameters_key + \"/\"\n",
    "\n",
    "n_train = 3 # Train at least 3 models per parameter\n",
    "\n",
    "if train:\n",
    "    for param in parameters:\n",
    "        for i in range(n_train):\n",
    "            save_path = SAVE_PATH + f\"{parameters_key}_{param}/\"\n",
    "\n",
    "            # Manually make the directories and file. Python can do it, but its cleaner to do it manually\n",
    "            with open(SAVE_PATH +\"out.txt\", \"a\") as f:\n",
    "                print(f\"Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n\", file=f)\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            disc = Qmake_discriminator(INPUT_SZ, 8, 2) # Testing out these values for now\n",
    "\n",
    "            steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "            vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "            history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "            # Make loss plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            # Plot training losses\n",
    "            for key, val in history.history.items():\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                plt.plot(val, label=key, \n",
    "                        linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "                \n",
    "            # Customize the plot\n",
    "            plt.title(f'Variant: {parameters_key} = {param} Training and Validation Losses Run: {i}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.semilogy()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "            # Iterative training. \n",
    "            # save_path = save_path + f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "            if save:\n",
    "                iter_save_path = save_path +  f\"n_{i}/\"\n",
    "\n",
    "                # Save progress to main out file\n",
    "                with open(SAVE_PATH + \"out.txt\", \"a\") as f: \n",
    "                    print(f\"SAVING Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n Save path: {iter_save_path}\\n\", file=f, flush = True)\n",
    "\n",
    "                # Save weights to iter specific folder\n",
    "                vae.save_weights(filepath=iter_save_path , save_format='tf')\n",
    "                # Now save the histories\n",
    "                with open(iter_save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                    pkl.dump(history.history, f)\n",
    "                plt.savefig(iter_save_path + parameters_key + f\"_{param}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reco_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reco_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "    gamma = history['gamma']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "    # plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    # plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    # plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "    # plt.plot(history['beta'],label=\"beta\")\n",
    "    # plt.plot(history['gamma'], label=\"$\\gamma$\")\n",
    "\n",
    "    # # Plot validation losses\n",
    "    # plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    # plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    # plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want as a AD metric? the discriminator or latent space vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
