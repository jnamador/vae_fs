{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 08:51:53.616394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-25 08:51:57.102217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 20\n",
    "LR_PATIENCE = 8\n",
    "DISC_H1_SZ = 8 # Size of first hidden layer of discriminator  # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H2_SZ = 2 # \"\" second hidden layer \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ITERATION 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:14:22.984368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 17:14:24.844834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-07-22 17:14:25.042679: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x558e56757730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-22 17:14:25.042699: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-22 17:14:25.046917: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-22 17:14:25.079864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-07-22 17:14:25.201197: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 14s 4ms/step - loss: 16.5286 - reco_loss: 0.9488 - kl_loss: 0.9969 - disc_loss: 0.4698 - beta: 0.9900 - raw_loss: 1.9187 - w_kl_loss: 0.7246 - w_disc_loss: 29.0353 - gamma: 62.2500 - val_loss: 59.7961 - val_reco_loss: 3.5106 - val_kl_loss: 1.2540 - val_raw_loss: 4.7646 - val_disc_loss: 0.4459 - val_w_kl_loss: 1.2415 - val_w_disc_loss: 55.0441 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 67.9162 - reco_loss: 0.7480 - kl_loss: 0.9931 - disc_loss: 0.4305 - beta: 1.0000 - raw_loss: 1.7677 - w_kl_loss: 0.7744 - w_disc_loss: 79.3588 - gamma: 184.7500 - val_loss: 103.8694 - val_reco_loss: 3.4273 - val_kl_loss: 1.2143 - val_raw_loss: 4.6416 - val_disc_loss: 0.4034 - val_w_kl_loss: 1.2143 - val_w_disc_loss: 99.2278 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 294.1837 - reco_loss: 1.0082 - kl_loss: 1.1980 - disc_loss: 0.1798 - beta: 1.0000 - raw_loss: 2.2095 - w_kl_loss: 0.9149 - w_disc_loss: 297.4889 - gamma: 1654.7499 - val_loss: 310.9544 - val_reco_loss: 4.4044 - val_kl_loss: 1.3642 - val_raw_loss: 5.7685 - val_disc_loss: 0.1779 - val_w_kl_loss: 1.3642 - val_w_disc_loss: 305.1859 - val_gamma: 1715.9508 - val_beta: 1.0000 - lr: 5.0000e-07\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 301.3344 - reco_loss: 1.3100 - kl_loss: 1.2079 - disc_loss: 0.1712 - beta: 0.9900 - raw_loss: 2.4473 - w_kl_loss: 0.8481 - w_disc_loss: 304.1359 - gamma: 1777.2499 - val_loss: 311.0865 - val_reco_loss: 4.3684 - val_kl_loss: 1.3933 - val_raw_loss: 5.7617 - val_disc_loss: 0.1661 - val_w_kl_loss: 1.3793 - val_w_disc_loss: 305.3388 - val_gamma: 1838.4508 - val_beta: 0.9900 - lr: 5.0000e-07\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 305.1580 - reco_loss: 1.0002 - kl_loss: 1.2471 - disc_loss: 0.1621 - beta: 1.0000 - raw_loss: 2.2767 - w_kl_loss: 0.9715 - w_disc_loss: 307.8527 - gamma: 1899.7499 - val_loss: 320.1518 - val_reco_loss: 4.6078 - val_kl_loss: 1.4235 - val_raw_loss: 6.0313 - val_disc_loss: 0.1602 - val_w_kl_loss: 1.4235 - val_w_disc_loss: 314.1205 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-07\n",
      "Epoch 17/100\n",
      "2491/2500 [============================>.] - ETA: 0s - loss: 308.2386 - reco_loss: 1.3470 - kl_loss: 1.2423 - disc_loss: 0.1536 - beta: 0.9000 - raw_loss: 2.4729 - w_kl_loss: 0.8376 - w_disc_loss: 310.4577 - gamma: 2022.0050             \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999993688107e-07.\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 308.2403 - reco_loss: 1.3464 - kl_loss: 1.2429 - disc_loss: 0.1536 - beta: 0.9900 - raw_loss: 2.4728 - w_kl_loss: 0.8390 - w_disc_loss: 310.4781 - gamma: 2022.2499 - val_loss: 323.9603 - val_reco_loss: 4.6991 - val_kl_loss: 1.4568 - val_raw_loss: 6.1559 - val_disc_loss: 0.1525 - val_w_kl_loss: 1.4422 - val_w_disc_loss: 317.8190 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-07\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 312.0462 - reco_loss: 1.1382 - kl_loss: 1.2715 - disc_loss: 0.1465 - beta: 1.0000 - raw_loss: 2.3308 - w_kl_loss: 0.9048 - w_disc_loss: 314.2053 - gamma: 2144.7499 - val_loss: 321.9220 - val_reco_loss: 4.8104 - val_kl_loss: 1.4741 - val_raw_loss: 6.2845 - val_disc_loss: 0.1431 - val_w_kl_loss: 1.4741 - val_w_disc_loss: 315.6375 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-07\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 321.2402 - reco_loss: 1.4885 - kl_loss: 1.2121 - disc_loss: 0.1425 - beta: 0.9900 - raw_loss: 2.6172 - w_kl_loss: 0.8408 - w_disc_loss: 322.9893 - gamma: 2267.2499 - val_loss: 321.2109 - val_reco_loss: 4.7114 - val_kl_loss: 1.4905 - val_raw_loss: 6.2019 - val_disc_loss: 0.1353 - val_w_kl_loss: 1.4756 - val_w_disc_loss: 315.0239 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-07\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 330.2083 - reco_loss: 1.2725 - kl_loss: 1.2968 - disc_loss: 0.1389 - beta: 1.0000 - raw_loss: 2.6398 - w_kl_loss: 1.0337 - w_disc_loss: 331.8254 - gamma: 2389.7499 - val_loss: 344.0531 - val_reco_loss: 4.7381 - val_kl_loss: 1.5073 - val_raw_loss: 6.2454 - val_disc_loss: 0.1378 - val_w_kl_loss: 1.5073 - val_w_disc_loss: 337.8077 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-07\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 338.5040 - reco_loss: 1.4560 - kl_loss: 1.2965 - disc_loss: 0.1353 - beta: 0.9900 - raw_loss: 2.6860 - w_kl_loss: 0.9176 - w_disc_loss: 339.9665 - gamma: 2512.2499 - val_loss: 359.9048 - val_reco_loss: 4.6640 - val_kl_loss: 1.5235 - val_raw_loss: 6.1875 - val_disc_loss: 0.1375 - val_w_kl_loss: 1.5083 - val_w_disc_loss: 353.7325 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-07\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 109.5045 - reco_loss: 1.2304 - kl_loss: 0.3760 - disc_loss: 3.7021 - beta: 0.9900 - raw_loss: 1.5843 - w_kl_loss: 0.2641 - w_disc_loss: 222.4109 - gamma: 62.2500 - val_loss: 355.2935 - val_reco_loss: 5.0478 - val_kl_loss: 0.7712 - val_raw_loss: 5.8191 - val_disc_loss: 2.8309 - val_w_kl_loss: 0.7635 - val_w_disc_loss: 349.4821 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 389.2117 - reco_loss: 0.9096 - kl_loss: 0.3842 - disc_loss: 2.5460 - beta: 1.0000 - raw_loss: 1.3042 - w_kl_loss: 0.2997 - w_disc_loss: 466.4936 - gamma: 184.7500 - val_loss: 566.0344 - val_reco_loss: 4.8949 - val_kl_loss: 0.7429 - val_raw_loss: 5.6378 - val_disc_loss: 2.2785 - val_w_kl_loss: 0.7429 - val_w_disc_loss: 560.3965 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 527.5626 - reco_loss: 1.2499 - kl_loss: 0.3800 - disc_loss: 1.9122 - beta: 0.9900 - raw_loss: 1.6111 - w_kl_loss: 0.2690 - w_disc_loss: 584.8647 - gamma: 307.2500 - val_loss: 608.0704 - val_reco_loss: 4.6003 - val_kl_loss: 0.7197 - val_raw_loss: 5.3200 - val_disc_loss: 1.6359 - val_w_kl_loss: 0.7125 - val_w_disc_loss: 602.7576 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 592.9406 - reco_loss: 0.9189 - kl_loss: 0.4080 - disc_loss: 1.4872 - beta: 1.0000 - raw_loss: 1.3207 - w_kl_loss: 0.3054 - w_disc_loss: 637.4246 - gamma: 429.7500 - val_loss: 666.7441 - val_reco_loss: 4.6389 - val_kl_loss: 0.7004 - val_raw_loss: 5.3393 - val_disc_loss: 1.3472 - val_w_kl_loss: 0.7004 - val_w_disc_loss: 661.4048 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 627.3443 - reco_loss: 1.1260 - kl_loss: 0.4271 - disc_loss: 1.2023 - beta: 0.9900 - raw_loss: 1.5246 - w_kl_loss: 0.2972 - w_disc_loss: 662.8267 - gamma: 552.2500 - val_loss: 709.4067 - val_reco_loss: 4.6140 - val_kl_loss: 0.6877 - val_raw_loss: 5.3017 - val_disc_loss: 1.1478 - val_w_kl_loss: 0.6808 - val_w_disc_loss: 704.1118 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 654.8910 - reco_loss: 1.0135 - kl_loss: 0.4480 - disc_loss: 1.0158 - beta: 1.0000 - raw_loss: 1.4745 - w_kl_loss: 0.3500 - w_disc_loss: 684.6374 - gamma: 674.7500 - val_loss: 720.7911 - val_reco_loss: 4.5245 - val_kl_loss: 0.6818 - val_raw_loss: 5.2063 - val_disc_loss: 0.9723 - val_w_kl_loss: 0.6818 - val_w_disc_loss: 715.5848 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 7/100\n",
      "1884/2500 [=====================>........] - ETA: 2s - loss: 686.5540 - reco_loss: 1.2458 - kl_loss: 0.4668 - disc_loss: 0.9027 - beta: 0.8300 - raw_loss: 1.6851 - w_kl_loss: 0.3271 - w_disc_loss: 705.7384 - gamma: 782.1335             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 11s 4ms/step - loss: 418.0427 - reco_loss: 1.1314 - kl_loss: 0.7341 - disc_loss: 0.5431 - beta: 0.9900 - raw_loss: 1.8385 - w_kl_loss: 0.5263 - w_disc_loss: 432.8615 - gamma: 797.2500 - val_loss: 463.3963 - val_reco_loss: 6.6711 - val_kl_loss: 2.1685 - val_raw_loss: 8.8396 - val_disc_loss: 0.5295 - val_w_kl_loss: 2.1468 - val_w_disc_loss: 454.5784 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 663.9691 - reco_loss: 1.0903 - kl_loss: 0.8205 - disc_loss: 0.3542 - beta: 1.0000 - raw_loss: 1.9530 - w_kl_loss: 0.6552 - w_disc_loss: 672.8793 - gamma: 1899.7499 - val_loss: 666.5762 - val_reco_loss: 6.7676 - val_kl_loss: 2.1535 - val_raw_loss: 8.9211 - val_disc_loss: 0.3354 - val_w_kl_loss: 2.1535 - val_w_disc_loss: 657.6550 - val_gamma: 1960.9508 - val_beta: 1.0000 - lr: 5.0000e-07\n",
      "Epoch 17/100\n",
      "2488/2500 [============================>.] - ETA: 0s - loss: 678.0487 - reco_loss: 1.4006 - kl_loss: 0.8097 - disc_loss: 0.3394 - beta: 0.8700 - raw_loss: 2.1817 - w_kl_loss: 0.5812 - w_disc_loss: 686.2209 - gamma: 2021.9315             \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999993688107e-07.\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 678.0617 - reco_loss: 1.4000 - kl_loss: 0.8107 - disc_loss: 0.3394 - beta: 0.9900 - raw_loss: 2.1812 - w_kl_loss: 0.5821 - w_disc_loss: 686.2878 - gamma: 2022.2499 - val_loss: 682.0645 - val_reco_loss: 6.8773 - val_kl_loss: 2.1602 - val_raw_loss: 9.0374 - val_disc_loss: 0.3230 - val_w_kl_loss: 2.1385 - val_w_disc_loss: 673.0487 - val_gamma: 2083.4509 - val_beta: 0.9900 - lr: 5.0000e-07\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 691.5153 - reco_loss: 1.1602 - kl_loss: 0.8344 - disc_loss: 0.3262 - beta: 1.0000 - raw_loss: 1.9860 - w_kl_loss: 0.6269 - w_disc_loss: 699.5263 - gamma: 2144.7499 - val_loss: 724.4560 - val_reco_loss: 6.6222 - val_kl_loss: 2.1659 - val_raw_loss: 8.7881 - val_disc_loss: 0.3244 - val_w_kl_loss: 2.1659 - val_w_disc_loss: 715.6678 - val_gamma: 2205.9509 - val_beta: 1.0000 - lr: 2.5000e-07\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 715.0607 - reco_loss: 1.4837 - kl_loss: 0.8342 - disc_loss: 0.3187 - beta: 0.9900 - raw_loss: 2.2835 - w_kl_loss: 0.5966 - w_disc_loss: 722.5447 - gamma: 2267.2499 - val_loss: 738.5421 - val_reco_loss: 6.8680 - val_kl_loss: 2.1718 - val_raw_loss: 9.0398 - val_disc_loss: 0.3133 - val_w_kl_loss: 2.1501 - val_w_disc_loss: 729.5240 - val_gamma: 2328.4509 - val_beta: 0.9900 - lr: 2.5000e-07\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 738.2393 - reco_loss: 1.2514 - kl_loss: 0.8474 - disc_loss: 0.3121 - beta: 1.0000 - raw_loss: 2.1028 - w_kl_loss: 0.6469 - w_disc_loss: 745.6998 - gamma: 2389.7499 - val_loss: 769.3058 - val_reco_loss: 6.9144 - val_kl_loss: 2.1781 - val_raw_loss: 9.0925 - val_disc_loss: 0.3102 - val_w_kl_loss: 2.1781 - val_w_disc_loss: 760.2133 - val_gamma: 2450.9509 - val_beta: 1.0000 - lr: 2.5000e-07\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 758.1406 - reco_loss: 1.6505 - kl_loss: 0.8339 - disc_loss: 0.3045 - beta: 0.9900 - raw_loss: 2.4511 - w_kl_loss: 0.5970 - w_disc_loss: 765.0216 - gamma: 2512.2499 - val_loss: 775.2415 - val_reco_loss: 6.8156 - val_kl_loss: 2.1840 - val_raw_loss: 8.9996 - val_disc_loss: 0.2978 - val_w_kl_loss: 2.1622 - val_w_disc_loss: 766.2637 - val_gamma: 2573.4509 - val_beta: 0.9900 - lr: 2.5000e-07\n",
      "SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "TRAINING ITERATION 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 13s 4ms/step - loss: 66.7724 - reco_loss: 1.2887 - kl_loss: 0.5939 - disc_loss: 2.1821 - beta: 0.9900 - raw_loss: 1.8497 - w_kl_loss: 0.4189 - w_disc_loss: 132.2657 - gamma: 62.2500 - val_loss: 217.0296 - val_reco_loss: 3.8310 - val_kl_loss: 0.6961 - val_raw_loss: 4.5272 - val_disc_loss: 1.7214 - val_w_kl_loss: 0.6892 - val_w_disc_loss: 212.5094 - val_gamma: 123.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 257.7206 - reco_loss: 0.9842 - kl_loss: 0.5529 - disc_loss: 1.6732 - beta: 1.0000 - raw_loss: 1.5568 - w_kl_loss: 0.4355 - w_disc_loss: 307.6212 - gamma: 184.7500 - val_loss: 371.7985 - val_reco_loss: 3.8700 - val_kl_loss: 0.7045 - val_raw_loss: 4.5746 - val_disc_loss: 1.4931 - val_w_kl_loss: 0.7045 - val_w_disc_loss: 367.2240 - val_gamma: 245.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 398.7786 - reco_loss: 1.1138 - kl_loss: 0.5107 - disc_loss: 1.4396 - beta: 0.9900 - raw_loss: 1.6183 - w_kl_loss: 0.3754 - w_disc_loss: 441.3994 - gamma: 307.2500 - val_loss: 492.5782 - val_reco_loss: 3.7286 - val_kl_loss: 0.7219 - val_raw_loss: 4.4505 - val_disc_loss: 1.3248 - val_w_kl_loss: 0.7147 - val_w_disc_loss: 488.1349 - val_gamma: 368.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 508.3165 - reco_loss: 0.8742 - kl_loss: 0.5191 - disc_loss: 1.2722 - beta: 1.0000 - raw_loss: 1.3615 - w_kl_loss: 0.3699 - w_disc_loss: 546.0326 - gamma: 429.7500 - val_loss: 587.4990 - val_reco_loss: 3.8319 - val_kl_loss: 0.7461 - val_raw_loss: 4.5780 - val_disc_loss: 1.1873 - val_w_kl_loss: 0.7461 - val_w_disc_loss: 582.9210 - val_gamma: 490.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 596.0929 - reco_loss: 1.0951 - kl_loss: 0.5063 - disc_loss: 1.1410 - beta: 0.9900 - raw_loss: 1.5972 - w_kl_loss: 0.3739 - w_disc_loss: 629.5391 - gamma: 552.2500 - val_loss: 660.9924 - val_reco_loss: 3.9053 - val_kl_loss: 0.7760 - val_raw_loss: 4.6813 - val_disc_loss: 1.0699 - val_w_kl_loss: 0.7683 - val_w_disc_loss: 656.3188 - val_gamma: 613.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 665.8482 - reco_loss: 0.9393 - kl_loss: 0.5260 - disc_loss: 1.0323 - beta: 1.0000 - raw_loss: 1.4740 - w_kl_loss: 0.4063 - w_disc_loss: 696.0725 - gamma: 674.7500 - val_loss: 722.8636 - val_reco_loss: 3.8944 - val_kl_loss: 0.8101 - val_raw_loss: 4.7045 - val_disc_loss: 0.9758 - val_w_kl_loss: 0.8101 - val_w_disc_loss: 718.1592 - val_gamma: 735.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 721.7141 - reco_loss: 1.1264 - kl_loss: 0.5315 - disc_loss: 0.9399 - beta: 0.9900 - raw_loss: 1.6068 - w_kl_loss: 0.3579 - w_disc_loss: 748.9522 - gamma: 797.2500 - val_loss: 765.2183 - val_reco_loss: 3.8439 - val_kl_loss: 0.8477 - val_raw_loss: 4.6916 - val_disc_loss: 0.8859 - val_w_kl_loss: 0.8392 - val_w_disc_loss: 760.5352 - val_gamma: 858.4510 - val_beta: 0.9900 - lr: 1.0000e-06\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 11s 4ms/step - loss: 773.3846 - reco_loss: 0.9150 - kl_loss: 0.5644 - disc_loss: 0.8686 - beta: 1.0000 - raw_loss: 1.4843 - w_kl_loss: 0.4327 - w_disc_loss: 798.5523 - gamma: 919.7500 - val_loss: 830.4034 - val_reco_loss: 3.9986 - val_kl_loss: 0.8869 - val_raw_loss: 4.8855 - val_disc_loss: 0.8415 - val_w_kl_loss: 0.8869 - val_w_disc_loss: 825.5179 - val_gamma: 980.9510 - val_beta: 1.0000 - lr: 1.0000e-06\n",
      "Epoch 9/100\n",
      " 968/2500 [==========>...................] - ETA: 6s - loss: 815.1224 - reco_loss: 1.3941 - kl_loss: 0.5313 - disc_loss: 0.8192 - beta: 1.0000 - raw_loss: 1.8596 - w_kl_loss: 0.3549 - w_disc_loss: 822.9734 - gamma: 1004.6915            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "NUM_TRAIN = 10 \n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt9/\" #\n",
    "\n",
    "lr = 0.0000001\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "# 7: Learning rate set to 0.00001, \n",
    "# 8: Learning rate set to 0.000001\n",
    "# 9: Learning rate set to 0.0000001\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ)\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ec6e1",
   "metadata": {},
   "source": [
    "Moving over to large parametric sweep to find something that will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# NUM_TRAIN = 4 # Train just once for now\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/attempt6/\" #\n",
    "train = False\n",
    "save = True\n",
    "NUM_TRAIN = 3 # Train just once for now\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "parameters = [0]\n",
    "parameters_key = \"max_gamma\"\n",
    "SAVE_PATH = SAVE_PATH + parameters_key + \"/\"\n",
    "\n",
    "n_train = 3 # Train at least 3 models per parameter\n",
    "\n",
    "if train:\n",
    "    for param in parameters:\n",
    "        for i in range(n_train):\n",
    "            save_path = SAVE_PATH + f\"{parameters_key}_{param}/\"\n",
    "\n",
    "            # Manually make the directories and file. Python can do it, but its cleaner to do it manually\n",
    "            with open(SAVE_PATH +\"out.txt\", \"a\") as f:\n",
    "                print(f\"Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n\", file=f)\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            disc = Qmake_discriminator(INPUT_SZ, 8, 2) # Testing out these values for now\n",
    "\n",
    "            steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "            vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "            history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "            # Make loss plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            # Plot training losses\n",
    "            for key, val in history.history.items():\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                plt.plot(val, label=key, \n",
    "                        linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "                \n",
    "            # Customize the plot\n",
    "            plt.title(f'Variant: {parameters_key} = {param} Training and Validation Losses Run: {i}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.semilogy()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "            # Iterative training. \n",
    "            # save_path = save_path + f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "            if save:\n",
    "                iter_save_path = save_path +  f\"n_{i}/\"\n",
    "\n",
    "                # Save progress to main out file\n",
    "                with open(SAVE_PATH + \"out.txt\", \"a\") as f: \n",
    "                    print(f\"SAVING Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n Save path: {iter_save_path}\\n\", file=f, flush = True)\n",
    "\n",
    "                # Save weights to iter specific folder\n",
    "                vae.save_weights(filepath=iter_save_path , save_format='tf')\n",
    "                # Now save the histories\n",
    "                with open(iter_save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                    pkl.dump(history.history, f)\n",
    "                plt.savefig(iter_save_path + parameters_key + f\"_{param}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reco_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reco_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "    gamma = history['gamma']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "    # plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    # plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    # plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "    # plt.plot(history['beta'],label=\"beta\")\n",
    "    # plt.plot(history['gamma'], label=\"$\\gamma$\")\n",
    "\n",
    "    # # Plot validation losses\n",
    "    # plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    # plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    # plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want as a AD metric? the discriminator or latent space vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
