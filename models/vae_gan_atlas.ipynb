{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d785825",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator, steps_per_epoch=20, cycle_length=20, min_beta=0, max_beta=1, min_gamma=0, max_gamma=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "        self.discriminator = discriminator\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.gamma_tracker = keras.metrics.Mean(name=\"gamma\")\n",
    "        \n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "        self.min_gamma = tf.cast(min_gamma, tf.float32)\n",
    "        self.max_gamma = tf.cast(max_gamma, tf.float32)\n",
    "        self.gamma = tf.Variable(min_gamma, dtype=tf.float32)\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super(VAE_Model, self).compile(**kwargs)\n",
    "        # Set the optimizer for the entire model (encoder + decoder + discriminator)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Collect trainable variables from encoder, decoder, and discriminator\n",
    "        trainable_variables = (\n",
    "            self.encoder.trainable_weights + \n",
    "            self.decoder.trainable_weights + \n",
    "            self.discriminator.trainable_weights\n",
    "        )\n",
    "        # Build the optimizer with the full variable list\n",
    "        self.optimizer.build(trainable_variables)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        # For first half (x < 0.5), scale 2x from 0 to 1\n",
    "        # For second half (x >= 0.5), stay at 1\n",
    "        scaled_x = tf.where(x < 0.5, 2.0 * x, 1.0)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * scaled_x\n",
    "    def get_gamma_schedule(self, epoch):\n",
    "        # Convert to float32 for TF operations\n",
    "        epoch = tf.cast(epoch, tf.float32)\n",
    "        \n",
    "        # Calculate annealing progress\n",
    "        anneal_progress = (epoch - 50.0) / 50.0\n",
    "        gamma_anneal = self.min_gamma + (self.max_gamma - self.min_gamma) * anneal_progress\n",
    "        \n",
    "        # Implement the conditions using tf.where\n",
    "        gamma = tf.where(epoch < 50.0, \n",
    "                        0.0,  # if epoch < 50\n",
    "                        tf.where(epoch >= 100.0,\n",
    "                                self.max_gamma,  # if epoch >= 100\n",
    "                                gamma_anneal))   # if 50 <= epoch < 100\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Get the current epoch number\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "        self.gamma.assign(self.get_gamma_schedule(epoch))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the Discriminator\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape_disc:\n",
    "            # Generate reconstructed data\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Get discriminator predictions\n",
    "            real_output = self.discriminator(data)\n",
    "            fake_output = self.discriminator(reconstruction * mask)\n",
    "            \n",
    "            # Labels for real and fake data\n",
    "            real_labels = tf.ones_like(real_output)\n",
    "            fake_labels = tf.zeros_like(fake_output)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "            d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        grads_disc = tape_disc.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_weights))\n",
    "\n",
    "        # ---------------------------\n",
    "        # Train the VAE (Generator)\n",
    "        # ---------------------------\n",
    "        with tf.GradientTape() as tape:\n",
    "            data_noisy = data + tf.random.normal(tf.shape(data), \n",
    "                                               mean=0, \n",
    "                                               stddev=0)\n",
    "            # Keep the mask logic\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            data_noisy = data_noisy * mask  # Apply mask to noisy data\n",
    "            \n",
    "            z_mean, z_log_var, z = self.encoder(data_noisy)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "            # Generator (VAE) wants to fool the discriminator\n",
    "            fake_output = self.discriminator(mask*reconstruction)\n",
    "            valid_labels = tf.ones_like(fake_output)  # Try to make discriminator think reconstructions are real\n",
    "            g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "            g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss * self.beta + g_loss_adv * self.gamma\n",
    "            \n",
    "        grads_vae = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads_vae, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.discriminator_loss_tracker.update_state(g_loss_adv)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"disc_loss\": self.discriminator_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "            \"raw_loss\": self.reconstruction_loss_tracker.result() + self.kl_loss_tracker.result(),\n",
    "            \"w_kl_loss\": self.kl_loss_tracker.result() * self.beta,\n",
    "            \"w_disc_loss\": self.discriminator_loss_tracker.result() * self.gamma,\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        # Discriminator loss (only for monitoring)\n",
    "        # pass both data and reconstruction through D to get generator adversarial loss\n",
    "        real_output = self.discriminator(data)\n",
    "        fake_output = self.discriminator(mask*reconstruction)\n",
    "        real_labels = tf.ones_like(real_output)\n",
    "        fake_labels = tf.zeros_like(fake_output)\n",
    "        d_loss_real = keras.losses.binary_crossentropy(real_labels, real_output)\n",
    "        d_loss_fake = keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss = tf.reduce_mean(d_loss)\n",
    "        \n",
    "        # Generator adversarial loss\n",
    "        valid_labels = tf.ones_like(fake_output)\n",
    "        g_loss_adv = keras.losses.binary_crossentropy(valid_labels, fake_output)\n",
    "        g_loss_adv = tf.reduce_mean(g_loss_adv)\n",
    "        total_loss = reconstruction_loss + kl_loss * self.beta + g_loss_adv * self.gamma\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"raw_loss\": reconstruction_loss + kl_loss,\n",
    "            \"disc_loss\": g_loss_adv,\n",
    "            \"w_kl_loss\": kl_loss * self.beta,\n",
    "            \"w_disc_loss\": g_loss_adv * self.gamma\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
