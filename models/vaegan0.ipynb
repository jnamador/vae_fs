{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8e97ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 17:16:29.371971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-10 17:16:30.631900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/nersc/pymon', '/global/common/software/nersc9/tensorflow/2.12.0/lib/python39.zip', '/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9', '/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/lib-dynload', '', '/global/homes/j/jananinf/.local/perlmutter/tensorflow2.12.0/lib/python3.9/site-packages', '/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages', '/eos/home-i02/h/hjia/.local/lib/python3.9/site-packages']\n",
      "20.371832715762604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn.metrics as sk\n",
    "import sys\n",
    "import os\n",
    "# Add your local packages directory to Python path\n",
    "sys.path.append('/eos/home-i02/h/hjia/.local/lib/python3.9/site-packages')\n",
    "# Print paths to verify\n",
    "print(sys.path)\n",
    "import math\n",
    "phi_res = 128/(2*math.pi)\n",
    "print(phi_res)\n",
    "# from dcor import distance_correlation as dcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse_loss_with_multi_index_scaling(masked_data, masked_reconstruction):\n",
    "    jet_scale = 1\n",
    "    tau_scale = 1\n",
    "    muon_sacle = 1\n",
    "    met_scale = 1\n",
    "\n",
    "    # Define the indices and their corresponding scale factors\n",
    "    scale_dict = {\n",
    "        0: jet_scale,\n",
    "        3: jet_scale,\n",
    "        6: jet_scale,\n",
    "        9: jet_scale,\n",
    "        12: jet_scale,\n",
    "        15: jet_scale,\n",
    "        18: tau_scale,\n",
    "        21: tau_scale,\n",
    "        24: tau_scale,\n",
    "        27: tau_scale,\n",
    "        30: muon_sacle,\n",
    "        33: muon_sacle,\n",
    "        36: muon_sacle,\n",
    "        39: muon_sacle,\n",
    "        42: met_scale\n",
    "    }\n",
    "    \n",
    "    # Create the scaling tensor\n",
    "    scale_tensor = tf.ones_like(masked_data)\n",
    "    \n",
    "    for index, factor in scale_dict.items():\n",
    "        index_mask = tf.one_hot(index, depth=tf.shape(masked_data)[-1])\n",
    "        scale_tensor += index_mask * (factor - 1)\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaled_data = masked_data * scale_tensor\n",
    "    scaled_reconstruction = masked_reconstruction * scale_tensor\n",
    "    \n",
    "#     # Hardcoded lists for eta and phi indices\n",
    "#     eta_indices = [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40]\n",
    "#     phi_indices = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 43]\n",
    "\n",
    "#     batch_size = tf.shape(scaled_reconstruction)[0]\n",
    "    \n",
    "#     # Apply constraints to eta\n",
    "#     for i in eta_indices:\n",
    "#         indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "#         updates = 3 * tf.tanh(scaled_reconstruction[:, i] / 3)\n",
    "#         scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "#     # Apply constraints to phi\n",
    "#     for i in phi_indices:\n",
    "#         indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "#         updates = 3.14159265258979*(10/8) * tf.tanh(scaled_reconstruction[:, i] / (3.14159265258979*(10/8)))\n",
    "#         scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    # Calculate MSE using keras.losses.mse\n",
    "    mse = keras.losses.mse(scaled_data, scaled_reconstruction)\n",
    "    \n",
    "    # Take the mean across all dimensions\n",
    "    return tf.reduce_mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a464b17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVAE_GAN_Model\u001b[39;00m(\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder, decoder, discriminator, steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,cycle_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, min_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,min_gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "class VAE_GAN_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator, steps_per_epoch=20,cycle_length=20, min_beta=0, max_beta=1,min_gamma=0, max_gamma=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        # per keras VAE example https://keras.io/examples/generative/vae/\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.gamma_tracker = keras.metrics.Mean(name=\"gamma\")\n",
    "\n",
    "\n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "\n",
    "        self.min_gamma = tf.cast(min_gamma, tf.float32)\n",
    "        self.max_gamma = tf.cast(max_gamma, tf.float32)\n",
    "        self.gamma = tf.Variable(min_gamma, dtype=tf.float32)\n",
    "\n",
    "    # def compile(self, optimizer, **kwargs):\n",
    "    #     super(VAE_GAN_Model, self).compile(**kwargs)\n",
    "    #     # Set the optimizer for the entire model (encoder + decoder + discriminator)\n",
    "    #     self.optimizer = optimizer\n",
    "\n",
    "    #     # Collect trainable variables from encoder, decoder, and discriminator\n",
    "    #     trainable_variables = (\n",
    "    #         self.encoder.trainable_weights + \n",
    "    #         self.decoder.trainable_weights + \n",
    "    #         self.discriminator.trainable_weights\n",
    "    #     )\n",
    "    #     # Build the optimizer with the full variable list\n",
    "    #     self.optimizer.build(trainable_variables)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        # For first half (x < 0.5), scale 2x from 0 to 1\n",
    "        # For second half (x >= 0.5), stay at 1\n",
    "        scaled_x = tf.where(x < 0.5, 2.0 * x, 1.0)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * scaled_x\n",
    "    \n",
    "    def get_gamma_schedule(self, epoch):\n",
    "        # Convert to float32 for TF operations\n",
    "        epoch = tf.cast(epoch, tf.float32)\n",
    "        \n",
    "        # Calculate annealing progress\n",
    "        anneal_progress = (epoch - 50.0) / 50.0\n",
    "        gamma_anneal = self.min_gamma + (self.max_gamma - self.min_gamma) * anneal_progress\n",
    "        \n",
    "        # Implement the conditions using tf.where\n",
    "        gamma = tf.where(epoch < 50.0, \n",
    "                        0.0,  # if epoch < 50\n",
    "                        tf.where(epoch >= 100.0,\n",
    "                                self.max_gamma,  # if epoch >= 100\n",
    "                                gamma_anneal))   # if 50 <= epoch < 100\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Is this the beta tuning?\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # here we shove in our custom reconstructionn loss function\n",
    "            # Ignore zero-padded entries. \n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx()) \n",
    "            reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*reconstruction, mask*data)\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            # This is just standard Kullback-Leibler diversion loss. I think this can stay.\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "            # Now let solve what beta is\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "    \n",
    "    # Since we overrode train_step we need test_step\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        \n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "        reconstruction_loss *= (1 - self.beta)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)        \n",
    "        kl_loss *=self.beta\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef11abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
