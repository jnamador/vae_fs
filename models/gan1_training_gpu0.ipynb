{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9774118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTEMPT_NUM = 10\n",
    "# To be copied from gan_training_history.txt ---\n",
    "NUM_TRAIN      = 10 # Number of iterations to train for.\n",
    "# VAE Architecture\n",
    "INPUT_SZ       = 57\n",
    "H1_SZ          = 32 # Hidden layer 1 size\n",
    "H2_SZ          = 16 # \"          \" 2 \"  \"\n",
    "LATENT_SZ      = 3\n",
    "# Discriminator Architecture # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H1_SZ     = 8 # Size of first hidden layer of discriminator  \n",
    "DISC_H2_SZ     = 2 # \"\" second hidden layer \"\"\n",
    "# Training schedule and parameters\n",
    "NUM_EPOCHS     = 100\n",
    "STEPS_EPOCH    = 20 # Steps per epoch\n",
    "BATCH_SIZE     = 1024\n",
    "STOP_PATIENCE  = 40\n",
    "LR_PATIENCE    = 20\n",
    "LR             = 0.001 # Learning rate\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "VAL_SPLIT      = 0.2 # Validation split\n",
    "CYCLE_LEN      = 20\n",
    "SHUFFLE_BOOL   = True\n",
    "# Hyperparameters\n",
    "MIN_BETA       = 0\n",
    "MAX_BETA       = 1\n",
    "MIN_GAMMA      = 1\n",
    "MAX_GAMMA      = 5\n",
    "# ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt{ATTEMPT_NUM}/\" \n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=LR, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ) # Modified this to the ATLAS-VAE-GAN\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(\n",
    "                            enc\n",
    "                            ,dec\n",
    "                            ,disc\n",
    "                            ,cycle_length=CYCLE_LEN\n",
    "                            ,min_beta=MIN_BETA\n",
    "                            ,max_beta=MAX_BETA\n",
    "                            ,min_gamma=MIN_GAMMA\n",
    "                            ,max_gamma=MAX_GAMMA\n",
    "                            ,max_epochs=NUM_EPOCHS\n",
    "                            ,steps_per_epoch=STEPS_EPOCH\n",
    "                            )\n",
    "        opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=VAL_SPLIT, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=SHUFFLE_BOOL)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty Loss plots while we're at it\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    \n",
    "    # Plot training losses\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\",\n",
    "                 marker= \"x\" if key[0:3] == 'val' else \"o\",\n",
    "                 markersize=10) \n",
    "\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses. Attempt: {ATTEMPT_NUM} Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7396c6-3db9-4eb2-86ff-d2c40bc164e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.12.0",
   "language": "python",
   "name": "tensorflow-2.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
