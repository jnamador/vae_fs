{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 13:58:23.104412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-25 13:58:26.499214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "import sys\n",
    "# Path to dir model.py lives in -------\n",
    "# NOTE: This needs to be modified to where your repo lives, path to /repo/path/VAE_FS/models/\n",
    "# If the jupyter notebook kernel is running from VAE_FS/models/ the\n",
    "\n",
    "# line below is not needed\n",
    "sys.path.append('/global/homes/j/jananinf/projs/VAE_FS/models/')\n",
    "\n",
    "# import the custom models and functions\n",
    "from models import Qmake_encoder_set_weights, Qmake_decoder_set_weights, Qmake_discriminator, VAE_GAN_Model\n",
    "from data_and_eval_utils import load_preprocessed_snl\n",
    "\n",
    "# in gan1. We train the VAE_GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "data = load_preprocessed_snl()\n",
    "X_train = data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be copied from gan_training_history.txt ---\n",
    "NUM_TRAIN      = 10 # Number of iterations to train for.\n",
    "# VAE Architecture\n",
    "INPUT_SZ       = 57\n",
    "H1_SZ          = 32 # Hidden layer 1 size\n",
    "H2_SZ          = 16 # \"          \" 2 \"  \"\n",
    "LATENT_SZ      = 3\n",
    "# Discriminator Architecture # 8, 2 is on ATLAS-VAE-GAN\n",
    "DISC_H1_SZ     = 8 # Size of first hidden layer of discriminator  \n",
    "DISC_H2_SZ     = 2 # \"\" second hidden layer \"\"\n",
    "# Training schedule and parameters\n",
    "NUM_EPOCHS     = 100\n",
    "STEPS_EPOCH    = 20 # Steps per epoch\n",
    "BATCH_SIZE     = 1024\n",
    "STOP_PATIENCE  = 40\n",
    "LR_PATIENCE    = 20\n",
    "LR             = 0.001 # Learning rate\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "VAL_SPLIT      = 0.2 # Validation split\n",
    "CYCLE_LEN      = 20\n",
    "SHUFFLE_BOOL   = True\n",
    "# Hyperparameters\n",
    "MIN_BETA       = 0\n",
    "MAX_BETA       = 1\n",
    "MIN_GAMMA      = 1\n",
    "MAX_GAMMA      = 50\n",
    "# ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeded4",
   "metadata": {},
   "source": [
    "### Simple training loop. No parameter sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86132",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "save = True\n",
    "SAVE_PATH = home_path+f\"/GAN_trainings/attempt6/\" #\n",
    "\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "# 7: Learning rate set to 0.00001, \n",
    "# 8: Learning rate set to 0.000001\n",
    "# 9: Learning rate set to 0.0000001\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=LR, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    if train:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(f\"TRAINING ITERATION {i} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "        disc = Qmake_discriminator(INPUT_SZ, DISC_H1_SZ, DISC_H2_SZ) # Modified this to the ATLAS-VAE-GAN\n",
    "\n",
    "        steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "        \n",
    "        # Modified these setting to match atlas VAE gan repo\n",
    "        vae = VAE_GAN_Model(\n",
    "                            enc\n",
    "                            ,dec\n",
    "                            ,disc\n",
    "                            ,cycle_length=CYCLE_LEN\n",
    "                            ,min_beta=MIN_BETA\n",
    "                            ,max_beta=MAX_BETA\n",
    "                            ,min_gamma=MIN_GAMMA\n",
    "                            ,max_gamma=MAX_GAMMA\n",
    "                            ,max_epochs=NUM_EPOCHS\n",
    "                            ,steps_per_epoch=STEPS_EPOCH\n",
    "                            )\n",
    "        opt = keras.optimizers.Adam(learning_rate=LR)\n",
    "        # --\n",
    "        vae.compile(optimizer=opt)\n",
    "        history = vae.fit(x=X_train, validation_split=VAL_SPLIT, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=SHUFFLE_BOOL)\n",
    "\n",
    "        \n",
    "        # Iterative training. \n",
    "        save_path = SAVE_PATH+f\"n_{i}/\" \n",
    "        if save:\n",
    "            print(f\"SAVING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            vae.save_weights(filepath=save_path, save_format='tf')\n",
    "\n",
    "            # Now save the histories\n",
    "            with open(save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                pkl.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ec6e1",
   "metadata": {},
   "source": [
    "Moving over to large parametric sweep to find something that will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# NUM_TRAIN = 4 # Train just once for now\n",
    "SAVE_PATH = home_path+f\"GAN_trainings/attempt6/\" #\n",
    "train = False\n",
    "save = True\n",
    "NUM_TRAIN = 3 # Train just once for now\n",
    "# Next attempt should go to 2\n",
    "# Attempt History. The original code for each folder should also be tied to the commits. \n",
    "# 0: First attempt. GAN as copied from other repo https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "# 1: Added GAN loss to \n",
    "# 2: Various parametric sweeps\n",
    "# 3: Better file naming convention and varied clipnorm\n",
    "# Notes: Smaller clipnorm ~ 0.1 tended to bring down the losses rather than blowing up.\n",
    "# Keeping clipnorm to 0.1 in future trainings\n",
    "# 4: Varying gamma maxes\n",
    "# 5: gamma = 0 sanity check\n",
    "# 6: Changed Discriminator to 8, 2 for hidden layers as in https://github.com/max-cohen54/AD_trigger_training/blob/main/L1AD/software/VAE_GAN/L1_VAE_Analyzer_FDL_GAN_ALT.ipynb\n",
    "## No longer sweeping for now.\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "parameters = [0]\n",
    "parameters_key = \"max_gamma\"\n",
    "SAVE_PATH = SAVE_PATH + parameters_key + \"/\"\n",
    "\n",
    "n_train = 3 # Train at least 3 models per parameter\n",
    "\n",
    "if train:\n",
    "    for param in parameters:\n",
    "        for i in range(n_train):\n",
    "            save_path = SAVE_PATH + f\"{parameters_key}_{param}/\"\n",
    "\n",
    "            # Manually make the directories and file. Python can do it, but its cleaner to do it manually\n",
    "            with open(SAVE_PATH +\"out.txt\", \"a\") as f:\n",
    "                print(f\"Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n\", file=f)\n",
    "\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "            disc = Qmake_discriminator(INPUT_SZ, 8, 2) # Testing out these values for now\n",
    "\n",
    "            steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "            vae = VAE_GAN_Model(enc, dec, disc, cycle_length=20, min_beta=0, max_beta=1, min_gamma=1, max_gamma=50)\n",
    "            opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "            history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping,reduce_lr], shuffle=True)\n",
    "\n",
    "            # Make loss plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            # Plot training losses\n",
    "            for key, val in history.history.items():\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                plt.plot(val, label=key, \n",
    "                        linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "                \n",
    "            # Customize the plot\n",
    "            plt.title(f'Variant: {parameters_key} = {param} Training and Validation Losses Run: {i}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.semilogy()\n",
    "\n",
    "            # # Show the plot\n",
    "            # plt.show()\n",
    "            \n",
    "            # Iterative training. \n",
    "            # save_path = save_path + f\"n_{i}/\" # As of 7/8/25. Should be synced with vae0_analysis\n",
    "            if save:\n",
    "                iter_save_path = save_path +  f\"n_{i}/\"\n",
    "\n",
    "                # Save progress to main out file\n",
    "                with open(SAVE_PATH + \"out.txt\", \"a\") as f: \n",
    "                    print(f\"SAVING Variant: {parameters_key} = {param} TRAINING ITERATION {i} ~~~~~~~~~~~\\n Save path: {iter_save_path}\\n\", file=f, flush = True)\n",
    "\n",
    "                # Save weights to iter specific folder\n",
    "                vae.save_weights(filepath=iter_save_path , save_format='tf')\n",
    "                # Now save the histories\n",
    "                with open(iter_save_path + f\"training_history.pkl\", 'wb') as f:\n",
    "                    pkl.dump(history.history, f)\n",
    "                plt.savefig(iter_save_path + parameters_key + f\"_{param}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d48f9",
   "metadata": {},
   "source": [
    "Plot Loss vs epoch history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "for i in range(NUM_TRAIN):\n",
    "    save_path = SAVE_PATH + f\"n_{i}/\"\n",
    "    with open(save_path + 'training_history.pkl', 'rb') as f:\n",
    "        history = pkl.load(f)\n",
    "\n",
    "    # Extract the loss values\n",
    "    total_loss = history['loss']\n",
    "    reco_loss = history['reco_loss']\n",
    "    kl_loss = history['kl_loss']\n",
    "    val_total_loss = history['val_loss']\n",
    "    val_reco_loss = history['val_reco_loss']\n",
    "    val_kl_loss = history['val_kl_loss']\n",
    "    gamma = history['gamma']\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot training losses\n",
    "    for key, val in history.items():\n",
    "        if key == 'lr':\n",
    "            continue\n",
    "        plt.plot(val, label=key, \n",
    "                 linestyle = \"dashed\" if key[0:3] == 'val' else \"solid\") \n",
    "    # plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "    # plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "    # plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "    # plt.plot(history['beta'],label=\"beta\")\n",
    "    # plt.plot(history['gamma'], label=\"$\\gamma$\")\n",
    "\n",
    "    # # Plot validation losses\n",
    "    # plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "    # plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "    # plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Training and Validation Losses Run: {i}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.semilogy()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we want as a AD metric? the discriminator or latent space vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
