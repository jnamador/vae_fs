{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 16:05:02.896987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-30 16:05:03.988722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(gpus[2], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "# from scipy.optimize imporjun26t curve_fit\n",
    "# from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import sklearn.metrics as sk\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, LeakyReLU, ReLU, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras.metrics import Precision\n",
    "# # from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm # These don't seem to be used\n",
    "# # from qkeras import quantized_relu, quantized_bits\n",
    "# from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "    X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "    X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "    Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data? \n",
    "    hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57)\n",
    "    hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57)\n",
    "    leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57)\n",
    "    print(\"Data loaded from preprocessed_SNL_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81916cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae07e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmake_encoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    \"\"\"\n",
    "    Makes encoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        size of input layer\n",
    "    h_dim_[X] : int\n",
    "        size of hidden layer X\n",
    "    latent_dim : int\n",
    "        size of latent layer\n",
    "    \"\"\"\n",
    "\n",
    "    # What is this and why? ----------------------------------------------------\n",
    "    # l2_factor = 1e-3 \n",
    "    # --    \n",
    "\n",
    "    # Input layer --------------------------------------------------------------\n",
    "    inputs = keras.Input(shape=(input_dim))\n",
    "    x = BatchNormalization(name=\"enc_BN_Input\")(inputs) # paper indicates batch \n",
    "                                               # normalization at the beginning\n",
    "                                               # of each layer\n",
    "    # --\n",
    "\n",
    "    # Hidden layer 1 -----------------------------------------------------------\n",
    "    x = Dense(h_dim_1,\n",
    "    # I am not quite seeing these in the paper. Maybe they're implied?\n",
    "            #  kernel_initializer=keras.initializers.HeNormal(seed=None), \n",
    "            #  bias_initializer=keras.initializers.Zeros(),\n",
    "            #  kernel_regularizer=l1_l2(l1=0, l2=l2_factor), # This is where the l2_factor is used.\n",
    "\n",
    "             name = \"enc_dense1\")(inputs)\n",
    "    x = BatchNormalization(name=\"enc_BN_h1\")(x)\n",
    "    x = LeakyReLU(name=\"enc_Lrelu1\")(x)\n",
    "    # ---\n",
    "\n",
    "    # Hidden Layer 1 -----------------------------------------------------------\n",
    "    x = Dense(h_dim_2,\n",
    "            #  kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "            #  bias_initializer=keras.initializers.Zeros(),\n",
    "            #  kernel_regularizer=l1_l2(l1=0, l2=l2_factor),\n",
    "             name = \"enc_dense2\")(x)    \n",
    "    x = BatchNormalization(name=\"enc_BN_h2\")(x)\n",
    "    x = LeakyReLU(name=\"enc_Lrelu2\")(x)\n",
    "    # ---\n",
    "\n",
    "    # Latent layer -------------------------------------------------------------\n",
    "    # No activation. \n",
    "    z_mean=Dense(latent_dim, name='z_mean',\n",
    "                #   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                #   bias_initializer=keras.initializers.Zeros(),\n",
    "                #   kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                )(x)\n",
    "    z_logvar=Dense(latent_dim, name='z_log_var',\n",
    "                    #   kernel_initializer=keras.initializers.Zeros(),\n",
    "                    #   bias_initializer=keras.initializers.Zeros(),\n",
    "                    #   kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                    )(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    # ---\n",
    "\n",
    "\n",
    "    encoder = keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def Qmake_decoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    \"\"\" \n",
    "    Makes decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        size of input layer\n",
    "    h_dim_[X] : int\n",
    "        size of hidden layer X\n",
    "    latent_dim : int\n",
    "        size of latent layer\n",
    "    \"\"\"\n",
    "    # l2_factor = 1e-3\n",
    "    # Input layer -------\n",
    "    inputs=keras.Input(shape=(latent_dim)) \n",
    "    x = BatchNormalization(name=\"dec_BN_IN\")(inputs)   \n",
    "    # --\n",
    "\n",
    "    # Hiden layer 1 (3 total, not counting latent) -------\n",
    "    x = Dense(h_dim_2,\n",
    "\n",
    "                #    kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                #    bias_initializer=keras.initializers.Zeros(),\n",
    "                #    kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(x)\n",
    "    x = BatchNormalization(name=\"dec_BN_h3\")(x)\n",
    "    x = LeakyReLU(name=\"dec_Lrelu3\")(x)\n",
    "    # --\n",
    "\n",
    "\n",
    "    # Hidden layer 2( 4 total, not counting laten) -----\n",
    "    x = Dense(h_dim_1,\n",
    "                #    activation='relu',\n",
    "                #    kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                #    bias_initializer=keras.initializers.Zeros(),\n",
    "                #    kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(x)\n",
    "    x = BatchNormalization(name=\"dec_BN_h4\")(x)\n",
    "    x = LeakyReLU(name=\"dec_Lrelu4\")(x)\n",
    "    # --\n",
    "\n",
    "    x = Dense(input_dim,\n",
    "                #    kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                #    bias_initializer=keras.initializers.Zeros(),\n",
    "                #    kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(x)\n",
    "    x = BatchNormalization(name=\"dec_BN_h5\")(x)\n",
    "    y = LeakyReLU(name=\"dec_Lrelu5\")(x)\n",
    "    decoder=keras.Model(inputs, y,name='decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " enc_dense1 (Dense)             (None, 32)           1856        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " enc_BN_h1 (BatchNormalization)  (None, 32)          128         ['enc_dense1[0][0]']             \n",
      "                                                                                                  \n",
      " enc_Lrelu1 (LeakyReLU)         (None, 32)           0           ['enc_BN_h1[0][0]']              \n",
      "                                                                                                  \n",
      " enc_dense2 (Dense)             (None, 16)           528         ['enc_Lrelu1[0][0]']             \n",
      "                                                                                                  \n",
      " enc_BN_h2 (BatchNormalization)  (None, 16)          64          ['enc_dense2[0][0]']             \n",
      "                                                                                                  \n",
      " enc_Lrelu2 (LeakyReLU)         (None, 16)           0           ['enc_BN_h2[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 3)            51          ['enc_Lrelu2[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 3)            51          ['enc_Lrelu2[0][0]']             \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,678\n",
      "Trainable params: 2,582\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 16:05:11.264985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2025-06-30 16:05:11.266764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2025-06-30 16:05:11.268525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38366 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2025-06-30 16:05:11.270293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38366 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fd932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dec_BN_IN (BatchNormalizati  (None, 3)                12        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                64        \n",
      "                                                                 \n",
      " dec_BN_h3 (BatchNormalizati  (None, 16)               64        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dec_Lrelu3 (LeakyReLU)      (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dec_BN_h4 (BatchNormalizati  (None, 32)               128       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dec_Lrelu4 (LeakyReLU)      (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      " dec_BN_h5 (BatchNormalizati  (None, 57)               228       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dec_Lrelu5 (LeakyReLU)      (None, 57)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,921\n",
      "Trainable params: 2,705\n",
      "Non-trainable params: 216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c93bef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_MSE(reconstruction, data):\n",
    "    # \"We use a dataset with standardized p_T as a target so that all quantities are O(1)\" arXiv: 2108.03986 \n",
    "\n",
    "    # Q: is the input also standardized?\n",
    "    \n",
    "    loss = keras.losses.mse(data, reconstruction)\n",
    "    return loss\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # per keras VAE example https://keras.io/examples/generative/vae/\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reonstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # here we shove in our custom reconstructionn loss function\n",
    "            \n",
    "            # Ignore zero-padded entries. \n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx()) \n",
    "            print(f\"DATA SHAPE = {tf.shape(data)}\\n\", f\"RECONSTRUCTION shape  = {tf.shape(reconstruction)}\")\n",
    "            # break\n",
    "            reconstruction_loss = _custom_MSE(mask*reconstruction, mask*data)\n",
    "\n",
    "            # This is just standard Kullback-Leibler diversion loss. I think this can stay.\n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis = 1))\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        # break\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "        \n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        } \n",
    "\n",
    "    # need to define the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "453cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE_Model(enc, dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "443fc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16bf886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "DATA SHAPE = Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      " RECONSTRUCTION shape  = Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_526191/1893446130.py\", line 42, in train_step\n        kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n\n    NameError: name 'ops' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejisecjxk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 42\u001b[0m, in \u001b[0;36mVAE_Model.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     39\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m _custom_MSE(mask\u001b[38;5;241m*\u001b[39mreconstruction, mask\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# This is just standard Kullback-Leibler diversion loss. I think this can stay.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m ops\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m ops\u001b[38;5;241m.\u001b[39mexp(z_log_var))\n\u001b[1;32m     43\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msum(kl_loss, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     45\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/global/common/software/nersc9/tensorflow/2.12.0/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_526191/1893446130.py\", line 42, in train_step\n        kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n\n    NameError: name 'ops' is not defined\n"
     ]
    }
   ],
   "source": [
    "vae.fit(x=X_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff3710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
