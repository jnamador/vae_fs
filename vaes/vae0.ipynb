{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b693cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 10:04:25.079869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 10:04:26.564442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" # on NERSC filelocking is not allowed\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "# Make notebook run on other GPUS. GPT's solution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(gpus[2], 'GPU')  # change 1 to 0, 2, 3 as needed\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "# from scipy.optimize imporjun26t curve_fit\n",
    "# from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import sklearn.metrics as sk\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, LeakyReLU, ReLU, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras.metrics import Precision\n",
    "# # from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm # These don't seem to be used\n",
    "# # from qkeras import quantized_relu, quantized_bits\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9774118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from preprocessed_SNL_data.h5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "home_path = \"/global/cfs/cdirs/m2616/jananinf/projsIO/VAE_FS/\" # Updated to NERSC\n",
    "file_path = home_path + \"preprocessed_SNL_data.h5\"\n",
    "with h5py.File(file_path, 'r') as hf:           # Shapes:\n",
    "    X_train = hf['X_train'][:]                  # (3200000, 57)\n",
    "    X_test  = hf['X_test'][:]                   # (800000,  57)\n",
    "    Ato4l_data  = hf['Ato4l_data'][:]           # (55969,   57) Signal data? \n",
    "    hToTauTau_data  = hf['hToTauTau_data'][:]   # (691283,  57)\n",
    "    hChToTauNu_data  = hf['hChToTauNu_data'][:] # (760272,  57)\n",
    "    leptoquark_data = hf['leptoquark_data'][:]  # (340544,  57)\n",
    "    print(\"Data loaded from preprocessed_SNL_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81916cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae07e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmake_encoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    \"\"\"\n",
    "    Makes encoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        size of input layer\n",
    "    h_dim_[X] : int\n",
    "        size of hidden layer X\n",
    "    latent_dim : int\n",
    "        size of latent layer\n",
    "    \"\"\"\n",
    "\n",
    "    # What is this and why? ----------------------------------------------------\n",
    "    # update: well we don't want to be too different from Kenny's repo afterall. Initialization in layers are kept to stay consistent with Kenny. Batch normalization removed for same reason.\n",
    "    l2_factor = 1e-3 \n",
    "    # --    \n",
    "\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=(input_dim))\n",
    "\n",
    "    # Hidden layer 1 -----------------------------------------------------------\n",
    "    x = Dense(h_dim_1,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None), \n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             kernel_regularizer=l1_l2(l1=0, l2=l2_factor), # This is where the l2_factor is used.\n",
    "             name = \"enc_dense1\")(inputs)\n",
    "    x = LeakyReLU(name=\"enc_Lrelu1\")(x)\n",
    "    # ---\n",
    "\n",
    "    # Hidden Layer 1 -----------------------------------------------------------\n",
    "    x = Dense(h_dim_2,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             kernel_regularizer=l1_l2(l1=0, l2=l2_factor),\n",
    "             name = \"enc_dense2\")(x)\n",
    "    x = LeakyReLU(name=\"enc_Lrelu2\")(x)\n",
    "    # ---\n",
    "\n",
    "    # Latent layer -------------------------------------------------------------\n",
    "    # No activation. \n",
    "    z_mean=Dense(latent_dim, name='z_mean',\n",
    "                  kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                  bias_initializer=keras.initializers.Zeros(),\n",
    "                  kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                )(x)\n",
    "    z_logvar=Dense(latent_dim, name='z_log_var',\n",
    "                      kernel_initializer=keras.initializers.Zeros(),\n",
    "                      bias_initializer=keras.initializers.Zeros(),\n",
    "                      kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                    )(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    # ---\n",
    "\n",
    "\n",
    "    encoder = keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def Qmake_decoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    \"\"\" \n",
    "    Makes decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        size of input layer\n",
    "    h_dim_[X] : int\n",
    "        size of hidden layer X\n",
    "    latent_dim : int\n",
    "        size of latent layer\n",
    "    \"\"\"\n",
    "    l2_factor = 1e-3\n",
    "    # Input layer -------\n",
    "    inputs=keras.Input(shape=(latent_dim)) \n",
    "\n",
    "    # Hiden layer 1 (3 total, not counting latent) -------\n",
    "    x = Dense(h_dim_2,\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(inputs)\n",
    "    x = LeakyReLU(name=\"dec_Lrelu3\")(x)\n",
    "    # --\n",
    "\n",
    "\n",
    "    # Hidden layer 2( 4 total, not counting laten) -----\n",
    "    x = Dense(h_dim_1,\n",
    "    # ? ----  #    activation='relu', # Why ReLU over papers leaky ReLU?\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(x)\n",
    "    x = LeakyReLU(name=\"dec_Lrelu4\")(x)\n",
    "    # --\n",
    "\n",
    "    x = Dense(input_dim,\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor)\n",
    "                   )(x)\n",
    "    y = LeakyReLU(name=\"dec_Lrelu5\")(x)\n",
    "    decoder=keras.Model(inputs, y,name='decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9877d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " enc_dense1 (Dense)             (None, 32)           1856        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " enc_Lrelu1 (LeakyReLU)         (None, 32)           0           ['enc_dense1[0][0]']             \n",
      "                                                                                                  \n",
      " enc_dense2 (Dense)             (None, 16)           528         ['enc_Lrelu1[0][0]']             \n",
      "                                                                                                  \n",
      " enc_Lrelu2 (LeakyReLU)         (None, 16)           0           ['enc_dense2[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 3)            51          ['enc_Lrelu2[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 3)            51          ['enc_Lrelu2[0][0]']             \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,486\n",
      "Trainable params: 2,486\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 10:04:35.397318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2025-07-04 10:04:35.399984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2025-07-04 10:04:35.402843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38366 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2025-07-04 10:04:35.404589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38366 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "INPUT_SZ = 57\n",
    "H1_SZ = 32\n",
    "H2_SZ = 16\n",
    "LATENT_SZ = 3\n",
    "enc = Qmake_encoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fd932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                64        \n",
      "                                                                 \n",
      " dec_Lrelu3 (LeakyReLU)      (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dec_Lrelu4 (LeakyReLU)      (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      " dec_Lrelu5 (LeakyReLU)      (None, 57)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,489\n",
      "Trainable params: 2,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec = Qmake_decoder_set_weights(INPUT_SZ, H1_SZ, H2_SZ, LATENT_SZ)\n",
    "dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c93bef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _custom_MSE(reconstruction, data):\n",
    "#     # \"We use a dataset with standardized p_T as a target so that all quantities are O(1)\" arXiv: 2108.03986 \n",
    "\n",
    "#     # Q: is the input also standardized?\n",
    "    \n",
    "#     loss = keras.losses.mse(data, reconstruction)\n",
    "#     return loss\n",
    "\n",
    "def _custom_MSE(masked_data, masked_reconstruction):\n",
    "#     jet_scale = 256/64\n",
    "#     tau_scale = 128/64\n",
    "#     muon_scale = 32/64\n",
    "#     met_scale = 512/64\n",
    "#     em_scale = 128/64\n",
    "    jet_scale = 1\n",
    "    tau_scale = 1\n",
    "    muon_scale = 1\n",
    "    met_scale = 1\n",
    "    em_scale = 1\n",
    "    # Define the indices and their corresponding scale factors\n",
    "    scale_dict = {\n",
    "        0: met_scale,\n",
    "        3: em_scale, 6: em_scale, 9: em_scale, 12: em_scale,\n",
    "        15: tau_scale, 18: tau_scale, 21: tau_scale, 24: tau_scale,\n",
    "        27: jet_scale, 30: jet_scale, 33: jet_scale, 36: jet_scale, 39: jet_scale, 42: jet_scale,\n",
    "        45: muon_scale, 48: muon_scale, 51: muon_scale, 54: muon_scale\n",
    "    }\n",
    "\n",
    "    # Create the scaling tensor\n",
    "    scale_tensor = tf.ones_like(masked_data)\n",
    "    for index, factor in scale_dict.items():\n",
    "        index_mask = tf.one_hot(index, depth=tf.shape(masked_data)[-1])\n",
    "        scale_tensor += index_mask * (factor - 1)\n",
    "\n",
    "    # Apply scaling\n",
    "    scaled_data = masked_data * scale_tensor\n",
    "    scaled_reconstruction = masked_reconstruction * scale_tensor\n",
    "\n",
    "    # Hardcoded lists for eta and phi indices\n",
    "    eta_indices = [4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52, 55]\n",
    "    phi_indices = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56]\n",
    "\n",
    "    batch_size = tf.shape(scaled_reconstruction)[0]\n",
    "    \n",
    "    # Set only the first eta (index 1) to zero\n",
    "    indices = tf.stack([tf.range(batch_size), tf.ones(batch_size, dtype=tf.int32)], axis=1)\n",
    "    updates = tf.zeros(batch_size)\n",
    "    scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "    # Apply constraints to eta\n",
    "    for i in eta_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3 * tf.tanh(scaled_reconstruction[:, i] / 3)\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "    # Apply constraints to phi\n",
    "    for i in phi_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3.14159265258979 * tf.tanh(scaled_reconstruction[:, i] / 3.14159265258979)\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "        \n",
    "    # Calculate MSE using keras.losses.mse\n",
    "    mse = keras.losses.mse(scaled_data, scaled_reconstruction)\n",
    "\n",
    "    # Take the sum across all dimensions\n",
    "    return tf.reduce_mean(mse)\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, steps_per_epoch=3125,cycle_length=10, min_beta=0.1, max_beta=0.85, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # beta turning part?\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "\n",
    "        # per keras VAE example https://keras.io/examples/generative/vae/\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        # is this the beta tuning?  \n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * tf.minimum(x, 1.0)\n",
    "    \n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Is this the beta tuning?\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # here we shove in our custom reconstructionn loss function\n",
    "            \n",
    "            # Ignore zero-padded entries. \n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx()) \n",
    "            reconstruction_loss = _custom_MSE(mask*reconstruction, mask*data)\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            # This is just standard Kullback-Leibler diversion loss. I think this can stay.\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "            # Now let solve what beta is\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "        \n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        } \n",
    "\n",
    "    # need to define the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f858d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16384\n",
    "STOP_PATIENCE = 15\n",
    "LR_PATIENCE = 10\n",
    "steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "vae = VAE_Model(enc, dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.8)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1000)\n",
    "vae.compile(optimizer=opt) # Not sure what weighted_mse is doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 10:04:38.458967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-07-04 10:04:38.729642: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f4390189a70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 10:04:38.729663: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-04 10:04:38.729667: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-04 10:04:38.729670: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-04 10:04:38.729673: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-04 10:04:38.764595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 10:04:38.846358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-07-04 10:04:39.027895: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 13ms/step - loss: 3.7145 - reconstruction_loss: 3.0402 - kl_loss: 0.0741 - beta: 0.1561 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 1.9435 - reconstruction_loss: 1.6133 - kl_loss: 0.0806 - beta: 0.2124 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 1.2065 - reconstruction_loss: 1.0459 - kl_loss: 0.0887 - beta: 0.2688 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.9018 - reconstruction_loss: 0.7510 - kl_loss: 0.0944 - beta: 0.3251 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.6862 - reconstruction_loss: 0.5755 - kl_loss: 0.0976 - beta: 0.3815 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.5579 - reconstruction_loss: 0.4586 - kl_loss: 0.0983 - beta: 0.4378 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4813 - reconstruction_loss: 0.3763 - kl_loss: 0.0960 - beta: 0.4942 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4298 - reconstruction_loss: 0.3137 - kl_loss: 0.0914 - beta: 0.5506 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3442 - reconstruction_loss: 0.2621 - kl_loss: 0.0865 - beta: 0.6069 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3160 - reconstruction_loss: 0.2188 - kl_loss: 0.0802 - beta: 0.6633 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2521 - reconstruction_loss: 0.1788 - kl_loss: 0.0741 - beta: 0.7196 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2240 - reconstruction_loss: 0.1438 - kl_loss: 0.0677 - beta: 0.7760 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2523 - reconstruction_loss: 0.3329 - kl_loss: 0.0326 - beta: 0.1324 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.4561 - reconstruction_loss: 0.4371 - kl_loss: 0.0128 - beta: 0.1887 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4300 - reconstruction_loss: 0.3938 - kl_loss: 0.0172 - beta: 0.2451 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.3792 - reconstruction_loss: 0.3572 - kl_loss: 0.0209 - beta: 0.3015 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.3545 - reconstruction_loss: 0.3242 - kl_loss: 0.0234 - beta: 0.3578 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3218 - reconstruction_loss: 0.2936 - kl_loss: 0.0253 - beta: 0.4142 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3014 - reconstruction_loss: 0.2643 - kl_loss: 0.0263 - beta: 0.4705 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2668 - reconstruction_loss: 0.2365 - kl_loss: 0.0268 - beta: 0.5269 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2516 - reconstruction_loss: 0.2093 - kl_loss: 0.0263 - beta: 0.5833 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2109 - reconstruction_loss: 0.1826 - kl_loss: 0.0255 - beta: 0.6396 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1875 - reconstruction_loss: 0.1563 - kl_loss: 0.0239 - beta: 0.6960 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.1586 - reconstruction_loss: 0.1295 - kl_loss: 0.0223 - beta: 0.7523 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.1367 - reconstruction_loss: 0.1541 - kl_loss: 0.0181 - beta: 0.1087 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4256 - reconstruction_loss: 0.4018 - kl_loss: 0.0042 - beta: 0.1651 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.3716 - reconstruction_loss: 0.3627 - kl_loss: 0.0109 - beta: 0.2214 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3461 - reconstruction_loss: 0.3173 - kl_loss: 0.0193 - beta: 0.2778 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3221 - reconstruction_loss: 0.2901 - kl_loss: 0.0231 - beta: 0.3341 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3025 - reconstruction_loss: 0.2677 - kl_loss: 0.0241 - beta: 0.3905 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2691 - reconstruction_loss: 0.2449 - kl_loss: 0.0237 - beta: 0.4469 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2498 - reconstruction_loss: 0.2231 - kl_loss: 0.0226 - beta: 0.5032 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2324 - reconstruction_loss: 0.2017 - kl_loss: 0.0210 - beta: 0.5596 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2095 - reconstruction_loss: 0.1798 - kl_loss: 0.0189 - beta: 0.6159 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1961 - reconstruction_loss: 0.1576 - kl_loss: 0.0166 - beta: 0.6723 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1527 - reconstruction_loss: 0.1343 - kl_loss: 0.0142 - beta: 0.7287 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1312 - reconstruction_loss: 0.1104 - kl_loss: 0.0122 - beta: 0.7850 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2210 - reconstruction_loss: 0.2961 - kl_loss: 0.0071 - beta: 0.1414 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3458 - reconstruction_loss: 0.3303 - kl_loss: 0.0153 - beta: 0.1977 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3268 - reconstruction_loss: 0.3069 - kl_loss: 0.0187 - beta: 0.2541 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3000 - reconstruction_loss: 0.2847 - kl_loss: 0.0200 - beta: 0.3104 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2636 - kl_loss: 0.0209 - beta: 0.3668 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2626 - reconstruction_loss: 0.2428 - kl_loss: 0.0210 - beta: 0.4232 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2431 - reconstruction_loss: 0.2237 - kl_loss: 0.0204 - beta: 0.4795 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2234 - reconstruction_loss: 0.2019 - kl_loss: 0.0190 - beta: 0.5359 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1981 - reconstruction_loss: 0.1812 - kl_loss: 0.0172 - beta: 0.5922 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1893 - reconstruction_loss: 0.1604 - kl_loss: 0.0152 - beta: 0.6486 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1594 - reconstruction_loss: 0.1391 - kl_loss: 0.0129 - beta: 0.7050 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1367 - reconstruction_loss: 0.1172 - kl_loss: 0.0103 - beta: 0.7613 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1269 - reconstruction_loss: 0.1836 - kl_loss: 0.0069 - beta: 0.1177 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3541 - reconstruction_loss: 0.3324 - kl_loss: 0.0112 - beta: 0.1740 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3258 - reconstruction_loss: 0.3059 - kl_loss: 0.0155 - beta: 0.2304 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3093 - reconstruction_loss: 0.2828 - kl_loss: 0.0183 - beta: 0.2868 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2795 - reconstruction_loss: 0.2615 - kl_loss: 0.0202 - beta: 0.3431 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2709 - reconstruction_loss: 0.2413 - kl_loss: 0.0210 - beta: 0.3995 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2444 - reconstruction_loss: 0.2219 - kl_loss: 0.0204 - beta: 0.4558 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2223 - reconstruction_loss: 0.2023 - kl_loss: 0.0192 - beta: 0.5122 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2023 - reconstruction_loss: 0.1827 - kl_loss: 0.0178 - beta: 0.5686 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1848 - reconstruction_loss: 0.1625 - kl_loss: 0.0160 - beta: 0.6249 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1613 - reconstruction_loss: 0.1425 - kl_loss: 0.0139 - beta: 0.6813 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1409 - reconstruction_loss: 0.1213 - kl_loss: 0.0115 - beta: 0.7376 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1141 - reconstruction_loss: 0.0999 - kl_loss: 0.0091 - beta: 0.7940 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2612 - reconstruction_loss: 0.3143 - kl_loss: 0.0069 - beta: 0.1503 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3286 - reconstruction_loss: 0.3047 - kl_loss: 0.0130 - beta: 0.2067 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2884 - reconstruction_loss: 0.2801 - kl_loss: 0.0171 - beta: 0.2631 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2713 - reconstruction_loss: 0.2600 - kl_loss: 0.0195 - beta: 0.3194 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2854 - reconstruction_loss: 0.2412 - kl_loss: 0.0202 - beta: 0.3758 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2481 - reconstruction_loss: 0.2220 - kl_loss: 0.0201 - beta: 0.4321 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2400 - reconstruction_loss: 0.2035 - kl_loss: 0.0195 - beta: 0.4885 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2031 - reconstruction_loss: 0.1839 - kl_loss: 0.0180 - beta: 0.5449 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1878 - reconstruction_loss: 0.1649 - kl_loss: 0.0166 - beta: 0.6012 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1555 - reconstruction_loss: 0.1455 - kl_loss: 0.0149 - beta: 0.6576 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1394 - reconstruction_loss: 0.1253 - kl_loss: 0.0129 - beta: 0.7140 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.1208 - reconstruction_loss: 0.1051 - kl_loss: 0.0106 - beta: 0.7703 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.1372 - reconstruction_loss: 0.2096 - kl_loss: 0.0067 - beta: 0.1266 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3241 - reconstruction_loss: 0.3079 - kl_loss: 0.0096 - beta: 0.1830 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2918 - reconstruction_loss: 0.2817 - kl_loss: 0.0145 - beta: 0.2394 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2939 - reconstruction_loss: 0.2618 - kl_loss: 0.0174 - beta: 0.2957 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2611 - reconstruction_loss: 0.2421 - kl_loss: 0.0186 - beta: 0.3521 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2427 - reconstruction_loss: 0.2232 - kl_loss: 0.0192 - beta: 0.4085 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2400 - reconstruction_loss: 0.2049 - kl_loss: 0.0188 - beta: 0.4648 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2017 - reconstruction_loss: 0.1876 - kl_loss: 0.0182 - beta: 0.5212 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1867 - reconstruction_loss: 0.1676 - kl_loss: 0.0170 - beta: 0.5776 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1740 - reconstruction_loss: 0.1488 - kl_loss: 0.0156 - beta: 0.6339 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1458 - reconstruction_loss: 0.1293 - kl_loss: 0.0139 - beta: 0.6903 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1291 - reconstruction_loss: 0.1100 - kl_loss: 0.0120 - beta: 0.7466 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.0999 - reconstruction_loss: 0.1057 - kl_loss: 0.0094 - beta: 0.1030 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3448 - reconstruction_loss: 0.3158 - kl_loss: 0.0060 - beta: 0.1593 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.3062 - reconstruction_loss: 0.2853 - kl_loss: 0.0115 - beta: 0.2157 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2958 - reconstruction_loss: 0.2639 - kl_loss: 0.0150 - beta: 0.2720 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2653 - reconstruction_loss: 0.2465 - kl_loss: 0.0170 - beta: 0.3284 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2506 - reconstruction_loss: 0.2256 - kl_loss: 0.0181 - beta: 0.3848 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2274 - reconstruction_loss: 0.2073 - kl_loss: 0.0183 - beta: 0.4411 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.2078 - reconstruction_loss: 0.1892 - kl_loss: 0.0177 - beta: 0.4975 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1923 - reconstruction_loss: 0.1710 - kl_loss: 0.0172 - beta: 0.5538 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1751 - reconstruction_loss: 0.1525 - kl_loss: 0.0162 - beta: 0.6102 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1538 - reconstruction_loss: 0.1339 - kl_loss: 0.0149 - beta: 0.6666 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1329 - reconstruction_loss: 0.1149 - kl_loss: 0.0133 - beta: 0.7229 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1142 - reconstruction_loss: 0.0957 - kl_loss: 0.0113 - beta: 0.7793 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1563 - reconstruction_loss: 0.2347 - kl_loss: 0.0060 - beta: 0.1356 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00 - val_beta: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_total_loss, val_reconstruction_loss, val_lk_loss and _val_beta are all 0 for the entirety of the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2716cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights(filepath=home_path+'attempt0/', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
